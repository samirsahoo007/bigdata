https://www.whizlabs.com/blog/big-data-interview-questions/

Basic Big Data Interview Questions

1. What do you know about the term “Big Data”?

Answer: Big Data is a term associated with complex and large datasets. A relational database cannot handle big data, and that’s why special tools and methods are used to perform operations on a vast collection of data. Big data enables companies to understand their business better and helps them derive meaningful information from the unstructured and raw data collected on a regular basis. Big data also allows the companies to take better business decisions backed by data.

2. What are the five V’s of Big Data?

Answer: The five V’s of Big data is as follows:

Volume – Volume represents the volume i.e. amount of data that is growing at a high rate i.e. data volume in Petabytes
Velocity – Velocity is the rate at which data grows. Social media contributes a major role in the velocity of growing data.
Variety – Variety refers to the different data types i.e. various data formats like text, audios, videos, etc.
Veracity – Veracity refers to the uncertainty of available data. Veracity arises due to the high volume of data that brings incompleteness and inconsistency.
Value –Value refers to turning data into value. By turning accessed big data into values, businesses may generate revenue.
Big Data Interview Questions
5 V’s of Big Data
Note: This is one of the basic and significant questions asked in the big data interview. You can choose to explain the five V’s in detail if you see the interviewer is interested to know more. However, the names can even be mentioned if you are asked about the term “Big Data”.

3. Tell us how big data and Hadoop are related to each other.

Answer: Big data and Hadoop are almost synonyms terms. With the rise of big data, Hadoop, a framework that specializes in big data operations also became popular. The framework can be used by professionals to analyze big data and help businesses to make decisions.

Note: This question is commonly asked in a big data interview. You can go further to answer this question and try to explain the main components of Hadoop.

Recommended Reading: Big Data Trends in 2018

4. How is big data analysis helpful in increasing business revenue?

Answer: Big data analysis has become very important for the businesses. It helps businesses to differentiate themselves from others and increase the revenue. Through predictive analytics, big data analytics provides businesses customized recommendations and suggestions. Also, big data analytics enables businesses to launch new products depending on customer needs and preferences. These factors make businesses earn more revenue, and thus companies are using big data analytics. Companies may encounter a significant increase of 5-20% in revenue by implementing big data analytics. Some popular companies those are using big data analytics to increase their revenue is – Walmart, LinkedIn, Facebook, Twitter, Bank of America etc.

5. Explain the steps to be followed to deploy a Big Data solution.

Answer: Followings are the three steps that are followed to deploy a Big Data Solution –

i. Data Ingestion

The first step for deploying a big data solution is the data ingestion i.e. extraction of data from various sources. The data source may be a CRM like Salesforce, Enterprise Resource Planning System like SAP, RDBMS like MySQL or any other log files, documents, social media feeds etc. The data can be ingested either through batch jobs or real-time streaming. The extracted data is then stored in HDFS.

Big Data Interview Questions and Answers
Steps of Deploying Big Data Solution
ii. Data Storage

After data ingestion, the next step is to store the extracted data. The data either be stored in HDFS or NoSQL database (i.e. HBase). The HDFS storage works well for sequential access whereas HBase for random read/write access.

iii. Data Processing

The final step in deploying a big data solution is the data processing. The data is processed through one of the processing frameworks like Spark, MapReduce, Pig, etc.

Also Read: Top HBase Interview Questions with Detailed Answers

6. Define respective components of HDFS and YARN

Answer: The two main components of HDFS are-

NameNode – This is the master node for processing metadata information for data blocks within the HDFS
DataNode/Slave node – This is the node which acts as slave node to store the data, for processing and use by the NameNode
In addition to serving the client requests, the NameNode executes either of two following roles –

CheckpointNode – It runs on a different host from the NameNode
BackupNode- It is a read-only NameNode which contains file system metadata information excluding the block locations
Hadoop core components

The two main components of YARN are–

ResourceManager– This component receives processing requests and accordingly allocates to respective NodeManagers depending on processing needs.
NodeManager– It executes tasks on each single Data Node
Preparing for HDFS interview? Here we cover the most common HDFS interview questions and answers to help you crack the interview!

7. Why is Hadoop used for Big Data Analytics?

Answer: Since data analysis has become one of the key parameters of business, hence, enterprises are dealing with massive amount of structured, unstructured and semi-structured data. Analyzing unstructured data is quite difficult where Hadoop takes major part with its capabilities of  

Storage
Processing
Data collection
Moreover, Hadoop is open source and runs on commodity hardware. Hence it is a cost-benefit solution for businesses.

8. What is fsck?

Answer: fsck stands for File System Check. It is a command used by HDFS. This command is used to check inconsistencies and if there is any problem in the file. For example, if there are any missing blocks for a file, HDFS gets notified through this command.

9. What are the main differences between NAS (Network-attached storage) and HDFS?

Answer: The main differences between NAS (Network-attached storage) and HDFS –

HDFS runs on a cluster of machines while NAS runs on an individual machine. Hence, data redundancy is a common issue in HDFS. On the contrary, the replication protocol is different in case of NAS. Thus the chances of data redundancy are much less.
Data is stored as data blocks in local drives in case of HDFS. In case of NAS, it is stored in dedicated hardware.
10. What is the Command to format the NameNode?

Answer: $ hdfs namenode -format

Big data is not just what you think, it’s a broad spectrum. There are a number of career options in Big Data World. Here is an interesting and explanatory visual on Big Data Careers.

Experience-based Big Data Interview Questions

If you have some considerable experience of working in Big Data world, you will be asked a number of questions in your big data interview based on your previous experience. These questions may be simply related to your experience or scenario based. So, get prepared with these best Big data interview questions and answers –

11. Do you have any Big Data experience? If so, please share it with us.

How to Approach: There is no specific answer to the question as it is a subjective question and the answer depends on your previous experience. Asking this question during a big data interview, the interviewer wants to understand your previous experience and is also trying to evaluate if you are fit for the project requirement.

So, how will you approach the question? If you have previous experience, start with your duties in your past position and slowly add details to the conversation. Tell them about your contributions that made the project successful. This question is generally, the 2nd or 3rd question asked in an interview. The later questions are based on this question, so answer it carefully. You should also take care not to go overboard with a single aspect of your previous job. Keep it simple and to the point.

12. Do you prefer good data or good models? Why?

How to Approach: This is a tricky question but generally asked in the big data interview. It asks you to choose between good data or good models. As a candidate, you should try to answer it from your experience. Many companies want to follow a strict process of evaluating data, means they have already selected data models. In this case, having good data can be game-changing. The other way around also works as a model is chosen based on good data.

As we already mentioned, answer it from your experience. However, don’t say that having both good data and good models is important as it is hard to have both in real life projects.

13. Will you optimize algorithms or code to make them run faster?

How to Approach: The answer to this question should always be “Yes.” Real world performance matters and it doesn’t depend on the data or model you are using in your project.

The interviewer might also be interested to know if you have had any previous experience in code or algorithm optimization. For a beginner, it obviously depends on which projects he worked on in the past. Experienced candidates can share their experience accordingly as well. However, be honest about your work, and it is fine if you haven’t optimized code in the past. Just let the interviewer know your real experience and you will be able to crack the big data interview.

14. How do you approach data preparation?

How to Approach: Data preparation is one of the crucial steps in big data projects. A big data interview may involve at least one question based on data preparation. When the interviewer asks you this question, he wants to know what steps or precautions you take during data preparation.

As you already know, data preparation is required to get necessary data which can then further be used for modeling purposes. You should convey this message to the interviewer. You should also emphasize the type of model you are going to use and reasons behind choosing that particular model. Last, but not the least, you should also discuss important data preparation terms such as transforming variables, outlier values, unstructured data, identifying gaps, and others.

15. How would you transform unstructured data into structured data?

How to Approach: Unstructured data is very common in big data. The unstructured data should be transformed into structured data to ensure proper data analysis. You can start answering the question by briefly differentiating between the two. Once done, you can now discuss the methods you use to transform one form to another. You might also share the real-world situation where you did it. If you have recently been graduated, then you can share information related to your academic projects.

By answering this question correctly, you are signaling that you understand the types of data, both structured and unstructured, and also have the practical experience to work with these. If you give an answer to this question specifically, you will definitely be able to crack the big data interview.

16. Which hardware configuration is most beneficial for Hadoop jobs?

Dual processors or core machines with a configuration of  4 / 8 GB RAM and ECC memory is ideal for running Hadoop operations. However, the hardware configuration varies based on the project-specific workflow and process flow and need customization accordingly.

17. What happens when two users try to access the same file in the HDFS?

HDFS NameNode supports exclusive write only. Hence, only the first user will receive the grant for file access and the second user will be rejected.

18. How to recover a NameNode when it is down?

The following steps need to execute to make the Hadoop cluster up and running:

Use the FsImage which is file system metadata replica to start a new NameNode. 
Configure the DataNodes and also the clients to make them acknowledge the newly started NameNode.
Once the new NameNode completes loading the last checkpoint FsImage which has received enough block reports from the DataNodes, it will start to serve the client. 
In case of large Hadoop clusters, the NameNode recovery process consumes a lot of time which turns out to be a more significant challenge in case of routine maintenance.

19. What do you understand by Rack Awareness in Hadoop?

It is an algorithm applied to the NameNode to decide how blocks and its replicas are placed. Depending on rack definitions network traffic is minimized between DataNodes within the same rack. For example, if we consider replication factor as 3, two copies will be placed on one rack whereas the third copy in a separate rack.

20. What is the difference between “HDFS Block” and “Input Split”?

The HDFS divides the input data physically into blocks for processing which is known as HDFS Block.

Input Split is a logical division of data by mapper for mapping operation.

Enhance your Big Data skills with the experts. Here is the Complete List of Big Data Blogs where you can find latest news, trends, updates, and concepts of Big Data.

Basic Big Data Hadoop Interview Questions

Hadoop is one of the most popular Big Data frameworks, and if you are going for a Hadoop interview prepare yourself with these basic level interview questions for Big Data Hadoop. These questions will be helpful for you whether you are going for a Hadoop developer or Hadoop Admin interview.

21. Explain the difference between Hadoop and RDBMS.

Answer: The difference between Hadoop and RDBMS is as follows –Big Data Interview Q & A22. What are the common input formats in Hadoop?

Answer: Below are the common input formats in Hadoop –

Text Input Format – The default input format defined in Hadoop is the Text Input Format.
Sequence File Input Format – To read files in a sequence, Sequence File Input Format is used.
Key Value Input Format – The input format used for plain text files (files broken into lines) is the Key Value Input Format.
23. Explain some important features of Hadoop.

Answer: Hadoop supports the storage and processing of big data. It is the best solution for handling big data challenges. Some important features of Hadoop are –

Open Source – Hadoop is an open source framework which means it is available free of cost. Also, the users are allowed to change the source code as per their requirements.
Distributed Processing – Hadoop supports distributed processing of data i.e. faster processing. The data in Hadoop HDFS is stored in a distributed manner and MapReduce is responsible for the parallel processing of data.
Fault Tolerance – Hadoop is highly fault-tolerant. It creates three replicas for each block at different nodes, by default. This number can be changed according to the requirement. So, we can recover the data from another node if one node fails. The detection of node failure and recovery of data is done automatically.
Reliability – Hadoop stores data on the cluster in a reliable manner that is independent of machine. So, the data stored in Hadoop environment is not affected by the failure of the machine.
Scalability – Another important feature of Hadoop is the scalability. It is compatible with the other hardware and we can easily ass the new hardware to the nodes.
High Availability – The data stored in Hadoop is available to access even after the hardware failure. In case of hardware failure, the data can be accessed from another path.
24. Explain the different modes in which Hadoop run.

Answer: Apache Hadoop runs in the following three modes –

Standalone (Local) Mode – By default, Hadoop runs in a local mode i.e. on a non-distributed, single node. This mode uses the local file system to perform input and output operation. This mode does not support the use of HDFS, so it is used for debugging. No custom configuration is needed for configuration files in this mode.
Pseudo-Distributed Mode – In the pseudo-distributed mode, Hadoop runs on a single node just like the Standalone mode. In this mode, each daemon runs in a separate Java process. As all the daemons run on a single node, there is the same node for both the Master and Slave nodes.
Fully – Distributed Mode – In the fully-distributed mode, all the daemons run on separate individual nodes and thus forms a multi-node cluster. There are different nodes for Master and Slave nodes.
25. Explain the core components of Hadoop.

Answer: Hadoop is an open source framework that is meant for storage and processing of big data in a distributed manner. The core components of Hadoop are –

HDFS (Hadoop Distributed File System) – HDFS is the basic storage system of Hadoop. The large data files running on a cluster of commodity hardware are stored in HDFS. It can store data in a reliable manner even when hardware fails.
Best big data interview questions and answers
Core Components of Hadoop
Hadoop MapReduce – MapReduce is the Hadoop layer that is responsible for data processing. It writes an application to process unstructured and structured data stored in HDFS. It is responsible for the parallel processing of high volume of data by dividing data into independent tasks. The processing is done in two phases Map and Reduce. The Map is the first phase of processing that specifies complex logic code and the Reduce is the second phase of processing that specifies light-weight operations.
YARN – The processing framework in Hadoop is YARN. It is used for resource management and provides multiple data processing engines i.e. data science, real-time streaming, and batch processing.
26. What are the configuration parameters in a “MapReduce” program?

The main configuration parameters in “MapReduce” framework are:

Input locations of Jobs in the distributed file system
Output location of Jobs in the distributed file system
The input format of data
The output format of data
The class which contains the map function
The class which contains the reduce function
JAR file which contains the mapper, reducer and the driver classes
27. What is a block in HDFS and what is its default size in Hadoop 1 and Hadoop 2? Can we change the block size?

Blocks are smallest continuous data storage in a hard drive. For HDFS, blocks are stored across Hadoop cluster.

The default block size in Hadoop 1 is: 64 MB
The default block size in Hadoop 2 is: 128 MB
Yes, we can change block size by using the parameter – dfs.block.size located in the hdfs-site.xml file.

28. What is  Distributed Cache in a MapReduce Framework

Distributed Cache is a feature of Hadoop MapReduce framework to cache files for applications. Hadoop framework makes cached files available for every map/reduce tasks running on the data nodes. Hence, the data files can access the cache file as a local file in the designated job.

29. What are the three running modes of Hadoop?

The three running modes of Hadoop are as follows:

i. Standalone or local: This is the default mode and does not need any configuration. In this mode, all the following components of Hadoop uses local file system and runs on a single JVM –

NameNode
DataNode
ResourceManager
NodeManager
ii. Pseudo-distributed: In this mode, all the master and slave Hadoop services are deployed and executed on a single node.

iii. Fully distributed: In this mode, Hadoop master and slave services are deployed and executed on separate nodes.

30. Explain JobTracker in Hadoop

JobTracker is a JVM process in Hadoop to submit and track MapReduce jobs.

JobTracker performs the following activities in Hadoop in a sequence –

JobTracker receives jobs that a client application submits to the job tracker
JobTracker notifies NameNode to determine data node
JobTracker allocates TaskTracker nodes based on available slots.
it submits the work on allocated TaskTracker Nodes,
JobTracker monitors the TaskTracker nodes.
When a task fails, JobTracker is notified and decides how to reallocate the task.
Prepare yourself for the next Hadoop Job Interview with Top 50 Hadoop Interview Questions and Answers.

Hadoop Developer Interview Questions for Fresher

It is not easy to crack Hadoop developer interview but the preparation can do everything. If you are a fresher, learn the Hadoop concepts and prepare properly. Have a good knowledge of the different file systems, Hadoop versions, commands, system security, etc.  Here are few questions that will help you pass the Hadoop developer interview.

31. What are the different configuration files in Hadoop?

Answer: The different configuration files in Hadoop are –

core-site.xml – This configuration file contains Hadoop core configuration settings, for example, I/O settings, very common for MapReduce and HDFS. It uses hostname a port.

mapred-site.xml – This configuration file specifies a framework name for MapReduce by setting mapreduce.framework.name

hdfs-site.xml – This configuration file contains HDFS daemons configuration settings. It also specifies default block permission and replication checking on HDFS.

yarn-site.xml – This configuration file specifies configuration settings for ResourceManager and NodeManager.

32. What are the differences between Hadoop 2 and Hadoop 3?

Answer: Following are the differences between Hadoop 2 and Hadoop 3 –

Top Big Data Interview Questions and Answers33. How can you achieve security in Hadoop?

Answer: Kerberos are used to achieve security in Hadoop. There are 3 steps to access a service while using Kerberos, at a high level. Each step involves a message exchange with a server.

 Authentication – The first step involves authentication of the client to the authentication server, and then provides a time-stamped TGT (Ticket-Granting Ticket) to the client.
 Authorization – In this step, the client uses received TGT to request a service ticket from the TGS (Ticket Granting Server).
 Service Request – It is the final step to achieve security in Hadoop. Then the client uses service ticket to authenticate himself to the server.
34. What is commodity hardware?

Answer: Commodity hardware is a low-cost system identified by less-availability and low-quality. The commodity hardware comprises of RAM as it performs a number of services that require RAM for the execution. One doesn’t require high-end hardware configuration or supercomputers to run Hadoop, it can be run on any commodity hardware.

35. How is NFS different from HDFS?

Answer: There are a number of distributed file systems that work in their own way. NFS (Network File System) is one of the oldest and popular distributed file storage systems whereas HDFS (Hadoop Distributed File System) is the recently used and popular one to handle big data. The main differences between NFS and HDFS are as follows –

Big Data Interview36. How do Hadoop MapReduce works?

There are two phases of MapReduce operation.

Map phase – In this phase, the input data is split by map tasks. The map tasks run in parallel. These split data is used for analysis purpose.
Reduce phase- In this phase, the similar split data is aggregated from the entire collection and shows the result.
37. What is MapReduce? What is the syntax you use to run a MapReduce program?

MapReduce is a programming model in Hadoop for processing large data sets over a cluster of computers, commonly known as HDFS. It is a parallel programming model.

The syntax to run a MapReduce program is – hadoop_jar_file.jar /input_path /output_path.

38. What are the Port Numbers for NameNode, Task Tracker, and Job Tracker?

NameNode – Port 50070
Task Tracker – Port 50060
Job Tracker – Port 50030
39.What are the different file permissions in HDFS for files or directory levels?

Hadoop distributed file system (HDFS) uses a specific permissions model for files and directories. Following user levels are used in HDFS –

Owner
Group
Others.
For each of the user mentioned above following permissions are applicable –

read (r)
write (w)
execute(x).
Above mentioned permissions work differently for files and directories.

For files –

The r permission is for reading a file
The w permission is for writing a file.
For directories –

The r permission lists the contents of a specific directory.
The w permission creates or deletes a directory.
The X permission is for accessing a child directory.
40.What are the basic parameters of a Mapper?

The basic parameters of a Mapper are

LongWritable and Text
Text and IntWritable
Hadoop and Spark are the two most popular big data frameworks. But there is a commonly asked question – do we need Hadoop to run Spark? Watch this video to find the answer to this question.



Hadoop Developer Interview Questions for Experienced

The interviewer has more expectations from an experienced Hadoop developer, and thus his questions are one-level up. So, if you have gained some experience, don’t forget to cover command based, scenario-based, real-experience based questions. Here we bring some sample interview questions for experienced Hadoop developers.

41. How to restart all the daemons in Hadoop?

Answer: To restart all the daemons, it is required to stop all the daemons first. The Hadoop directory contains sbin directory that stores the script files to stop and start daemons in Hadoop.

Use stop daemons command /sbin/stop-all.sh to stop all the daemons and then use /sin/start-all.sh command to start all the daemons again.

42. What is the use of jps command in Hadoop?

Answer: The jps command is used to check if the Hadoop daemons are running properly or not. This command shows all the daemons running on a machine i.e. Datanode, Namenode, NodeManager, ResourceManager etc.

43. Explain the process that overwrites the replication factors in HDFS.

Answer: There are two methods to overwrite the replication factors in HDFS –

Method 1: On File Basis

In this method, the replication factor is changed on the basis of file using Hadoop FS shell. The command used for this is:

$hadoop fs – setrep –w2/my/test_file

Here, test_file is the filename that’s replication factor will be set to 2.

Method 2: On Directory Basis

In this method, the replication factor is changed on directory basis i.e. the replication factor for all the files under a given directory is modified.

$hadoop fs –setrep –w5/my/test_dir

Here, test_dir is the name of the directory, the replication factor for the directory and all the files in it will be set to 5.

44. What will happen with a NameNode that doesn’t have any data?

Answer: A NameNode without any data doesn’t exist in Hadoop. If there is a NameNode, it will contain some data in it or it won’t exist.

45. Explain NameNode recovery process.

Answer: The NameNode recovery process involves the below-mentioned steps to make Hadoop cluster running:

In the first step in the recovery process, file system metadata replica (FsImage) starts a new NameNode.
The next step is to configure DataNodes and Clients. These DataNodes and Clients will then acknowledge new NameNode.
During the final step, the new NameNode starts serving the client on the completion of last checkpoint FsImage loading and receiving block reports from the DataNodes.
Note: Don’t forget to mention, this NameNode recovery process consumes a lot of time on large Hadoop clusters. Thus, it makes routine maintenance difficult. For this reason, HDFS high availability architecture is recommended to use.

46. How Is Hadoop CLASSPATH essential to start or stop Hadoop daemons?

CLASSPATH includes necessary directories that contain jar files to start or stop Hadoop daemons. Hence, setting CLASSPATH is essential to start or stop Hadoop daemons.

However, setting up CLASSPATH every time is not the standard that we follow. Usually CLASSPATH is written inside /etc/hadoop/hadoop-env.sh file. Hence, once we run Hadoop, it will load the CLASSPATH automatically.

47. Why is HDFS only suitable for large data sets and not the correct tool to use for many small files?

This is due to the performance issue of NameNode. Usually, NameNode is allocated with huge space to store metadata for the large-scale file. The metadata is supposed to be a from a single file for optimum space utilization and cost benefit. In case of small size files, NameNode does not utilize the entire space which is a performance optimization issue.

48. Why do we need Data Locality in Hadoop? Explain.

Datasets in HDFS store as blocks in DataNodes the Hadoop cluster. During the execution of a MapReduce job the individual Mapper processes the blocks (Input Splits). If the data does not reside in the same node where the Mapper is executing the job, the data needs to be copied from the DataNode over the network to the mapper DataNode.

Now if a MapReduce job has more than 100 Mapper and each Mapper tries to copy the data from other DataNode in the cluster simultaneously, it would cause serious network congestion which is a big performance issue of the overall system. Hence, data proximity to the computation is an effective and cost-effective solution which is technically termed as Data locality in Hadoop. It helps to increase the overall throughput of the system.

Data locality

Data locality can be of three types:

Data local – In this type data and the mapper resides on the same node. This is the closest proximity of data and the most preferred scenario.
Rack Local – In this scenarios mapper and data reside on the same rack but on the different data nodes.
Different Rack – In this scenario mapper and data reside on the different racks.
49. DFS can handle a large volume of data then why do we need Hadoop framework?

Hadoop is not only for storing large data but also to process those big data. Though DFS(Distributed File System) too can store the data, but it lacks below features-

It is not fault tolerant
Data movement over a network depends on bandwidth.
50. What is Sequencefileinputformat?

Hadoop uses a specific file format which is known as Sequence file. The sequence file stores data in a serialized key-value pair. Sequencefileinputformat is an input format to read sequence files.
=============================================================================================================================================

https://intellipaat.com/interview-question/big-data-hadoop-interview-questions/

Top Answers to Hadoop Interview Questions

1. Compare Hadoop & Spark
Criteria	Hadoop	Spark
Dedicated storage	HDFS	None
Speed of processing	average	excellent
Libraries	Separate tools available	Spark Core, SQL, Streaming, MLlib, GraphX
2. What are real-time industry applications of Hadoop?
Hadoop, well known as Apache Hadoop, is an open-source software platform for scalable and distributed computing of large volumes of data. It provides rapid, high performance and cost-effective analysis of structured and unstructured data generated on digital platforms and within the enterprise. It is used in almost all departments and sectors today.Some of the instances where Hadoop is used:

Managing traffic on streets.
Streaming processing.
Content Management and Archiving Emails.
Processing Rat Brain Neuronal Signals using a Hadoop Computing Cluster.
Fraud detection and Prevention.
Advertisements Targeting Platforms are using Hadoop to capture and analyze click stream, transaction, video and social media data.
Managing content, posts, images and videos on social media platforms.
Analyzing customer data in real-time for improving business performance.
Public sector fields such as intelligence, defense, cyber security and scientific research.
Financial agencies are using Big Data Hadoop to reduce risk, analyze fraud patterns, identify rogue traders, more precisely target their marketing campaigns based on customer segmentation, and improve customer satisfaction.
Getting access to unstructured data like output from medical devices, doctor’s notes, lab results, imaging reports, medical correspondence, clinical data, and financial data.
Read this log to find out how Big Data is transforming real estate now.

Become Hadoop Certified in 85 hrs.
GET CERTIFIED
3. How is Hadoop different from other parallel computing systems?
Hadoop is a distributed file system, which lets you store and handle massive amount of data on a cloud of machines, handling data redundancy. Go through this HDFS content to know how the distributed file system works. The primary benefit is that since data is stored in several nodes, it is better to process it in distributed manner. Each node can process the data stored on it instead of spending time in moving it over the network.

On the contrary, in Relational database computing system, you can query data in real-time, but it is not efficient to store data in tables, records and columns when the data is huge.

Learn about Oracle DBA now.

Hadoop also provides a scheme to build a Column Database with Hadoop HBase, for runtime queries on rows.

Learn more in this HBase Tutorial.

4. What all modes Hadoop can be run in?
Hadoop can run in three modes:

Standalone Mode: Default mode of Hadoop, it uses local file stystem for input and output operations. This mode is mainly used for debugging purpose, and it does not support the use of HDFS. Further, in this mode, there is no custom configuration required for mapred-site.xml, core-site.xml, hdfs-site.xml files. Much faster when compared to other modes.
Pseudo-Distributed Mode (Single Node Cluster): In this case, you need configuration for all the three files mentioned above. In this case, all daemons are running on one node and thus, both Master and Slave node are the same.
Fully Distributed Mode (Multiple Cluster Node): This is the production phase of Hadoop (what Hadoop is known for) where data is used and distributed across several nodes on a Hadoop cluster. Separate nodes are allotted as Master and Slave.
Learn more about Hadoop in this Hadoop Certification course to get ahead in your career!

5. Explain the major difference between HDFS block and InputSplit.
In simple terms, block is the physical representation of data while split is the logical representation of data present in the block. Split acts a s an intermediary between block and mapper.
Suppose we have two blocks:
Block 1: ii nntteell
Block 2: Ii ppaatt
Now, considering the map, it will read first block from ii till ll, but does not know how to process the second block at the same time. Here comes Split into play, which will form a logical group of Block1 and Block 2 as a single block.

It then forms key-value pair using inputformat and records reader and sends map for further processing With inputsplit, if you have limited resources, you can increase the split size to limit the number of maps. For instance, if there are 10 blocks of 640MB (64MB each) and there are limited resources, you can assign ‘split size’ as 128MB. This will form a logical group of 128MB, with only 5 maps executing at a time.

However, if the ‘split size’ property is set to false, whole file will form one inputsplit and is processed by single map, consuming more time when the file is bigger.

6. What is distributed cache and what are its benefits?
Distributed Cache, in Hadoop, is a service by MapReduce framework to cache files when needed. Learn more in this MapReduce Tutorial now. Once a file is cached for a specific job, hadoop will make it available on each data node both in system and in memory, where map and reduce tasks are executing.Later, you can easily access and read the cache file and populate any collection (like array, hashmap) in your code.

Benefits of using distributed cache are:

It distributes simple, read only text/data files and/or complex types like jars, archives and others. These archives are then un-archived at the slave node.
Distributed cache tracks the modification timestamps of cache files, which notifies that the files should not be modified until a job is executing currently.
Give your career a big boost by going through our Hadoop Online Training Videos now!

7. Explain the difference between NameNode, Checkpoint NameNode and BackupNode.
NameNode is the core of HDFS that manages the metadata – the information of what file maps to what block locations and what blocks are stored on what datanode. In simple terms, it’s the data about the data being stored. NameNode supports a directory tree-like structure consisting of all the files present in HDFS on a Hadoop cluster. It uses following files for namespace:
fsimage file- It keeps track of the latest checkpoint of the namespace.
edits file-It is a log of changes that have been made to the namespace since checkpoint.
Checkpoint NameNode has the same directory structure as NameNode, and creates checkpoints for namespace at regular intervals by downloading the fsimage and edits file and margining them within the local directory. The new image after merging is then uploaded to NameNode.
There is a similar node like Checkpoint, commonly known as Secondary Node, but it does not support the ‘upload to NameNode’ functionality.
Backup Node provides similar functionality as Checkpoint, enforcing synchronization with NameNode. It maintains an up-to-date in-memory copy of file system namespace and doesn’t require getting hold of changes after regular intervals. The backup node needs to save the current state in-memory to an image file to create a new checkpoint.
Learn about the various Hadoop components in this Big Data Hadoop Video Tutorial.

8. What are the most common Input Formats in Hadoop?
There are three most common input formats in Hadoop:

Text Input Format: Default input format in Hadoop.
Key Value Input Format: used for plain text files where the files are broken into lines
Sequence File Input Format: used for reading files in sequence
9. Define DataNode and how does NameNode tackle DataNode failures?
DataNode stores data in HDFS; it is a node where actual data resides in the file system. Each datanode sends a heartbeat message to notify that it is alive. If the namenode does noit receive a message from datanode for 10 minutes, it considers it to be dead or out of place, and starts replication of blocks that were hosted on that data node such that they are hosted on some other data node.A BlockReport contains list of all blocks on a DataNode. Now, the system starts to replicate what were stored in dead DataNode.

The NameNode manages the replication of data blocksfrom one DataNode to other. In this process, the replication data transfers directly between DataNode such that the data never passes the NameNode.

Learn Hadoop from Experts! Enrol Today

10. What are the core methods of a Reducer?
The three core methods of a Reducer are:

setup(): this method is used for configuring various parameters like input data size, distributed cache.
public void setup (context)
reduce(): heart of the reducer always called once per key with the associated reduced task
public void reduce(Key, Value, context)
cleanup(): this method is called to clean temporary files, only once at the end of the task
public void cleanup (context)
11. What is SequenceFile in Hadoop?
Extensively used in MapReduce I/O formats, SequenceFile is a flat file containing binary key/value pairs. The map outputs are stored as SequenceFile internally. It provides Reader, Writer and Sorter classes. The three SequenceFile formats are:

Uncompressed key/value records.
Record compressed key/value records – only ‘values’ are compressed here.
Block compressed key/value records – both keys and values are collected in ‘blocks’ separately and compressed. The size of the ‘block’ is configurable.
12. What is Job Tracker role in Hadoop?
Job Tracker’s primary function is resource management (managing the task trackers), tracking resource availability and task life cycle management (tracking the taks progress and fault tolerance).

It is a process that runs on a separate node, not on a DataNode often.
Job Tracker communicates with the NameNode to identify data location.
Finds the best Task Tracker Nodes to execute tasks on given nodes.
Monitors individual Task Trackers and submits the overall job back to the client.
It tracks the execution of MapReduce workloads local to the slave node.
13. What is the use of RecordReader in Hadoop?
Since Hadoop splits data into various blocks, RecordReader is used to read the slit data into single record. For instance, if our input data is split like:
Row1: Welcome to

Row2: Intellipaat
It will be read as “Welcome to Intellipaat” using RecordReader.

14. What is Speculative Execution in Hadoop?
One limitation of Hadoop is that by distributing the tasks on several nodes, there are chances that few slow nodes limit the rest of the program. Tehre are various reasons for the tasks to be slow, which are sometimes not easy to detect. Instead of identifying and fixing the slow-running tasks, Hadoop tries to detect when the task runs slower than expected and then launches other equivalent task as backup. This backup mechanism in Hadoop is Speculative Execution.

It creates a duplicate task on another disk. The same input can be processed multiple times in parallel. When most tasks in a job comes to completion, the speculative execution mechanism schedules duplicate copies of remaining tasks (which are slower) across the nodes that are free currently. When these tasks finish, it is intimated to the JobTracker. If other copies are executing speculatively, Hadoop notifies the TaskTrackers to quit those tasks and reject their output.

Speculative execution is by default true in Hadoop. To disable, set mapred.map.tasks.speculative.execution and mapred.reduce.tasks.speculative.execution
JobConf options to false.

15. What happens if you try to run a Hadoop job with an output directory that is already present?
It will throw an exception saying that the output file directory already exists.

To run the MapReduce job, you need to ensure that the output directory does not exist before in the HDFS.

To delete the directory before running the job, you can use shell:Hadoop fs –rmr /path/to/your/output/Or via the Java API: FileSystem.getlocal(conf).delete(outputDir, true);

Prepare yourself for the MapReduce Interview questions and answers Now

16. How can you debug Hadoop code?
First, check the list of MapReduce jobs currently running. Next, we need to see that there are no orphaned jobs running; if yes, you need to determine the location of RM logs.

Run: “ps –ef | grep –I ResourceManager”
and look for log directory in the displayed result. Find out the job-id from the displayed list and check if there is any error message associated with that job.
On the basis of RM logs, identify the worker node that was involved in execution of the task.
Now, login to that node and run – “ps –ef | grep –iNodeManager”
Examine the Node Manager log. The majority of errors come from user level logs for each map-reduce job.
17. How to configure Replication Factor in HDFS?
hdfs-site.xml is used to configure HDFS. Changing the dfs.replication property in hdfs-site.xml will change the default replication for all files placed in HDFS.
You can also modify the replication factor on a per-file basis using the

Hadoop FS Shell:[training@localhost ~]$ hadoopfs –setrep –w 3 /my/fileConversely,
you can also change the replication factor of all the files under a directory.

[training@localhost ~]$ hadoopfs –setrep –w 3 -R /my/dir
Go through Hadoop Administration Training to learn about Replication Factor In HDFS now!

18. How to compress mapper output but not the reducer output?
To achieve this compression, you should set:

conf.set("mapreduce.map.output.compress", true)
conf.set("mapreduce.output.fileoutputformat.compress", false)
19. What is the difference between Map Side join and Reduce Side Join?
Map side Join at map side is performed data reaches the map. You need a strict structure for defining map side join. On the other hand, Reduce side Join (Repartitioned Join) is simpler than map side join since the input datasets need not be structured. However, it is less efficient as it will have to go through sort and shuffle phases, coming with network overheads.

20. How can you transfer data from Hive to HDFS?
By writing the query:

hive> insert overwrite directory '/' select * from emp;
You can write your query for the data you want to import from Hive to HDFS. The output you receive will be stored in part files in the specified HDFS path.

21. What companies use Hadoop, any idea?
Learn how Big Data and Hadoop have changed the rules of the game in this blog post. Yahoo! (the biggest contributor to the creation of Hadoop) – Yahoo search engine uses Hadoop, Facebook – Developed Hive for analysis,Amazon,Netflix,Adobe,eBay,Spotify,Twitter,Adobe.

================================================================================================================================

https://www.edureka.co/blog/interview-questions/top-50-hadoop-interview-questions-2016/

Top 50 Hadoop Interview Questions You Must Prepare In 2018
Recommended by 513 users
Top 50 Hadoop Interview Questions You Must Prepare In 2018
Shubham Sinha
Shubham Sinha
Published on Dec 24,2018
    27 Comments
 198.3K Views
Bookmark
Email Post
Top 50 Hadoop Interview Questions for 2018
In this Hadoop interview questions blog, we will be covering all the frequently asked questions that will help you ace the interview with their best solutions. But before that, let me tell you how the demand is continuously increasing for Big Data and Hadoop experts. You can check out this skill report which talks about the top technical skills to master in 2018.

Following are a few stats that reflect the growth in the demand for Big Data & Hadoop certification quite accurately:

Big Data will drive $48.6 billion in annual spending by 2019- IDC.
McKinsey predicts that by 2018 there will be a shortage of 1.5M data experts
Average salary of a Big Data Hadoop developer in the US is $135k- Indeed.com
Average annual salary in the United Kingdom is £66,250-£66,750- itjobswatch.co.uk
I would like to draw your attention towards the Big Data revolution. Earlier, organizations were only concerned about operational data, which was less than 20% of the whole data. Later, they realized that analyzing the whole data will give them better business insights & decision-making capability. That was the time when big giants like Yahoo, Facebook, Google, etc. started adopting Hadoop & Big Data related technologies. In fact, nowadays one of every fifth company is moving to Big Data analytics. Hence, the demand for jobs in Big Data Hadoop is rising like anything. Therefore, if you want to boost your career, Hadoop and Spark are just the technology you need. This would always give you a good start either as a fresher or experienced.

Big Data Hadoop Certification Training
Watch The Course Preview
Prepare with these top Hadoop interview questions to get an edge in the burgeoning Big Data market where global and local enterprises, big or small, are looking for the quality Big Data and Hadoop experts. This definitive list of top Hadoop interview questions will take you through the questions and answers around Hadoop Cluster, HDFS, MapReduce, Pig, Hive, HBase. This blog is the gateway to your next Hadoop job.

In case you have come across a few difficult questions in a Hadoop interview and are still confused about the best answer, kindly put those questions in the comment section below. We will be happy to answer them.

Hadoop Interview Questions and Answers | Edureka

 

In the meantime, you can maximize the Big Data Analytics career opportunities that are sure to come your way by taking Hadoop online training with Edureka.  Click below to know more.

Get Skilled in Hadoop

1. What are the basic differences between relational database and HDFS?

Here are the key differences between HDFS and relational database:

RDBMS vs. Hadoop

RDBMS	Hadoop
Data Types	RDBMS relies on the structured data and the schema of the data is always known.	Any kind of data can be stored into Hadoop i.e. Be it structured, unstructured or semi-structured.
Processing	RDBMS provides limited or no processing capabilities.	Hadoop allows us to process the data which is distributed across the cluster in a parallel fashion.
Schema on Read Vs. Write	RDBMS is based on ‘schema on write’ where schema validation is done before loading the data.	On the contrary, Hadoop follows the schema on read policy.
Read/Write Speed	In RDBMS, reads are fast because the schema of the data is already known.	The writes are fast in HDFS because no schema validation happens during HDFS write.
Cost	Licensed software, therefore, I have to pay for the software.	Hadoop is an open source framework. So, I don’t need to pay for the software.
Best Fit Use Case	RDBMS is used for OLTP (Online Trasanctional Processing) system.	Hadoop is used for Data discovery, data analytics or OLAP system.
 
trend Trending Courses in this category
Big Data Hadoop Certification Training
5 (56450)
142k Learners Enrolled Live Class
Best Price
 18,695  21,995
Similar Courses
Apache Spark and Scala Certification TrainingPython Spark Certification Training using PySparkApache Kafka Certification Training

2. Explain “Big Data” and what are five V’s of Big Data?

“Big data” is the term for a collection of large and complex data sets, that makes it difficult to process using relational database management tools or traditional data processing applications. It is difficult to capture, curate, store, search, share, transfer, analyze, and visualize Big data. Big Data has emerged as an opportunity for companies. Now they can successfully derive value from their data and will have a distinct advantage over their competitors with enhanced business decisions making capabilities.

♣ Tip: It will be a good idea to talk about the 5Vs in such questions, whether it is asked specifically or not!

Volume: The volume represents the amount of data which is growing at an exponential rate i.e. in Petabytes and Exabytes. 
Velocity: Velocity refers to the rate at which data is growing, which is very fast. Today, yesterday’s data are considered as old data. Nowadays, social media is a major contributor to the velocity of growing data.
Variety: Variety refers to the heterogeneity of data types. In another word, the data which are gathered has a variety of formats like videos, audios, csv, etc. So, these various formats represent the variety of data.
Veracity: Veracity refers to the data in doubt or uncertainty of data available due to data inconsistency and incompleteness. Data available can sometimes get messy and may be difficult to trust. With many forms of big data, quality and accuracy are difficult to control. The volume is often the reason behind for the lack of quality and accuracy in the data.
Value: It is all well and good to have access to big data but unless we can turn it into a value it is useless. By turning it into value I mean, Is it adding to the benefits of the organizations? Is the organization working on Big Data achieving high ROI (Return On Investment)? Unless, it adds to their profits by working on Big Data, it is useless.
As we know Big Data is growing at an accelerating rate, so the factors associated with it are also evolving. To go through them and understand it in detail, I recommend you to go through Big Data Tutorial blog.

3. What is Hadoop and its components. 

When “Big Data” emerged as a problem, Apache Hadoop evolved as a solution to it. Apache Hadoop is a framework which provides us various services or tools to store and process Big Data. It helps in analyzing Big Data and making business decisions out of it, which can’t be done efficiently and effectively using traditional systems.

♣ Tip: Now, while explaining Hadoop, you should also explain the main components of Hadoop, i.e.:

Storage unit– HDFS (NameNode, DataNode)
Processing framework– YARN (ResourceManager, NodeManager)
4. What are HDFS and YARN?

HDFS (Hadoop Distributed File System) is the storage unit of Hadoop. It is responsible for storing different kinds of data as blocks in a distributed environment. It follows master and slave topology.

♣ Tip: It is recommended to explain the HDFS components too i.e.

NameNode: NameNode is the master node in the distributed environment and it maintains the metadata information for the blocks of data stored in HDFS like block location, replication factors etc.
DataNode: DataNodes are the slave nodes, which are responsible for storing data in the HDFS. NameNode manages all the DataNodes.
YARN (Yet Another Resource Negotiator) is the processing framework in Hadoop, which manages resources and provides an execution environment to the processes.

♣ Tip: Similarly, as we did in HDFS, we should also explain the two components of YARN:   

ResourceManager: It receives the processing requests, and then passes the parts of requests to corresponding NodeManagers accordingly, where the actual processing takes place. It allocates resources to applications based on the needs.
NodeManager: NodeManager is installed on every DataNode and it is responsible for the execution of the task on every single DataNode.
If you want to learn in detail about HDFS & YARN go through Hadoop Tutorial blog.

5. Tell me about the various Hadoop daemons and their roles in a Hadoop cluster.

Generally approach this question by first explaining the HDFS daemons i.e. NameNode, DataNode and Secondary NameNode, and then moving on to the YARN daemons i.e. ResorceManager and NodeManager, and lastly explaining the JobHistoryServer.

NameNode: It is the master node which is responsible for storing the metadata of all the files and directories. It has information about blocks, that make a file, and where those blocks are located in the cluster.
Datanode: It is the slave node that contains the actual data.
Secondary NameNode: It periodically merges the changes (edit log) with the FsImage (Filesystem Image), present in the NameNode. It stores the modified FsImage into persistent storage, which can be used in case of failure of NameNode.
ResourceManager: It is the central authority that manages resources and schedule applications running on top of YARN.
NodeManager: It runs on slave machines, and is responsible for launching the application’s containers (where applications execute their part), monitoring their resource usage (CPU, memory, disk, network) and reporting these to the ResourceManager.
JobHistoryServer: It maintains information about MapReduce jobs after the Application Master terminates.
Hadoop Installation Interview Questions

Hadoop HDFS Interview Questions
6. Compare HDFS with Network Attached Storage (NAS).

In this question, first explain NAS and HDFS, and then compare their features as follows:

Network-attached storage (NAS) is a file-level computer data storage server connected to a computer network providing data access to a heterogeneous group of clients. NAS can either be a hardware or software which provides services for storing and accessing files. Whereas Hadoop Distributed File System (HDFS) is a distributed filesystem to store data using commodity hardware.
In HDFS Data Blocks are distributed across all the machines in a cluster. Whereas in NAS data is stored on a dedicated hardware.
HDFS is designed to work with MapReduce paradigm, where computation is moved to the data. NAS is not suitable for MapReduce since data is stored separately from the computations.
HDFS uses commodity hardware which is cost-effective, whereas a NAS is a high-end storage devices which includes high cost.
7. List the difference between Hadoop 1 and Hadoop 2.

This is an important question and while answering this question, we have to mainly focus on two points i.e. Passive NameNode and YARN architecture.

In Hadoop 1.x, “NameNode” is the single point of failure. In Hadoop 2.x, we have Active and Passive “NameNodes”. If the active “NameNode” fails, the passive “NameNode” takes charge. Because of this, high availability can be achieved in Hadoop 2.x.
Also, in Hadoop 2.x, YARN provides a central resource manager. With YARN, you can now run multiple applications in Hadoop, all sharing a common resource. MRV2 is a particular type of distributed application that runs the MapReduce framework on top of YARN. Other tools can also perform data processing via YARN, which was a problem in Hadoop 1.x.
Hadoop 1.x vs. Hadoop 2.x

Hadoop 1.x	Hadoop 2.x
Passive  NameNode	NameNode is a Single Point of Failure	Active & Passive NameNode
Processing	MRV1 (Job Tracker & Task Tracker)	MRV2/YARN (ResourceManager & NodeManager)
Big Data Hadoop Certification Training
Watch The Course Preview

8. What are active and passive “NameNodes”? 

In HA (High Availability) architecture, we have two NameNodes – Active “NameNode” and Passive “NameNode”.

Active “NameNode” is the “NameNode” which works and runs in the cluster.
Passive “NameNode” is a standby “NameNode”, which has similar data as active “NameNode”.
When the active “NameNode” fails, the passive “NameNode” replaces the active “NameNode” in the cluster. Hence, the cluster is never without a “NameNode” and so it never fails.

9. Why does one remove or add nodes in a Hadoop cluster frequently?

One of the most attractive features of the Hadoop framework is its utilization of commodity hardware. However, this leads to frequent “DataNode” crashes in a Hadoop cluster. Another striking feature of Hadoop Framework is the ease of scale in accordance with the rapid growth in data volume. Because of these two reasons, one of the most common task of a Hadoop administrator is to commission (Add) and decommission (Remove) “Data Nodes” in a Hadoop Cluster.

Read this blog to get a detailed understanding on commissioning and decommissioning nodes in a Hadoop cluster.

10. What happens when two clients try to access the same file in the HDFS?

HDFS supports exclusive writes only.

When the first client contacts the “NameNode” to open the file for writing, the “NameNode” grants a lease to the client to create this file. When the second client tries to open the same file for writing, the “NameNode” will notice that the lease for the file is already granted to another client, and will reject the open request for the second client.

11. How does NameNode tackle DataNode failures?

NameNode periodically receives a Heartbeat (signal) from each of the DataNode in the cluster, which implies DataNode is functioning properly.

A block report contains a list of all the blocks on a DataNode. If a DataNode fails to send a heartbeat message, after a specific period of time it is marked dead.

The NameNode replicates the blocks of dead node to another DataNode using the replicas created earlier.

12. What will you do when NameNode is down?

The NameNode recovery process involves the following steps to make the Hadoop cluster up and running:

Use the file system metadata replica (FsImage) to start a new NameNode. 
Then, configure the DataNodes and clients so that they can acknowledge this new NameNode, that is started.
Now the new NameNode will start serving the client after it has completed loading the last checkpoint FsImage (for metadata information) and received enough block reports from the DataNodes. 
Whereas, on large Hadoop clusters this NameNode recovery process may consume a lot of time and this becomes even a greater challenge in the case of the routine maintenance. Therefore, we have HDFS High Availability Architecture which is covered in the HA architecture blog.

13. What is a checkpoint?

In brief, “Checkpointing” is a process that takes an FsImage, edit log and compacts them into a new FsImage. Thus, instead of replaying an edit log, the NameNode can load the final in-memory state directly from the FsImage. This is a far more efficient operation and reduces NameNode startup time. Checkpointing is performed by Secondary NameNode. 

14. How is HDFS fault tolerant? 

When data is stored over HDFS, NameNode replicates the data to several DataNode. The default replication factor is 3. You can change the configuration factor as per your need. If a DataNode goes down, the NameNode will automatically copy the data to another node from the replicas and make the data available. This provides fault tolerance in HDFS.

15. Can NameNode and DataNode be a commodity hardware? 

The smart answer to this question would be, DataNodes are commodity hardware like personal computers and laptops as it stores data and are required in a large number. But from your experience, you can tell that, NameNode is the master node and it stores metadata about all the blocks stored in HDFS. It requires high memory (RAM) space, so NameNode needs to be a high-end machine with good memory space.

16. Why do we use HDFS for applications having large data sets and not when there are a lot of small files? 

HDFS is more suitable for large amounts of data sets in a single file as compared to small amount of data spread across multiple files. As you know, the NameNode stores the metadata information regarding the file system in the RAM. Therefore, the amount of memory produces a limit to the number of files in my HDFS file system. In other words, too many files will lead to the generation of too much metadata. And, storing these metadata in the RAM will become a challenge. As a thumb rule, metadata for a file, block or directory takes 150 bytes. 

Check Out Our Hadoop Course

17. How do you define “block” in HDFS? What is the default block size in Hadoop 1 and in Hadoop 2? Can it be changed? 

Blocks are the nothing but the smallest continuous location on your hard drive where data is stored. HDFS stores each as blocks, and distribute it across the Hadoop cluster. Files in HDFS are broken down into block-sized chunks, which are stored as independent units.

Hadoop 1 default block size: 64 MB
Hadoop 2 default block size:  128 MB
Yes, blocks can be configured. The dfs.block.size parameter can be used in the hdfs-site.xml file to set the size of a block in a Hadoop environment.

18. What does ‘jps’ command do? 

The ‘jps’ command helps us to check if the Hadoop daemons are running or not. It shows all the Hadoop daemons i.e namenode, datanode, resourcemanager, nodemanager etc. that are running on the machine.

19. How do you define “Rack Awareness” in Hadoop?

Rack Awareness is the algorithm in which the “NameNode” decides how blocks and their replicas are placed, based on rack definitions to minimize network traffic between “DataNodes” within the same rack. Let’s say we consider replication factor 3 (default), the policy is that “for every block of data, two copies will exist in one rack, third copy in a different rack”. This rule is known as the “Replica Placement Policy”.

To know rack awareness in more detail, refer to the HDFS architecture blog. 

20. What is “speculative execution” in Hadoop? 

If a node appears to be executing a task slower, the master node can redundantly execute another instance of the same task on another node. Then, the task which finishes first will be accepted and the other one is killed. This process is called “speculative execution”.

21. How can I restart “NameNode” or all the daemons in Hadoop? 

This question can have two answers, we will discuss both the answers. We can restart NameNode by following methods:

You can stop the NameNode individually using. /sbin /hadoop-daemon.sh stop namenode command and then start the NameNode using. /sbin/hadoop-daemon.sh start namenode command.
To stop and start all the daemons, use. /sbin/stop-all.sh and then use ./sbin/start-all.sh command which will stop all the daemons first and then start all the daemons.
These script files reside in the sbin directory inside the Hadoop directory.

22. What is the difference between an “HDFS Block” and an “Input Split”?

The “HDFS Block” is the physical division of the data while “Input Split” is the logical division of the data. HDFS divides data in blocks for storing the blocks together, whereas for processing, MapReduce divides the data into the input split and assign it to mapper function.

23. Name the three modes in which Hadoop can run.

The three modes in which Hadoop can run are as follows:

Standalone (local) mode: This is the default mode if we don’t configure anything. In this mode, all the components of Hadoop, such NameNode, DataNode, ResourceManager, and NodeManager, run as a single Java process. This uses the local filesystem.
Pseudo-distributed mode: A single-node Hadoop deployment is considered as running Hadoop system in pseudo-distributed mode. In this mode, all the Hadoop services, including both the master and the slave services, were executed on a single compute node.
Fully distributed mode: A Hadoop deployments in which the Hadoop master and slave services run on separate nodes, are stated as fully distributed mode.
Check out more questions on HDFS


Hadoop MapReduce Interview Questions
24. What is “MapReduce”? What is the syntax to run a “MapReduce” program? 

It is a framework/a programming model that is used for processing large data sets over a cluster of computers using parallel programming. The syntax to run a MapReduce program is hadoop_jar_file.jar /input_path /output_path.

If you have any doubt in MapReduce or want to revise your concepts you can refer this MapReduce tutorial.

25. What are the main configuration parameters in a “MapReduce” program?

The main configuration parameters which users need to specify in “MapReduce” framework are:

Job’s input locations in the distributed file system
Job’s output location in the distributed file system
Input format of data
Output format of data
Class containing the map function
Class containing the reduce function
JAR file containing the mapper, reducer and driver classes
26. State the reason why we can’t perform “aggregation” (addition) in mapper? Why do we need the “reducer” for this?

This answer includes many points, so we will go through them sequentially.

We cannot perform “aggregation” (addition) in mapper because sorting does not occur in the “mapper” function. Sorting occurs only on the reducer side and without sorting aggregation cannot be done.
During “aggregation”, we need the output of all the mapper functions which may not be possible to collect in the map phase as mappers may be running on the different machine where the data blocks are stored.
And lastly, if we try to aggregate data at mapper, it requires communication between all mapper functions which may be running on different machines. So, it will consume high network bandwidth and can cause network bottlenecking.
27. What is the purpose of “RecordReader” in Hadoop?

The “InputSplit” defines a slice of work, but does not describe how to access it. The “RecordReader” class loads the data from its source and converts it into (key, value) pairs suitable for reading by the “Mapper” task. The “RecordReader” instance is defined by the “Input Format”.

28. Explain “Distributed Cache” in a “MapReduce Framework”.

Distributed Cache can be explained as, a facility provided by the MapReduce framework to cache files needed by applications. Once you have cached a file for your job, Hadoop framework will make it available on each and every data nodes where you map/reduce tasks are running. Then you can access the cache file as a local file in your Mapper or Reducer job.

29. How do “reducers” communicate with each other? 

This is a tricky question. The “MapReduce” programming model does not allow “reducers” to communicate with each other. “Reducers” run in isolation.

30. What does a “MapReduce Partitioner” do?

A “MapReduce Partitioner” makes sure that all the values of a single key go to the same “reducer”, thus allowing even distribution of the map output over the “reducers”. It redirects the “mapper” output to the “reducer” by determining which “reducer” is responsible for the particular key.

Big Data Hadoop Certification Training
Watch The Course Preview
31. How will you write a custom partitioner?

Custom partitioner for a Hadoop job can be written easily by following the below steps:

Create a new class that extends Partitioner Class
Override method – getPartition, in the wrapper that runs in the MapReduce.
Add the custom partitioner to the job by using method set Partitioner or add the custom partitioner to the job as a config file.
32. What is a “Combiner”? 

A “Combiner” is a mini “reducer” that performs the local “reduce” task. It receives the input from the “mapper” on a particular “node” and sends the output to the “reducer”. “Combiners” help in enhancing the efficiency of “MapReduce” by reducing the quantum of data that is required to be sent to the “reducers”.

33. What do you know about “SequenceFileInputFormat”?

“SequenceFileInputFormat” is an input format for reading within sequence files. It is a specific compressed binary file format which is optimized for passing the data between the outputs of one “MapReduce” job to the input of some other “MapReduce” job.

Sequence files can be generated as the output of other MapReduce tasks and are an efficient intermediate representation for data that is passing from one MapReduce job to another.

More Questions on MapReduce


Apache Pig Interview Questions
34. What are the benefits of Apache Pig over MapReduce? 

Apache Pig is a platform, used to analyze large data sets representing them as data flows developed by Yahoo. It is designed to provide an abstraction over MapReduce, reducing the complexities of writing a MapReduce program.

Pig Latin is a high-level data flow language, whereas MapReduce is a low-level data processing paradigm.
Without writing complex Java implementations in MapReduce, programmers can achieve the same implementations very easily using Pig Latin.
Apache Pig reduces the length of the code by approx 20 times (according to Yahoo). Hence, this reduces the development period by almost 16 times.
Pig provides many built-in operators to support data operations like joins, filters, ordering, sorting etc. Whereas to perform the same function in MapReduce is a humongous task.
Performing a Join operation in Apache Pig is simple. Whereas it is difficult in MapReduce to perform a Join operation between the data sets, as it requires multiple MapReduce tasks to be executed sequentially to fulfill the job.
In addition, pig also provides nested data types like tuples, bags, and maps that are missing from MapReduce.
35. What are the different data types in Pig Latin?

Pig Latin can handle both atomic data types like int, float, long, double etc. and complex data types like tuple, bag and map.

Atomic data types: Atomic or scalar data types are the basic data types which are used in all the languages like string, int, float, long, double, char[], byte[].

Complex Data Types: Complex data types are Tuple, Map and Bag.

To know more about these data types, you can go through our Pig tutorial blog.

36. What are the different relational operations in “Pig Latin” you worked with?

Different relational operators are:

for each
order by
filters
group
distinct
join
limit
37. What is a UDF?

If some functions are unavailable in built-in operators, we can programmatically create User Defined Functions (UDF) to bring those functionalities using other languages like Java, Python, Ruby, etc. and embed it in Script file.

Check out Pig Interview Questions

Apache Hive Interview Questions
38. What is “SerDe” in “Hive”?

Apache Hive is a data warehouse system built on top of Hadoop and is used for analyzing structured and semi-structured data developed by Facebook. Hive abstracts the complexity of Hadoop MapReduce.

The “SerDe” interface allows you to instruct “Hive” about how a record should be processed. A “SerDe” is a combination of a “Serializer” and a “Deserializer”. “Hive” uses “SerDe” (and “FileFormat”) to read and write the table’s row.

To know more about Apache Hive, you can go through this Hive tutorial blog.

trend Trending Courses in this category
Big Data Hadoop Certification Training
5 (56450)
142k Learners Enrolled Live Class
Best Price
 18,695  21,995
Similar Courses
Apache Spark and Scala Certification TrainingPython Spark Certification Training using PySparkApache Kafka Certification Training
39. Can the default “Hive Metastore” be used by multiple users (processes) at the same time?

“Derby database” is the default “Hive Metastore”. Multiple users (processes) cannot access it at the same time. It is mainly used to perform unit tests.

40. What is the default location where “Hive” stores table data?

The default location where Hive stores table data is inside HDFS in /user/hive/warehouse.

Check out Hive Interview Questions

Apache HBase Interview Questions
41. What is Apache HBase?

HBase is an open source, multidimensional, distributed, scalable and a NoSQL database written in Java. HBase runs on top of HDFS (Hadoop Distributed File System) and provides BigTable (Google) like capabilities to Hadoop. It is designed to provide a fault-tolerant way of storing the large collection of sparse data sets. HBase achieves high throughput and low latency by providing faster Read/Write Access on huge datasets.

To know more about HBase you can go through our HBase tutorial blog.

42. What are the components of Apache HBase?

HBase has three major components, i.e. HMaster Server, HBase RegionServer and Zookeeper.

Region Server: A table can be divided into several regions. A group of regions is served to the clients by a Region Server.
HMaster: It coordinates and manages the Region Server (similar as NameNode manages DataNode in HDFS).
ZooKeeper: Zookeeper acts like as a coordinator inside HBase distributed environment. It helps in maintaining server state inside the cluster by communicating through sessions.
To know more, you can go through this HBase architecture blog.

43. What are the components of Region Server?

The components of a Region Server are:

WAL: Write Ahead Log (WAL) is a file attached to every Region Server inside the distributed environment. The WAL stores the new data that hasn’t been persisted or committed to the permanent storage.
Block Cache: Block Cache resides in the top of Region Server. It stores the frequently read data in the memory.
MemStore: It is the write cache. It stores all the incoming data before committing it to the disk or permanent memory. There is one MemStore for each column family in a region.
HFile: HFile is stored in HDFS. It stores the actual cells on the disk.
44. Explain “WAL” in HBase?

Write Ahead Log (WAL) is a file attached to every Region Server inside the distributed environment. The WAL stores the new data that hasn’t been persisted or committed to the permanent storage. It is used in case of failure to recover the data sets.

45. Mention the differences between “HBase” and “Relational Databases”?

HBase is an open source, multidimensional, distributed, scalable and a NoSQL database written in Java. HBase runs on top of HDFS and provides BigTable like capabilities to Hadoop. Let us see the differences between HBase and relational database.

HBase vs. Relational Database

HBase	Relational Database
It is schema-less	It is schema-based database
It is column-oriented data store	It is row-oriented data store
It is used to store de-normalized data	It is used to store normalized data
It contains sparsely populated tables	It contains thin tables
Automated partitioning is done is HBase	There is no such provision or built-in support for partitioning
Check out HBase Interview Questions

Apache Spark Interview Questions
46. What is Apache Spark?

The answer to this question is, Apache Spark is a framework for real-time data analytics in a distributed computing environment. It executes in-memory computations to increase the speed of data processing.

It is 100x faster than MapReduce for large-scale data processing by exploiting in-memory computations and other optimizations.

47. Can you build “Spark” with any particular Hadoop version?

Yes, one can build “Spark” for a specific Hadoop version. Check out this blog to learn more about building YARN and HIVE on Spark. 

48. Define RDD.

RDD is the acronym for Resilient Distribution Datasets – a fault-tolerant collection of operational elements that run parallel. The partitioned data in RDD are immutable and distributed, which is a key component of Apache Spark.

Check out Spark Interview Questions

Oozie & ZooKeeper Interview Questions
49. What is Apache ZooKeeper and Apache Oozie?

Apache ZooKeeper coordinates with various services in a distributed environment. It saves a lot of time by performing synchronization, configuration maintenance, grouping and naming.

Apache Oozie is a scheduler which schedules Hadoop jobs and binds them together as one logical work. There are two kinds of Oozie jobs:

Oozie Workflow: These are the sequential set of actions to be executed. You can assume it as a relay race. Where each athlete waits for the last one to complete his part.
Oozie Coordinator: These are the Oozie jobs which are triggered when the data is made available to it. Think of this as the response-stimuli system in our body. In the same manner, as we respond to an external stimulus, an Oozie coordinator responds to the availability of data and it rests otherwise.
50. How do you configure an “Oozie” job in Hadoop?

“Oozie” is integrated with the rest of the Hadoop stack supporting several types of Hadoop jobs such as “Java MapReduce”, “Streaming MapReduce”, “Pig”, “Hive” and “Sqoop”.

To understand “Oozie” in detail and learn how to configure an “Oozie” job, do check out this introduction to Apache Oozie blog.

Feeling overwhelmed with all the questions the interviewer might ask in your Hadoop interview? Now it is time to go through a series of Hadoop interview questions which covers different aspects of the Hadoop framework. It’s never too late to strengthen your basics. Learn Hadoop from industry experts while working with real-life use cases.

=============================================================================================================================
https://www.digitalvidya.com/blog/big-data-interview-questions/

1 – Define Big Data And Explain The Five Vs of Big Data.

One of the most introductory Big Data questions asked during interviews, the answer to this is fairly straightforward-

Big Data is defined as a collection of large and complex unstructured data sets from where insights are derived from Data Analysis using open-source tools like Hadoop.

The five Vs of Big Data are –

Volume – Amount of data in Petabytes and Exabytes
Variety – Includes formats like videos, audio sources, textual data, etc.
Velocity – Everyday data growth which includes conversations in forums, blogs, social media posts, etc.
Veracity – Degree of accuracy of data available
Value – Deriving insights from collected data to achieve business milestones and new heights
2- How is Hadoop related to Big Data? Describe its components.

Another fairly simple question. Apache Hadoop is an open-source framework used for storing, processing, and analyzing complex unstructured data sets for deriving insights and actionable intelligence for businesses.

The three main components of Hadoop are-

MapReduce – A programming model which processes large datasets in parallel
HDFS – A Java-based distributed file system used for data storage without prior organization
YARN – A framework that manages resources and handles requests from distributed applications
3- Define HDFS and YARN, and talk about their respective components.

The Hadoop Distributed File System (HDFS) is the storage unit that’s responsible for storing different types of data blocks in a distributed environment.

The two main components of HDFS are-

NameNode – A master node that processes metadata information for data blocks contained in the HDFS
DataNode – Nodes which act as slave nodes and simply store the data, for use and processing by the NameNode
The Yet Another Resource Negotiator (YARN) is the processing component of Apache Hadoop and is responsible for managing resources and providing an execution environment for said processes.

The two main components of YARN are-

ResourceManager– Receives processing requests and allocates its parts to respective NodeManagers based on processing needs.
NodeManager– Executes tasks on every single Data Node
4 – Explain the term ‘Commodity Hardware.’

Commodity Hardware refers to the minimal hardware resources and components, collectively needed, to run the Apache Hadoop framework and related data management tools. Apache Hadoop requires 64-512 GB of RAM to execute tasks, and any hardware that supports its minimum requirements is known as ‘Commodity Hardware.’

5 – Define and describe the term FSCK.

FSCK (File System Check) is a command used to run a Hadoop summary report that describes the state of the Hadoop file system. This command is used to check the health of the file distribution system when one or more file blocks become corrupt or unavailable in the system. FSCK only checks for errors in the system and does not correct them, unlike the traditional FSCK utility tool in Hadoop. The command can be run on the whole system or on a subset of files.

The correct command for FSCK is bin/HDFS FSCK.

6 – What is the purpose of the JPS command in Hadoop?

The JBS command is used to test whether all Hadoop daemons are running correctly or not. It specifically checks daemons in Hadoop like the  NameNode, DataNode, ResourceManager, NodeManager, and others.

7 – Name the different commands for starting up and shutting down Hadoop Daemons.

To start up all the Hadoop Deamons together-

./sbin/start-all.sh

To shut down all the Hadoop Daemons together-

./sbin/stop-all.sh

To start up all the daemons related to DFS, YARN, and MR Job History Server, respectively-

./sbin/start-dfs.sh

./sbin/start-yarn.sh

sbin/mr-jobhistory-daemon.sh start history server

To stop the DFS, YARN, and MR Job History Server daemons, respectively-

./sbin/stop-dfs.sh
./sbin/stop-yarn.sh
/sbin/mr-jobhistory-daemon.sh stop historyserver

The final way is to start up and stop all the Hadoop Daemons individually –

./sbin/hadoop-daemon.sh start namenode
./sbin/hadoop-daemon.sh start datanode
./sbin/yarn-daemon.sh start resourcemanager
./sbin/yarn-daemon.sh start nodemanager
./sbin/mr-jobhistory-daemon.sh start historyserver

8 – Why do we need Hadoop for Big Data Analytics?

In most cases, exploring and analyzing large unstructured data sets becomes difficult with the lack of analysis tools. This is where Hadoop comes in as it offers storage, processing, and data collection capabilities. Hadoop stores data in its raw forms without the use of any schema and allows the addition of any number of nodes.

Since Hadoop is open-source and is run on commodity hardware, it is also economically feasible for businesses and organizations to use it for the purpose of Big Data Analytics.

9 – Explain the different features of Hadoop. 

Listed in many Big Data Interview Questions and Answers, the answer to this is-

Open-Source- Open-source frameworks include source code that is available and accessible by all over the World Wide Web. These code snippets can be rewritten, edited, and modifying according to user and analytics requirements.
Scalability – Although Hadoop runs on commodity hardware, additional hardware resources can be added to new nodes.
Data Recovery – Hadoop allows the recovery of data by splitting blocks into three replicas across clusters. Hadoop allows users to recover data from node to node in cases of failure and recovers tasks/nodes automatically during such instances.
User-Friendly – For users who are new to Data Analytics, Hadoop is the perfect framework to use as its user interface is simple and there is no need for clients to handle distributed computing processes as the framework takes care of it.
Data Locality – Hadoop features Data Locality which moves computation to data instead of data to computation. Data is moved to clusters rather than bringing them to the location where MapReduce algorithms are processed and submitted.
10 – Define the Port Numbers for NameNode, Task Tracker and Job Tracker.

NameNode – Port 50070

Task Tracker – Port 50060

Job Tracker – Port 50030

11 – How does HDFS Index Data blocks? Explain.

HDFS indexes data blocks based on their respective sizes. The end of a data block points to the address of where the next chunk of data blocks get stored. The DataNodes store the blocks of data while the NameNode manages these data blocks by using an in-memory image of all the files of said data blocks. Clients receive information related to data blocked from the NameNode.

12 – What are Edge Nodes in Hadoop?

Edge nodes are gateway nodes in Hadoop which act as the interface between the Hadoop cluster and external network. They run client applications and cluster administration tools in Hadoop and are used as staging areas for data transfers to the Hadoop cluster. Enterprise-class storage capabilities (like 900GB SAS Drives with Raid HDD Controllers) is required for Edge Nodes, and a single edge node usually suffices for multiple Hadoop clusters.

13 – What are some of the data management tools used with Edge Nodes in Hadoop?

Oozie, Ambari, Hue, Pig, and Flume are the most common data management tools that work with edge nodes in Hadoop. Other similar tools include HCatalog, BigTop, and Avro.

14 – Explain the core methods of a Reducer.

There are three core methods of a reducer. They are-

setup() – Configures different parameters like distributed cache, heap size, and input data.
reduce() – A parameter that is called once per key with the concerned reduce task
cleanup() – Clears all temporary files and called only at the end of a reducer task.
15 – Talk about the different tombstone markers used for deletion purposes in HBase.

There are three main tombstone markers used for deletion in HBase. They are-

Family Delete Marker – Marks all the columns of a column family
Version Delete Marker – Marks a single version of a single column
Column Delete Marker– Marks all the versions of a single column

=============================================================================================================================
https://dzone.com/articles/top-25-big-data-interview-questions-and-answers-yo

1. What do you understand by the term 'big data'?

Big data deals with complex and large sets of data that cannot be handled using conventional software.

2. How is big data useful for businesses?

Big Data helps organizations understand their customers better by allowing them to draw conclusions from large data sets collected over the years. It helps them make better decisions.

3. What is the Port Number for NameNode?

NameNode – Port 50070

4. What is the function of the JPS command?

 The JPS command is used to test whether all the Hadoop daemons are running correctly or not.

5. What is the command to start up all the Hadoop daemons together?

./sbin/start-all.sh
6. Name a few features of Hadoop.

 Some of the most useful features of Hadoop,

It's open source nature.

User-friendly.

Scalability.

Data locality.

Data recovery.

7. What are the five V’s of Big Data?

 The five V’s of Big data are Volume, Velocity, Variety, Veracity, and Value.

8. What are the components of HDFS?

 The two main components of HDFS are:

Name Node

Data Node

9. How is Hadoop related to Big Data?

Hadoop is a framework that specializes in big data operations.

10. Name a few data management tools used with Edge Nodes?

Oozie, Flume, Ambari, and Hue are some of the data management tools that work with edge nodes in Hadoop.

11. What are the steps to deploy a Big Data solution?

The three steps to deploying a Big Data solution are:

Data Ingestion

Data Storage and

Data Processing

12. How many modes can Hadoop be run in?

 Hadoop can be run in three modes— Standalone mode, Pseudo-distributed mode and fully-distributed mode.

13.    Name the core methods of a reducer

 The three core methods of a reducer are,

setup()

reduce()

cleanup()

14.    What is the command for shutting down all the Hadoop Daemons together?

./sbin/stop-all.sh
15. What is the role of NameNode in HDFS?

NameNode is responsible for processing metadata information for data blocks within HDFS.

16. What is FSCK?

FSCK (File System Check) is a command used to detect inconsistencies and issues in the file.

17. What are the real-time applications of Hadoop?

 Some of the real-time applications of Hadoop are in the fields of:

Content management.

Financial agencies.

Defense and cybersecurity.

Managing posts on social media.

18. What is the function of HDFS?

 The HDFS (Hadoop Distributed File System) is Hadoop’s default storage unit. It is used for storing different types of data in a distributed environment.

19. What is commodity hardware?

Commodity hardware can be defined as the basic hardware resources needed to run the Apache Hadoop framework.

20. Name a few daemons used for testing JPS command.

NameNode

NodeManager

DataNode

ResourceManager

21. What are the most common input formats in Hadoop?

Text Input Format

Key Value Input Format

Sequence File Input Format

22. Name a few companies that use Hadoop.

 Yahoo, Facebook, Netflix, Amazon, and Twitter.

23. What is the default mode for Hadoop?

 Standalone mode is Hadoop's default mode. It is primarily used for debugging purpose.

24. What is the role of Hadoop in big data analytics?

By providing storage and helping in the collection and processing of data, Hadoop helps in the analytics of big data.

25. What are the components of YARN?

 The two main components of YARN (Yet Another Resource Negotiator) are:

Resource Manager

Node Manager
======================================================================================================================

https://www.upgrad.com/blog/15-must-know-big-data-interview-questions-and-answers/


Home > Data > Big Data > 15 Must-know Big Data Interview Questions and Answers
We’re in the era of Big Data and analytics. With data powering everything around us, there has been a sudden surge in demand for skilled data professionals. Organizations are always on the lookout for upskilled individuals who can help them make sense of their heaps of data.

The keyword here is ‘upskilled’ and hence Big Data interviews are not really a cakewalk. There are some essential Big Data interview questions that you must know before you attend one. These will help you find your way through.



The questions have been arranged in an order that will help you pick up from the basics and reach a somewhat advanced level.

Let’s look at the must-know Big Data interview questions and answers!

1. Define Big Data and explain the Vs of Big Data.

This is one of the most introductory yet important Big Data interview questions. The answer to this is quite straightforward:

Big Data can be defined as a collection of complex unstructured or semi-structured data sets which have the potential to deliver actionable insights.

The four Vs of Big Data are –

Volume – Talks about the amount of data
Variety – Talks about the various formats of data
Velocity – Talks about the ever increasing speed at which the data is growing
Veracity – Talks about the degree of accuracy of data available

 Big Data Tutorial for Beginners: All You Need to Know
 

2. How is Hadoop related to Big Data?

When we talk about Big Data, we talk about Hadoop. So, this is another Big Data interview question that you will definitely face in an interview.

Hadoop is an open-source framework for storing, processing, and analyzing complex unstructured data sets for deriving insights and intelligence.

3. Define HDFS and YARN, and talk about their respective components.

Now that we’re in the zone of Hadoop, the next Big Data interview question you might face will revolve around the same.

The HDFS is Hadoop’s default storage unit and is responsible for storing different types of data in a distributed environment.

HDFS has the following two components:

NameNode – This is the master node that has the metadata information for all the data blocks in the HDFS.

DataNode – These are the nodes that act as slave nodes and are responsible for storing the data.

YARN, short for Yet Another Resource Negotiator, is responsible for managing resources and providing an execution environment for the said processes.

The two main components of YARN are –

ResourceManager – Responsible for allocating resources to respective NodeManagers based on the needs.

NodeManager – Executes tasks on every DataNode.

 7 Interesting Big Data Projects You Need To Watch Out
 

4. What do you mean by commodity hardware?

This is yet another Big Data interview question you’re most likely to come across in any interview you sit for.

Commodity Hardware refers to the minimal hardware resources needed to run the Apache Hadoop framework. Any hardware that supports Hadoop’s minimum requirements is known as ‘Commodity Hardware.’

5. Define and describe the term FSCK.

FSCK stands for Filesystem Check. It is a command used to run a Hadoop summary report that describes the state of HDFS. It only checks for errors and does not correct them. This command can be executed on either the whole system or a subset of files.

6. What is the purpose of the JPS command in Hadoop?

The JPS command is used for testing the working of all the Hadoop daemons. It specifically tests daemons like NameNode, DataNode, ResourceManager, NodeManager and more.

(In any Big Data interview, you’re likely to find one question on JPS and its importance.)

 Big Data: Must Know Tools and Technologies
 

7. Name the different commands for starting up and shutting down Hadoop Daemons.

This is one of the most important Big Data interview questions to help the interviewer gauge your knowledge of commands.

To start all the daemons:

./sbin/start-all.sh

To shut down all the daemons:

./sbin/stop-all.sh

8. Why do we need Hadoop for Big Data Analytics?

This Hadoop interview questions test your awareness regarding the practical aspects of Big Data and Analytics.

In most cases, Hadoop helps in exploring and analyzing large and unstructured data sets. Hadoop offers storage, processing and data collection capabilities that help in analytics.

9. Explain the different features of Hadoop.

Listed in many Big Data Interview Questions and Answers, the best answer to this is –

Open-Source – Hadoop is an open-sourced platform. It allows the code to be rewritten or modified according to user and analytics requirements.

Scalability – Hadoop supports the addition of hardware resources to the new nodes.

Data Recovery – Hadoop follows replication which allows the recovery of data in the case of any failure.

Data Locality – This means that Hadoop moves the computation to the data and not the other way round. This way, the whole process speeds up.

10. Define the Port Numbers for NameNode, Task Tracker and Job Tracker.

NameNode – Port 50070

Task Tracker – Port 50060

Job Tracker – Port 50030

11. What do you mean by indexing in HDFS?

HDFS indexes data blocks based on their sizes. The end of a data block points to the address of where the next chunk of data blocks get stored. The DataNodes store the blocks of data while NameNode stores these data blocks.

 Big Data Applications in Pop-Culture
 

12. What are Edge Nodes in Hadoop?

Edge nodes refer to the gateway nodes which act as an interface between Hadoop cluster and the external network. These nodes run client applications and cluster management tools and are used as staging areas as well. Enterprise-class storage capabilities are required for Edge Nodes, and a single edge node usually suffices for multiple Hadoop clusters.

13. What are some of the data management tools used with Edge Nodes in Hadoop?

This Big Data interview question aims to test your awareness regarding various tools and frameworks.

Oozie, Ambari, Pig and Flume are the most common data management tools that work with Edge Nodes in Hadoop.

14. Explain the core methods of a Reducer.

There are three core methods of a reducer. They are-

setup() – This is used to configure different parameters like heap size, distributed cache and input data.

reduce() – A parameter that is called once per key with the concerned reduce task

cleanup() – Clears all temporary files and called only at the end of a reducer task.

15. Talk about the different tombstone markers used for deletion purposes in HBase.

This Big Data interview question dives into your knowledge of HBase and its working.

There are three main tombstone markers used for deletion in HBase. They are-

Family Delete Marker – For marking all the columns of a column family.
Version Delete Marker – For marking a single version of a single column.
Column Delete Marker – For marking all the versions of a single column.

=====================================================================================================================================

https://www.educba.com/big-data-interview-questions/

Part 1 – Big Data Interview Questions (Basic)

This first part covers basic Big Data interview questions and answers

1. What is the meaning of big data and how is it different?

Answer:
Big data is the term to represent all kind of data generated on the internet. On the internet over hundreds of GB of data is generated only by online activity. Here, online activity implies web activity, blogs, text, video/audio files, images, email, social network activity, and so on. Big data can be referred to data created from all these activities. Data generated online is mostly in unstructured form. Big data will also include transactions data in the database, system log files, along with data generated from smart devices such as sensors, IoT, RFID tags, and so on in addition to online activities.
Big data needs specialized systems and software tools to process all unstructured data. In fact, according to some industry estimates almost 85% of data generated on the internet is unstructured. Usually, relational databases have a structured format and the database is centralized. Hence, with RDBMS processing can be quickly done using a query language such as SQL. On the other hand, big data is very large and is distributed across the internet and hence processing big data will need distributed systems and tools to extract information from them. Big data needs specialized tools such as Hadoop, Hive, or others along with high-performance hardware and networks to process them.

2. What are the characteristics of big data?

Answer:
Big data has three main characteristics: Volume, Variety, and Velocity.
Volume characteristic refers to the size of data. Estimates show that over 3 million GB of data is generated every day. Processing this volume of data is not possible in a normal personal computer or in a client-server network in an office environment with limited compute bandwidth and storage capacities. However, cloud services provide solutions to handle big data volumes and process them efficiently using distributed computing architectures.
Variety characteristic refers to the format of big data – structured or unstructured. Traditional RDBMS fits into the structured format. An example of unstructured data format is, a video file format, image files, plain text format, from web document or standard MS Word documents, all have unique formats, and so on. Also to note, RDBMS does not have the capacity to handle unstructured data formats. Further, all this unstructured data must be grouped and consolidated which creates the need for specialized tools and systems. In addition new, data is added each day, or each minute and data grows continuously. Hence big data is more synonymous with variety.
The velocity characteristic refers to the speed at which data is created and the efficiency required to process all the data. For example, Facebook is accessed by over 1.6 billion users in a month. Likewise, there are other social network sites, YouTube, Google services, etc. Such data streams must be processed using queries in real time and must be stored without data loss. Thus, velocity characteristic is important in big data processing.
In addition, other characteristics include veracity and value. Veracity will determine the dependability and reliability of data and value is the value derived by organizations from big data processing.

Let us move to the next Big Data  Interview Questions

3. Why is big data important for organizations?

Answer:
This is the basic Big Data interview question asked in an interview. Big data is important because by processing big data, organizations can obtain insight information related to:
• Cost reduction
• Improvements in products or services
• To understand customer behavior and markets
• Effective decision making
• To become more competitive

4. Name some tools or systems used in big data processing?

Answer:
Big data processing and analysis can be done using,
• Hadoop
• Hive
• Pig
• Mahout
• Flume

Part 2 – Big data Interview Questions (Advanced)

Let us now have a look at the advanced Big data Interview Questions.

5. How can big data support organizations?

Answer:
Big data has the potential to support organizations in many ways. Information extracted from big data can be used in,
• Better coordination with customers and stakeholders and to resolve problems
• Improve reporting and analysis for product or service improvements
• Customize products and services to selected markets
• Ensure better information sharing
• Support in management decisions
• Identify new opportunities, product ideas, and new markets
• Gather data from multiple sources and archive them for future reference
• Maintain databases, systems
• Determine performance metrics
• Understand interdependencies between business functions
• Evaluate organizational performance

6. Explain how big data can be used to increase business value?

Answer:
While understanding the need for analyzing big data, such analysis will help businesses to identify their position in markets, and help businesses to differentiate themselves from their competitors. For example, from the results of big data analysis, organizations can understand the need for customized products or can understand potential markets towards increasing revenue and value. Analyzing big data will involve grouping data from various sources to understand trends and information related to business. When big data analysis is done in a planned manner by gathering data from the right sources, organizations can easily generate business value and revenue by almost 5% to 20%. Some examples of such organizations are Amazon, Linkedin, WalMart, and many others.

Let us move to the next Big Data  Interview Questions

7. What is big data solution implementation?

Answer:
Big data solutions are implemented at a small scale first, based on a concept as appropriate for the business. From the result, which is a prototype solution, the business solution is scaled further. This is the most popular Big Data interview questions asked in a Big Data interview Some of the best practices followedthe in industry include,
• To have clear project objectives and to collaborate wherever necessary
• Gathering data from the right sources
• Ensure the results are not skewed because this can lead to wrong conclusions
• Be prepared to innovate by considering hybrid approaches in processing by including data from structured and unstructured types, include both internal and external data sources
• Understand the impact of big data on existing information flows in the organization

8. What are the steps involved in big data solutions?

Answer:
Big data solutions follow three standard steps in its implementation. They are:
Data ingestion: This step will define the approach to extract and consolidate data from multiple sources. For example, data sources can be social network feeds, CRM, RDBMS, etc. The data extracted from different sources is stored in a Hadoop distributed file system (HDFS).
Data storage: This is the second step, the extracted data is stored. This storage can be in HDFS or HBase (NoSQL database).
Process the data: This is the last step. The data stored must be processed. Processing is done using tools such as Spark, Pig, MapReduce, and others.

=================================================================================================================

https://www.greycampus.com/blog/big-data/top-10-big-data-interview-questions-you-should-be-prepared-for

Top 10 Big Data Interview Questions You Should Be Prepared For

When it comes to landing the job, Big Data engineers (such as you!) are expected to answer tricky interview questions that can leave them flustered and at a loss of words – right? Prior preparation of these top 10 Big Data interview questions will surely help in earning brownie points and set the ball rolling for a fruitful career. 
You may like to prepare for these questions in advance to have the correct answers up your sleeve at the interview table.

Q # 10 Which are the essential Hadoop tools for effective working of Big Data?

Ans: Ambari, “Hive”, “HBase, HDFS (Hadoop Distributed File System), Sqoop, Pig, ZooKeeper, NoSQL, Lucene/SolrSee, Mahout, Avro, Oozie, Flume, GIS Tools, Clouds, and SQL on Hadoop are some of the many Hadoop tools that enhance the performance of Big Data.

For further reference: Click here

Q # 9 It’s true that HDFS is to be used for applications that have large data sets. Why is it not the correct tool to use when there are many small files?

Ans: In most cases, HDFS is not considered as an essential tool for handling bits and pieces of data spread across different small-sized files. The reason behind this is “Namenode” happens to be a very costly and high-performing system. The space allocated to “Namenode” should be used for essential metadata that’s generated for a single file only, instead of numerous small files. While handling large quantities of data attributed to a single file, “Namenode” occupies lesser space and therefore gives off optimized performance. With this in view, HDFS should be used for supporting large data files rather than multiple files with small data.

For further reference: Click here

Suggested read - Fast Data or Big Data: What's right for you?

Q # 8 Which hardware configuration is most beneficial for Hadoop jobs?

Ans: It is best to use dual processors or core machines with 4 / 8 GB RAM and ECC memory for conducting Hadoop operations. Though ECC memory cannot be considered low-end, it is helpful for Hadoop users as it does not deliver any checksum errors. The hardware configuration for different Hadoop jobs would also depend on the process and workflow needs of specific projects and may have to be customized accordingly.

For further reference: Click here

Q # 7 What are the main distinctions between NAS and HDFS?

Ans: 

HDFS needs a cluster of machines for its operations, while NAS runs on just a single machine. Because of this, data redundancy becomes a common feature in HDFS. As the replication protocol is different in case of NAS, the probability of the occurrence of redundant data is much less.
Data is stored on dedicated hardware in NAS. On the other hand, the local drives of the machines in the cluster are used for saving data blocks in HDFS.
Unlike HDFS, Hadoop MapReduce has no role in the processing of NAS data. This is because computation is not moved to data in NAS jobs, and the resultant data files are stored without the same. 
For further reference: Click here

Q # 6 What do you mean by TaskInstance?

Ans: A TaskInstance refers to a specific Hadoop MapReduce work process that runs on any given slave node. Each task instance has its very own JVM process that is created by default for aiding its performance.

For further reference: Click here

Suggested read - Why Big Data is the new competitive advantage?

Q # 5 Why are counters useful in Hadoop?

Ans: Counters are an integral part of any Hadoop job as they are very useful for gathering relevant statistics. For instance, if a particular job consists of 150 node clusters with 150 mappers running at any given point of time, it becomes cumbersome and time-consuming to map and consolidate invalid records for the log entries. Here, counters can be used for keeping a final count of all such records and presenting a single output.

For further reference: Click here

Q # 4 How are file systems checked in HDFS?

Ans: The "fsck" command is used for conducting file system checks in Linux Hadoop and HDFS. It is helpful in blocking names and locations, as well as ascertaining the overall health of any given file system.

For further reference: Click here

Q # 3 What do you mean by “speculative execution” in context to Hadoop?

Ans: In certain cases, where a specific node slows down the performance of any given task, the master node is capable of executing another task instance on a separate note redundantly. In such a scenario, the task that reaches its completion before the other is accepted, while the other is killed. This entire process is referred to as “speculative execution”.

For further reference: Click here

You may also like - How To Bag Top Big Data Jobs

Q # 2 Pig Latin contains different relational operations; name them.

Ans: The important relational operations in Pig Latin are:

group
distinct
join
for each
order by
filters
limit
For further reference: Click here

Q # 1 Where does Hive store table data by default?

Ans: The default location for the storage of table data by Hive is:
hdfs://namenode/user/hive/warehouse

For further reference: Click here

These top 10 Big Data questions that you should be prepared for are just a few of the many that may come your way at a job interview. So, read and research as much as you can, and then, let your enhanced knowledge base do the talking for you.

=====================================================================================================================================
https://www.dezyre.com/article/top-100-hadoop-interview-questions-and-answers-2018/159#mapReduce

Big Data Hadoop Interview Questions and Answers

These are Hadoop Basic Interview Questions and Answers for freshers and experienced.

1. What is the difference between Hadoop and Traditional RDBMS?

Hadoop vs RDBMS

Criteria

Hadoop

RDBMS

Datatypes	Processes semi-structured and unstructured data.	Processes structured data.
Schema	Schema on Read	Schema on Write
Best Fit for Applications	Data discovery and Massive Storage/Processing of Unstructured data.	Best suited for OLTP and complex ACID transactions.
Speed	Writes are Fast	Reads are Fast
Hadoop vs RDBMS
 

Master Big Data with real-world Hadoop Projects


2. What do the four V’s of Big Data denote? Click here to Tweet

IBM has a nice, simple explanation for the four critical features of big data:
a) Volume –Scale of data
b) Velocity –Analysis of streaming data
c) Variety – Different forms of data
d) Veracity –Uncertainty of data

Here is an explanatory video on the four V’s of Big Data



3. How big data analysis helps businesses increase their revenue? Give example.Click here to Tweet

Big data analysis is helping businesses differentiate themselves – for example Walmart the world’s largest retailer in 2014 in terms of revenue - is using big data analytics to increase its sales through better predictive analytics, providing customized recommendations and launching new products based on customer preferences and needs. Walmart observed a significant 10% to 15% increase in online sales for $1 billion in incremental revenue. There are many more companies like Facebook, Twitter, LinkedIn, Pandora, JPMorgan Chase, Bank of America, etc. using big data analytics to boost their revenue.

Here is an interesting video that explains how various industries are leveraging big data analysis to increase their revenue



4. Name some companies that use Hadoop. Click here to tweet this question

Yahoo (One of the biggest user & more than 80% code contributor to Hadoop)
Facebook
Netflix
Amazon
Adobe
eBay
Hulu
Spotify
Rubikloud
Twitter

What companies are you applying to for Hadoop job roles?

Click on this link to view a detailed list of some of the top companies using Hadoop.

FREE eBook on 250 Hadoop Interview Questions and Answers

Hadoop Interview Questions PDF 

5. Differentiate between Structured and Unstructured data.Click here to Tweet

Data which can be stored in traditional database systems in the form of rows and columns, for example the online purchase transactions can be referred to as Structured Data. Data which can be stored only partially in traditional database systems, for example, data in XML records can be referred to as semi structured data. Unorganized and raw data that cannot be categorized as semi structured or structured data is referred to as unstructured data. Facebook updates, Tweets on Twitter, Reviews, web logs, etc. are all examples of unstructured data.

Work on Hands on Projects in Big Data and Data Science

6. On what concept the Hadoop framework works? Click here to Tweet

Hadoop Framework works on the following two core components-

1)HDFS – Hadoop Distributed File System is the java based file system for scalable and reliable storage of large datasets. Data in HDFS is stored in the form of blocks and it operates on the Master Slave Architecture.

2)Hadoop MapReduce-This is a java based programming paradigm of Hadoop framework that provides scalability across various Hadoop clusters. MapReduce distributes the workload into various tasks that can run in parallel. Hadoop jobs perform 2 separate tasks- job. The map job breaks down the data sets into key-value pairs or tuples. The reduce job then takes the output of the map job and combines the data tuples to into smaller set of tuples. The reduce job is always performed after the map job is executed.

Here is a visual that clearly explain the HDFS and Hadoop MapReduce Concepts-

 



Learn Hadoop to become a Microsoft Certified Big Data Engineer.

 

7) What are the main components of a Hadoop Application? Click here to Tweet

Hadoop applications have wide range of technologies that provide great advantage in solving complex business problems.

Core components of a Hadoop application are-

1) Hadoop Common

2) HDFS

3) Hadoop MapReduce

4) YARN

Data Access Components are - Pig and Hive

Data Storage Component is - HBase

Data Integration Components are - Apache Flume, Sqoop, Chukwa

Data Management and Monitoring Components are - Ambari, Oozie and Zookeeper.

Data Serialization Components are - Thrift and Avro

Data Intelligence Components are - Apache Mahout and Drill.

8. What is Hadoop streaming? Click here to Tweet

Hadoop distribution has a generic application programming interface for writing Map and Reduce jobs in any desired programming language like Python, Perl, Ruby, etc. This is referred to as Hadoop Streaming. Users can create and run jobs with any kind of shell scripts or executable as the Mapper or Reducers.

9. What is the best hardware configuration to run Hadoop? Click here to Tweet

The best configuration for executing Hadoop jobs is dual core machines or dual processors with 4GB or 8GB RAM that use ECC memory. Hadoop highly benefits from using ECC memory though it is not low - end. ECC memory is recommended for running Hadoop because most of the Hadoop users have experienced various checksum errors by using non ECC memory. However, the hardware configuration also depends on the workflow requirements and can change accordingly.

10. What are the most commonly defined input formats in Hadoop? Click here to Tweet

The most common Input Formats defined in Hadoop are:

Text Input Format- This is the default input format defined in Hadoop.
Key Value Input Format- This input format is used for plain text files wherein the files are broken down into lines.
Sequence File Input Format- This input format is used for reading files in sequence.
11. What are the steps involved in deploying a big data solution?

i) Data Ingestion – The foremost step in deploying big data solutions is to extract data from different sources which could be an Enterprise Resource Planning System like SAP, any CRM like Salesforce or Siebel , RDBMS like MySQL or Oracle, or could be the log files, flat files, documents, images, social media feeds. This data needs to be stored in HDFS. Data can either be ingested through batch jobs that run every 15 minutes, once every night and so on or through streaming in real-time from 100 ms to 120 seconds.

ii) Data Storage – The subsequent step after ingesting data is to store it either in HDFS or NoSQL database like HBase.  HBase storage works well for random read/write access whereas HDFS is optimized for sequential access.

iii) Data Processing – The ultimate step is to process the data using one of the processing frameworks like mapreduce, spark, pig, hive, etc.

12. How will you choose various file formats for storing and processing data using Apache Hadoop ?

The decision to choose a particular file format is based on the following factors-

i) Schema evolution to add, alter and rename fields.

ii) Usage pattern like accessing 5 columns out of 50 columns vs accessing most of the columns.

iii)Splittability to be processed in parallel.

iv) Read/Write/Transfer performance vs block compression saving storage space

File Formats that can be used with Hadoop - CSV, JSON, Columnar, Sequence files, AVRO, and Parquet file.

CSV Files 

CSV files are an ideal fit for exchanging data between hadoop and external systems. It is advisable not to use header and footer lines when using CSV files.

JSON Files

Every JSON File has its own record. JSON stores both data and schema together in a record and also enables complete schema evolution and splitability. However, JSON files do not support block level compression.

Avro FIiles

This kind of file format is best suited for long term storage with Schema. Avro files store metadata with data and also let you specify independent schema for reading the files.

Parquet Files

A columnar file format that supports block level compression and is optimized for query performance as it allows selection of 10 or less columns from from 50+ columns records.

Test Your Practical Hadoop Knowledge

Scenario Based Hadoop Interview Question - 

You have a file that contains 200 billion URLs. How will you find the first unique URL using Hadoop MapReduce?  

Hadoop Hive Interview Question_Finding Unique URLs Using Hive

 

11. What is Big Data?Click here to Tweet

Big data is defined as the voluminous amount of structured, unstructured or semi-structured data that has huge potential for mining but is so large that it cannot be processed using traditional database systems. Big data is characterized by its high velocity, volume and variety that requires cost effective and innovative methods for information processing to draw meaningful business insights. More than the volume of the data – it is the nature of the data that defines whether it is considered as Big Data or not.

Here is an interesting and explanatory visual on “What is Big Data?”



 

We have further categorized Big Data Interview Questions for Freshers and Experienced-

Hadoop Interview Questions and Answers for Freshers - Q.Nos- 1,2,4,5,6,7,8,9
Hadoop Interview Questions and Answers for Experienced - Q.Nos-3,8,9,10
For a detailed PDF report on Hadoop Salaries - CLICK HERE

Hadoop HDFS Interview Questions and Answers 

1. What is a block and block scanner in HDFS? Click here to Tweet

Block - The minimum amount of data that can be read or written is generally referred to as a “block” in HDFS. The default size of a block in HDFS is 64MB.

Block Scanner - Block Scanner tracks the list of blocks present on a DataNode and verifies them to find any kind of checksum errors. Block Scanners use a throttling mechanism to reserve disk bandwidth on the datanode.

2. Explain the difference between NameNode, Backup Node and Checkpoint NameNode. Click here to Tweet

NameNode: NameNode is at the heart of the HDFS file system which manages the metadata i.e. the data of the files is not stored on the NameNode but rather it has the directory tree of all the files present in the HDFS file system on a hadoop cluster. NameNode uses two files for the namespace-

fsimage file- It keeps track of the latest checkpoint of the namespace.

edits file-It is a log of changes that have been made to the namespace since checkpoint.

Checkpoint Node-

Checkpoint Node keeps track of the latest checkpoint in a directory that has same structure as that of NameNode’s directory. Checkpoint node creates checkpoints for the namespace at regular intervals by downloading the edits and fsimage file from the NameNode and merging it locally. The new image is then again updated back to the active NameNode.

BackupNode:

Backup Node also provides check pointing functionality like that of the checkpoint node but it also maintains its up-to-date in-memory copy of the file system namespace that is in sync with the active NameNode.



3. What is commodity hardware? Click here to Tweet

Commodity Hardware refers to inexpensive systems that do not have high availability or high quality. Commodity Hardware consists of RAM because there are specific services that need to be executed on RAM. Hadoop can be run on any commodity hardware and does not require any super computer s or high end hardware configuration to execute jobs.

4. What is the port number for NameNode, Task Tracker and Job Tracker? Click here to Tweet

NameNode 50070

Job Tracker 50030

Task Tracker 50060

5. Explain about the process of inter cluster data copying. Click here to Tweet

HDFS provides a distributed data copying facility through the DistCP from source to destination. If this data copying is within the hadoop cluster then it is referred to as inter cluster data copying. DistCP requires both source and destination to have a compatible or same version of hadoop.

6. How can you overwrite the replication factors in HDFS? Click here to Tweet

The replication factor in HDFS can be modified or overwritten in 2 ways-

1)Using the Hadoop FS Shell, replication factor can be changed per file basis using the below command-

$hadoop fs –setrep –w 2 /my/test_file (test_file is the filename whose replication factor will be set to 2)

2)Using the Hadoop FS Shell, replication factor of all files under a given directory can be modified using the below command-

3)$hadoop fs –setrep –w 5 /my/test_dir (test_dir is the name of the directory and all the files in this directory will have a replication factor set to 5)

Attend a Hadoop Interview session with experts from the industry!

7. Explain the difference between NAS and HDFS. Click here to Tweet

NAS runs on a single machine and thus there is no probability of data redundancy whereas HDFS runs on a cluster of different machines thus there is data redundancy because of the replication protocol.
NAS stores data on a dedicated hardware whereas in HDFS all the data blocks are distributed across local drives of the machines.
In NAS data is stored independent of the computation and hence Hadoop MapReduce cannot be used for processing whereas HDFS works with Hadoop MapReduce as the computations in HDFS are moved to data.
What technologies are you working on currently? (Java, Datawarehouse, Business Intelligence, ETL, etc.)

8. Explain what happens if during the PUT operation, HDFS block is assigned a replication factor 1 instead of the default value 3. Click here to Tweet

Replication factor is a property of HDFS that can be set accordingly for the entire cluster to adjust the number of times the blocks are to be replicated to ensure high data availability. For every block that is stored in HDFS, the cluster will have n-1 duplicated blocks. So, if the replication factor during the PUT operation is set to 1 instead of the default value 3, then it will have a single copy of data. Under these circumstances when the replication factor is set to 1 ,if the DataNode crashes under any circumstances, then only single copy of the data would be lost.

Implement Hadoop Job for Real-Time Querying

9. What is the process to change the files at arbitrary locations in HDFS? Click here to Tweet

HDFS does not support modifications at arbitrary offsets in the file or multiple writers but files are written by a single writer in append only format i.e. writes to a file in HDFS are always made at the end of the file.

10. Explain about the indexing process in HDFS. Click here to Tweet

Indexing process in HDFS depends on the block size. HDFS stores the last part of the data that further points to the address where the next part of data chunk is stored.

11. What is a rack awareness and on what basis is data stored in a rack? Click here to Tweet

All the data nodes put together form a storage area i.e. the physical location of the data nodes is referred to as Rack in HDFS. The rack information i.e. the rack id of each data node is acquired by the NameNode. The process of selecting closer data nodes depending on the rack information is known as Rack Awareness.

The contents present in the file are divided into data block as soon as the client is ready to load the file into the hadoop cluster. After consulting with the NameNode, client allocates 3 data nodes for each data block. For each data block, there exists 2 copies in one rack and the third copy is present in another rack. This is generally referred to as the Replica Placement Policy.

12. What happens to a NameNode that has no data?

There does not exist any NameNode without data. If it is a NameNode then it should have some sort of data in it.

13. What happens when a user submits a Hadoop job when the NameNode is down- does the job get in to hold or does it fail.

The Hadoop job fails when the NameNode is down.

14. What happens when a user submits a Hadoop job when the Job Tracker is down- does the job get in to hold or does it fail.

The Hadoop job fails when the Job Tracker is down.

15. Whenever a client submits a hadoop job, who receives it?

NameNode receives the Hadoop job which then looks for the data requested by the client and provides the block information. JobTracker takes care of resource allocation of the hadoop job to ensure timely completion.

16. What do you understand by edge nodes in Hadoop?

Edges nodes are the interface between hadoop cluster and the external network. Edge nodes are used for running cluster adminstration tools and client applications.Edge nodes are also referred to as gateway nodes.

We have further categorized Hadoop HDFS Interview Questions for Freshers and Experienced-

Hadoop Interview Questions and Answers for Freshers - Q.Nos- 2,3,7,9,10,11,13,14
Hadoop Interview Questions and Answers for Experienced - Q.Nos- 1,2, 4,5,6,7,8,12,15
Here are few more frequently asked Hadoop HDFS Interview Questions and Answers for Freshers and Experienced

Click here to know more about our Certified Hadoop Developer course

Hadoop MapReduce Interview Questions and Answers

1. Explain the usage of Context Object. Click here to Tweet

Context Object is used to help the mapper interact with other Hadoop systems. Context Object can be used for updating counters, to report the progress and to provide any application level status updates. ContextObject has the configuration details for the job and also interfaces, that helps it to generating the output.

Learn to design Hadoop Architecture

2. What are the core methods of a Reducer? Click here to Tweet

The 3 core methods of a reducer are –

1)setup () – This method of the reducer is used for configuring various parameters like the input data size, distributed cache, heap size, etc.

Function Definition- public void setup (context)

2)reduce () it is heart of the reducer which is called once per key with the associated reduce task.

Function Definition -public void reduce (Key,Value,context)

3)cleanup () - This method is called only once at the end of reduce task for clearing all the temporary files.

Function Definition -public void cleanup (context)

3. Explain about the partitioning, shuffle and sort phase Click here to Tweet

Shuffle Phase-Once the first map tasks are completed, the nodes continue to perform several other map tasks and also exchange the intermediate outputs with the reducers as required. This process of moving the intermediate outputs of map tasks to the reducer is referred to as Shuffling.

Sort Phase- Hadoop MapReduce automatically sorts the set of intermediate keys on a single node before they are given as input to the reducer.

Partitioning Phase-The process that determines which intermediate keys and value will be received by each reducer instance is referred to as partitioning. The destination partition is same for any key irrespective of the mapper instance that generated it.

4. How to write a custom partitioner for a Hadoop MapReduce job? Click here to Tweet

Steps to write a Custom Partitioner for a Hadoop MapReduce Job-

A new class must be created that extends the pre-defined Partitioner Class.
getPartition method of the Partitioner class must be overridden.
The custom partitioner to the job can be added as a config file in the wrapper which runs Hadoop MapReduce or the custom partitioner can be added to the job by using the set method of the partitioner class.
5. What are side data distribution techniques in Hadoop?

The extra read only data required by a hadoop job to process the main dataset is referred to as side data. Hadoop has two side data distribution techniques -

i) Using the job configuration - This technique should not be used for transferring more than few kilobytes of data as it can pressurize the memory usage of hadoop daemons,particularly if your system is running several hadoop jobs.

ii) Distributed Cache - Rather than serializing side data using the job configuration,  it is suggested to distribute data using hadoop's distributed cache mechanism.

We have further categorized Hadoop MapReduce Interview Questions for Freshers and Experienced-

Hadoop Interview Questions and Answers for Freshers - Q.Nos- 2
Hadoop Interview Questions and Answers for Experienced - Q.Nos- 1,3,4,5
Here are a few more frequently asked Hadoop MapReduce Interview Questions and Answers

Hadoop HBase Interview Questions and Answers

1. When should you use HBase and what are the key components of HBase?

HBase should be used when the big data application has –

1)A variable schema

2)When data is stored in the form of collections

3)If the application demands key based access to data while retrieving.

Key components of HBase are –

Region- This component contains memory data store and Hfile.

Region Server-This monitors the Region.

HBase Master-It is responsible for monitoring the region server.

Zookeeper- It takes care of the coordination between the HBase Master component and the client.

Catalog Tables-The two important catalog tables are ROOT and META.ROOT table tracks where the META table is and META table stores all the regions in the system.

2. What are the different operational commands in HBase at record level and table level?

Record Level Operational Commands in HBase are –put, get, increment, scan and delete.

Table Level Operational Commands in HBase are-describe, list, drop, disable and scan.

3. What is Row Key?

Every row in an HBase table has a unique identifier known as RowKey. It is used for grouping cells logically and it ensures that all cells that have the same RowKeys are co-located on the same server. RowKey is internally regarded as a byte array.

4. Explain the difference between RDBMS data model and HBase data model.

RDBMS is a schema based database whereas HBase is schema less data model.

RDBMS does not have support for in-built partitioning whereas in HBase there is automated partitioning.

RDBMS stores normalized data whereas HBase stores de-normalized data.

5. Explain about the different catalog tables in HBase?

The two important catalog tables in HBase, are ROOT and META. ROOT table tracks where the META table is and META table stores all the regions in the system.

6. What is column families? What happens if you alter the block size of ColumnFamily on an already populated database?

The logical deviation of data is represented through a key known as column Family. Column families consist of the basic unit of physical storage on which compression features can be applied. In an already populated database, when the block size of column family is altered, the old data will remain within the old block size whereas the new data that comes in will take the new block size. When compaction takes place, the old data will take the new block size so that the existing data is read correctly.

7. Explain the difference between HBase and Hive.

HBase and Hive both are completely different hadoop based technologies-Hive is a data warehouse infrastructure on top of Hadoop whereas HBase is a NoSQL key value store that runs on top of Hadoop. Hive helps SQL savvy people to run MapReduce jobs whereas HBase supports 4 primary operations-put, get, scan and delete. HBase is ideal for real time querying of big data where Hive is an ideal choice for analytical querying of data collected over period of time.

8. Explain the process of row deletion in HBase.

On issuing a delete command in HBase through the HBase client, data is not actually deleted from the cells but rather the cells are made invisible by setting a tombstone marker. The deleted cells are removed at regular intervals during compaction.

9. What are the different types of tombstone markers in HBase for deletion?

There are 3 different types of tombstone markers in HBase for deletion-

1)Family Delete Marker- This markers marks all columns for a column family.

2)Version Delete Marker-This marker marks a single version of a column.

3)Column Delete Marker-This markers marks all the versions of a column.

10. Explain about HLog and WAL in HBase.

All edits in the HStore are stored in the HLog. Every region server has one HLog. HLog contains entries for edits of all regions performed by a particular Region Server.WAL abbreviates to Write Ahead Log (WAL) in which all the HLog edits are written immediately.WAL edits remain in the memory till the flush period in case of deferred log flush.

We have further categorized Hadoop HBase Interview Questions for Freshers and Experienced-

Hadoop Interview Questions and Answers for Freshers - Q.Nos-1,2,4,5,7
Hadoop Interview Questions and Answers for Experienced - Q.Nos-2,3,6,8,9,10
Here are few more HBase Interview Questions and Answers

Hadoop Sqoop Interview Questions and Answers

1. Explain about some important Sqoop commands other than import and export.

Create Job (--create)

Here we are creating a job with the name my job, which can import the table data from RDBMS table to HDFS. The following command is used to create a job that is importing data from the employee table in the db database to the HDFS file.

$ Sqoop job --create myjob \

--import \

--connect jdbc:mysql://localhost/db \

--username root \

--table employee --m 1

Verify Job (--list)

‘--list’ argument is used to verify the saved jobs. The following command is used to verify the list of saved Sqoop jobs.

$ Sqoop job --list

Inspect Job (--show)

‘--show’ argument is used to inspect or verify particular jobs and their details. The following command and sample output is used to verify a job called myjob.

$ Sqoop job --show myjob

Execute Job (--exec)

‘--exec’ option is used to execute a saved job. The following command is used to execute a saved job called myjob.

$ Sqoop job --exec myjob

2. How Sqoop can be used in a Java program?

The Sqoop jar in classpath should be included in the java code. After this the method Sqoop.runTool () method must be invoked. The necessary parameters should be created to Sqoop programmatically just like for command line.

3. What is the process to perform an incremental data load in Sqoop?

The process to perform incremental data load in Sqoop is to synchronize the modified or updated data (often referred as delta data) from RDBMS to Hadoop. The delta data can be facilitated through the incremental load command in Sqoop.

Incremental load can be performed by using Sqoop import command or by loading the data into hive without overwriting it. The different attributes that need to be specified during incremental load in Sqoop are-

1)Mode (incremental) –The mode defines how Sqoop will determine what the new rows are. The mode can have value as Append or Last Modified.

2)Col (Check-column) –This attribute specifies the column that should be examined to find out the rows to be imported.

3)Value (last-value) –This denotes the maximum value of the check column from the previous import operation.

4. Is it possible to do an incremental import using Sqoop?

Yes, Sqoop supports two types of incremental imports-

1)Append

2)Last Modified

To insert only rows Append should be used in import command and for inserting the rows and also updating Last-Modified should be used in the import command.

5. What is the standard location or path for Hadoop Sqoop scripts?

/usr/bin/Hadoop Sqoop

6. How can you check all the tables present in a single database using Sqoop?

The command to check the list of all tables present in a single database using Sqoop is as follows-

Sqoop list-tables –connect jdbc: mysql: //localhost/user;

7. How are large objects handled in Sqoop?

Sqoop provides the capability to store large sized data into a single field based on the type of data. Sqoop supports the ability to store-

1)CLOB ‘s – Character Large Objects

2)BLOB’s –Binary Large Objects

Large objects in Sqoop are handled by importing the large objects into a file referred as “LobFile” i.e. Large Object File. The LobFile has the ability to store records of huge size, thus each record in the LobFile is a large object.

8. Can free form SQL queries be used with Sqoop import command? If yes, then how can they be used?

Sqoop allows us to use free form SQL queries with the import command. The import command should be used with the –e and – query options to execute free form SQL queries. When using the –e and –query options with the import command the –target dir value must be specified.

9. Differentiate between Sqoop and distCP.

DistCP utility can be used to transfer data between clusters whereas Sqoop can be used to transfer data only between Hadoop and RDBMS.

10. What are the limitations of importing RDBMS tables into Hcatalog directly?

There is an option to import RDBMS tables into Hcatalog directly by making use of –hcatalog –database option with the –hcatalog –table but the limitation to it is that there are several arguments like –as-avrofile , -direct, -as-sequencefile, -target-dir , -export-dir are not supported.

11. Is it sugggested to place the data transfer utility sqoop on an edge node ?

It is not suggested to place sqoop on an edge node or gateway node because the high data transfer volumes could risk the ability of hadoop services on the same node to communicate. Messages are the lifeblood of any hadoop service and high latency could result in the whole node being cut off from the hadoop cluster.

We have further categorized Hadoop Sqoop Interview Questions for Freshers and Experienced-

Hadoop Interview Questions and Answers for Freshers - Q.Nos- 4,5,6,9
Hadoop Interview Questions and Answers for Experienced - Q.Nos- 1,2,3,6,7,8,10
Here are few more frequently asked Sqoop Interview Questions and Answers for Freshers and Experienced

Hadoop Flume Interview Questions and Answers

1) Explain about the core components of Flume.

The core components of Flume are –

Event- The single log entry or unit of data that is transported.

Source- This is the component through which data enters Flume workflows.

Sink-It is responsible for transporting data to the desired destination.

Channel- it is the duct between the Sink and Source.

Agent- Any JVM that runs Flume.

Client- The component that transmits event to the source that operates with the agent.

2) Does Flume provide 100% reliability to the data flow?

Yes, Apache Flume provides end to end reliability because of its transactional approach in data flow.

3) How can Flume be used with HBase?

Apache Flume can be used with HBase using one of the two HBase sinks –

HBaseSink (org.apache.flume.sink.hbase.HBaseSink) supports secure HBase clusters and also the novel HBase IPC that was introduced in the version HBase 0.96.
AsyncHBaseSink (org.apache.flume.sink.hbase.AsyncHBaseSink) has better performance than HBase sink as it can easily make non-blocking calls to HBase.
Working of the HBaseSink –

In HBaseSink, a Flume Event is converted into HBase Increments or Puts. Serializer implements the HBaseEventSerializer which is then instantiated when the sink starts. For every event, sink calls the initialize method in the serializer which then translates the Flume Event into HBase increments and puts to be sent to HBase cluster.

Working of the AsyncHBaseSink-

AsyncHBaseSink implements the AsyncHBaseEventSerializer. The initialize method is called only once by the sink when it starts. Sink invokes the setEvent method and then makes calls to the getIncrements and getActions methods just similar to HBase sink. When the sink stops, the cleanUp method is called by the serializer.

4) Explain about the different channel types in Flume. Which channel type is faster?

The 3 different built in channel types available in Flume are-

MEMORY Channel – Events are read from the source into memory and passed to the sink.

JDBC Channel – JDBC Channel stores the events in an embedded Derby database.

FILE Channel –File Channel writes the contents to a file on the file system after reading the event from a source. The file is deleted only  after the contents are successfully delivered to the sink.

MEMORY Channel is the fastest channel among the three however has the risk of data loss. The channel that you choose completely depends on the nature of the big data application and the value of each event.

5) Which is the reliable channel in Flume to ensure that there is no data loss?

FILE Channel is the most reliable channel among the 3 channels JDBC, FILE and MEMORY.

6) Explain about the replication and multiplexing selectors in Flume.

Channel Selectors are used to handle multiple channels. Based on the Flume header value, an event can be written just to a single channel or to multiple channels. If a channel selector is not specified to the source then by default it is the Replicating selector. Using the replicating selector, the same event is written to all the channels in the source’s channels list. Multiplexing channel selector is used when the application has to send different events to different channels.

7) How multi-hop agent can be setup in Flume?

Avro RPC Bridge mechanism is used to setup Multi-hop agent in Apache Flume.

8) Does Apache Flume provide support for third party plug-ins?

Most of the data analysts use Apache Flume has plug-in based architecture as it can load data from external sources and transfer it to external destinations.

9) Is it possible to leverage real time analysis on the big data collected by Flume directly? If yes, then explain how.

Data from Flume can be extracted, transformed and loaded in real-time into Apache Solr servers using MorphlineSolrSink

10) Differentiate between FileSink and FileRollSink

The major difference between HDFS FileSink and FileRollSink is that HDFS File Sink writes the events into the Hadoop Distributed File System (HDFS) whereas File Roll Sink stores the events into the local file system.

Hadoop Flume Interview Questions and Answers for Freshers - Q.Nos- 1,2,4,5,6,10

Hadoop Flume Interview Questions and Answers for Experienced- Q.Nos-  3,7,8,9

Hadoop Zookeeper Interview Questions and Answers

1) Can Apache Kafka be used without Zookeeper?

It is not possible to use Apache Kafka without Zookeeper because if the Zookeeper is down Kafka cannot serve client request.

2) Name a few companies that use Zookeeper.

Yahoo, Solr, Helprace, Neo4j, Rackspace

3) What is the role of Zookeeper in HBase architecture?

In HBase architecture, ZooKeeper is the monitoring server that provides different services like –tracking server failure and network partitions, maintaining the configuration information, establishing communication between the clients and region servers, usability of ephemeral nodes to identify the available servers in the cluster.

4) Explain about ZooKeeper in Kafka

Apache Kafka uses ZooKeeper to be a highly distributed and scalable system. Zookeeper is used by Kafka to store various configurations and use them across the hadoop cluster in a distributed manner. To achieve distributed-ness, configurations are distributed and replicated throughout the leader and follower nodes in the ZooKeeper ensemble. We cannot directly connect to Kafka by bye-passing ZooKeeper because if the ZooKeeper is down it will not be able to serve the client request.

5) Explain how Zookeeper works

ZooKeeper is referred to as the King of Coordination and distributed applications use ZooKeeper to store and facilitate important configuration information updates. ZooKeeper works by coordinating the processes of distributed applications. ZooKeeper is a robust replicated synchronization service with eventual consistency. A set of nodes is known as an ensemble and persisted data is distributed between multiple nodes.

3 or more independent servers collectively form a ZooKeeper cluster and elect a master. One client connects to any of the specific server and migrates if a particular node fails. The ensemble of ZooKeeper nodes is alive till the majority of nods are working. The master node in ZooKeeper is dynamically selected by the consensus within the ensemble so if the master node fails then the role of master node will migrate to another node which is selected dynamically. Writes are linear and reads are concurrent in ZooKeeper.

6) List some examples of Zookeeper use cases.

Found by Elastic uses Zookeeper comprehensively for resource allocation, leader election, high priority notifications and discovery. The entire service of Found built up of various systems that read and write to   Zookeeper.
Apache Kafka that depends on ZooKeeper is used by LinkedIn
Storm that relies on ZooKeeper is used by popular companies like Groupon and Twitter.
7) How to use Apache Zookeeper command line interface?

ZooKeeper has a command line client support for interactive use. The command line interface of ZooKeeper is similar to the file and shell system of UNIX. Data in ZooKeeper is stored in a hierarchy of Znodes where each znode can contain data just similar to a file. Each znode can also have children just like directories in the UNIX file system.

Zookeeper-client command is used to launch the command line client. If the initial prompt is hidden by the log messages after entering the command, users can just hit ENTER to view the prompt.

8) What are the different types of Znodes?

There are 2 types of Znodes namely- Ephemeral and Sequential Znodes.

The Znodes that get destroyed as soon as the client that created it disconnects are referred to as Ephemeral Znodes.
Sequential Znode is the one in which sequential number is chosen by the ZooKeeper ensemble and is pre-fixed when the client assigns name to the znode.
9) What are watches?

Client disconnection might be troublesome problem especially when we need to keep a track on the state of Znodes at regular intervals. ZooKeeper has an event system referred to as watch which can be set on Znode to trigger an event whenever it is removed, altered or any new children are created below it.

10) What problems can be addressed by using Zookeeper?

In the development of distributed systems, creating own protocols for coordinating the hadoop cluster results in failure and frustration for the developers. The architecture of a distributed system can be prone to deadlocks, inconsistency and race conditions. This leads to various difficulties in making the hadoop cluster fast, reliable and scalable. To address all such problems, Apache ZooKeeper can be used as a coordination service to write correct distributed applications without having to reinvent the wheel from the beginning.

Hadoop ZooKeeper Interview Questions and Answers for Freshers - Q.Nos- 1,2,8,9

Hadoop ZooKeeper Interview Questions and Answers for Experienced- Q.Nos-3,4,5,6,7, 10

Hadoop Pig Interview Questions and Answers

1) What are different modes of execution in Apache Pig?

Apache Pig runs in 2 modes- one is the “Pig (Local Mode) Command Mode” and the other is the “Hadoop MapReduce (Java) Command Mode”. Local Mode requires access to only a single machine where all files are installed and executed on a local host whereas MapReduce requires accessing the Hadoop cluster.

2) Explain about co-group in Pig.

COGROUP operator in Pig is used to work with multiple tuples. COGROUP operator is applied on statements that contain or involve two or more relations. The COGROUP operator can be applied on up to 127 relations at a time. When using the COGROUP operator on two tables at once-Pig first groups both the tables and after that joins the two tables on the grouped columns.

We have further categorized Hadoop Pig Interview Questions for Freshers and Experienced-

Hadoop Interview Questions and Answers for Freshers - Q.No-1
Hadoop Interview Questions and Answers for Experienced - Q.No- 2
Here are a few more frequently asked Pig Hadoop Interview Questions and Answers for Freshers and Experienced

Hadoop Hive Interview Questions and Answers

1) Explain about the SMB Join in Hive.

In SMB join in Hive, each mapper reads a bucket from the first table and the corresponding bucket from the second table and then a merge sort join is performed. Sort Merge Bucket (SMB) join in hive is mainly used as there is no limit on file or partition or table join. SMB join can best be used when the tables are large. In SMB join the columns are bucketed and sorted using the join columns. All tables should have the same number of buckets in SMB join.

2) How can you connect an application, if you run Hive as a server?

When running Hive as a server, the application can be connected in one of the 3 ways-

ODBC Driver-This supports the ODBC protocol

JDBC Driver- This supports the JDBC protocol

Thrift Client- This client can be used to make calls to all hive commands using different programming language like PHP, Python, Java, C++ and Ruby.

3) What does the overwrite keyword denote in Hive load statement?

Overwrite keyword in Hive load statement deletes the contents of the target table and replaces them with the files referred by the file path i.e. the files that are referred by the file path will be added to the table when using the overwrite keyword.

4) What is SerDe in Hive? How can you write your own custom SerDe?

SerDe is a Serializer DeSerializer. Hive uses SerDe to read and write data from tables. Generally, users prefer to write a Deserializer instead of a SerDe as they want to read their own data format rather than writing to it. If the SerDe supports DDL i.e. basically SerDe with parameterized columns and different column types, the users can implement a Protocol based DynamicSerDe rather than writing the SerDe from scratch.

We have further categorized Hadoop Hive Interview Questions for Freshers and Experienced-

Hadoop Hive Interview Questions and Answers for Freshers- Q.Nos-3

Hadoop Hive Interview Questions and Answers for Experienced- Q.Nos-1,2,4

Here are a few more frequently asked  Hadoop Hive Interview Questions and Answers for Freshers and Experienced

Hadoop YARN Interview Questions and Answers

1)What are the stable versions of Hadoop?

Release 2.7.1 (stable)

Release 2.4.1

Release 1.2.1 (stable)

2) What is Apache Hadoop YARN?

YARN is a powerful and efficient feature rolled out as a part of Hadoop 2.0.YARN is a large scale distributed system for running big data applications.

3) Is YARN a replacement of Hadoop MapReduce?

YARN is not a replacement of Hadoop but it is a more powerful and efficient technology that supports MapReduce and is also referred to as Hadoop 2.0 or MapReduce 2.

4) What are the additional benefits YARN brings in to Hadoop?

Effective utilization of the resources as multiple applications can be run in YARN all sharing a common resource.In Hadoop MapReduce there are seperate slots for Map and Reduce tasks whereas in YARN there is no fixed slot. The same container can be used for Map and Reduce tasks leading to better utilization.
YARN is backward compatible so all the existing MapReduce jobs.
Using YARN, one can even run applications that are not based on the MaReduce model
5)  How can native libraries be included in YARN jobs?

There are two ways to include native libraries in YARN jobs-

1)  By setting the -Djava.library.path on the command line  but in this case there are chances that the native libraries might not be loaded correctly and there is possibility of errors.

2) The better option to include native libraries is to the set the LD_LIBRARY_PATH in the .bashrc file.

6) Explain the differences between Hadoop 1.x and Hadoop 2.x

In Hadoop 1.x, MapReduce is responsible for both processing and cluster management whereas in Hadoop 2.x processing is taken care of by other processing models and YARN is responsible for cluster management.
Hadoop 2.x scales better when compared to Hadoop 1.x with close to 10000 nodes per cluster.
Hadoop 1.x has single point of failure problem and whenever the NameNode fails it has to be recovered manually. However, in case of Hadoop 2.x StandBy NameNode overcomes the SPOF problem and whenever the NameNode fails it is configured for automatic recovery.
Hadoop 1.x works on the concept of slots whereas Hadoop 2.x works on the concept of containers and can also run generic tasks.
7) What are the core changes in Hadoop 2.0?

Hadoop 2.x provides an upgrade to Hadoop 1.x in terms of resource management, scheduling and the manner in which execution occurs. In Hadoop 2.x the cluster resource management capabilities work in isolation from the MapReduce specific programming logic. This helps Hadoop to share resources dynamically between multiple parallel processing frameworks like Impala and the core MapReduce component. Hadoop 2.x Hadoop 2.x allows workable and fine grained resource configuration leading to efficient and better cluster utilization so that the application can scale to process larger number of jobs.

8) Differentiate between NFS, Hadoop NameNode and JournalNode.

HDFS is a write once file system so a user cannot update the files once they exist either they can read or write to it. However, under certain scenarios in the enterprise environment like file uploading, file downloading, file browsing or data streaming –it is not possible to achieve all this using the standard HDFS. This is where a distributed file system protocol Network File System (NFS) is used. NFS allows access to files on remote machines just similar to how local file system is accessed by applications.

Namenode is the heart of the HDFS file system that maintains the metadata and tracks where the file data is kept across the Hadoop cluster.

StandBy Nodes and Active Nodes communicate with a group of light weight nodes to keep their state synchronized. These are known as Journal Nodes.

9) What are the modules that constitute the Apache Hadoop 2.0 framework?

Hadoop 2.0 contains four important modules of which 3 are inherited from Hadoop 1.0 and a new module YARN is added to it.

Hadoop Common – This module consists of all the basic utilities and libraries that required by other modules.
HDFS- Hadoop Distributed file system that stores huge volumes of data on commodity machines across the cluster.
MapReduce- Java based programming model for data processing.
YARN- This is a new module introduced in Hadoop 2.0 for cluster resource management and job scheduling.
CLICK HERE to read more about the YARN module in Hadoop 2.x.

10) How is the distance between two nodes defined in Hadoop?

Measuring bandwidth is difficult in Hadoop so network is denoted as a tree in Hadoop. The distance between two nodes in the tree plays a vital role in forming a Hadoop cluster  and is defined by the network topology and java interface DNStoSwitchMapping. The distance is equal to the sum of the distance to the closest common ancestor of both the nodes. The method getDistance(Node node1, Node node2) is used to calculate the distance between two nodes with the assumption that the distance from a node to its parent node is always 1.

We have further categorized Hadoop YARN Interview Questions for Freshers and Experienced-

Hadoop Interview Questions and Answers for Freshers - Q.Nos- 2,3,4,6,7,9
Hadoop Interview Questions and Answers for Experienced - Q.Nos- 1,5,8,10
What other questions do you have regarding your Hadoop career?

Hadoop Testing Interview Questions

1) How will you test data quality ?

The entire data that has been collected could be important but all data is not equal so it is necessary to first define from where the data came , how the data would be used and consumed. Data that will be consumed by vendors or customers within the business ecosystem should be checked for quality and needs to cleaned. This can be done by applying stringent data quality rules and by inspecting different properties like conformity, perfection, repetition, reliability, validity, completeness of data, etc.

2)  What are the challenges that you encounter when testing large datasets?

More data needs to be substantiated.
Testing large datsets requires automation.
Testing options across all platforms need to be defined.
3) What do you understand by MapReduce validation ?

This the subsequent and most important step of the big data testing process. Hadoop developer needs to verify the right implementation of the business logic on every hadoop cluster node and validate the data after executing it on all the nodes to determine -

Appropriate functioning of the MapReduce function.
Validate if rules for data segregation are implemented.
Pairing and creation of key-value pairs.
Hadoop Interview FAQ’s – An Interviewee Should Ask an Interviewer

For many hadoop job seekers, the question from the interviewer – “Do you have any questions for me?” indicates the end of a Hadoop developer job interview. It is always enticing for a Hadoop job seeker to immediately say “No” to the question for the sake of keeping the first impression intact.However, to land a hadoop job or any other job, it is always preferable to fight that urge and ask relevant questions to the interviewer. 

Asking questions related to the Hadoop technology implementation, shows your interest in the open hadoop job role and also conveys your interest in working with the company.Just like any other interview, even hadoop interviews are a two-way street- it helps the interviewer decide whether you have the desired hadoop skills they in are looking for in a hadoop developer, and helps an interviewee decide if that is the kind of big data infrastructure and hadoop technology implementation you want to devote your skills for foreseeable future growth in the big data domain.

Candidates should not be afraid to ask questions to the interviewer. To ease this for hadoop job seekers, DeZyre has collated few hadoop interview FAQ’s that every candidate should ask an interviewer during their next hadoop job interview-

1) What is the size of the biggest hadoop cluster a company X operates?

Asking this question helps a hadoop job seeker understand the hadoop maturity curve at a company.Based on the answer of the interviewer, a candidate can judge how much an organization invests in Hadoop and their enthusiasm to buy big data products from various vendors. The candidate can also get an idea on the hiring needs of the company based on their hadoop infrastructure.

2) For what kind of big data problems, did the organization choose to use Hadoop?

Asking this question to the interviewer shows the candidates keen interest in understanding the reason for hadoop implementation from a business perspective. This question gives the impression to the interviewer that the candidate is not merely interested in the hadoop developer job role but is also interested in the growth of the company.

3) Based on the answer to question no 1, the candidate can ask the interviewer why the hadoop infrastructure is configured in that particular way, why the company chose to use the selected big data tools and how workloads are constructed in the hadoop environment.

Asking this question to the interviewer gives the impression that you are not just interested in maintaining the big data system and developing products around it but are also seriously thoughtful on how the infrastructure can be improved to help business growth and make cost savings.

4) What kind of data the organization works with or what are the HDFS file formats the company uses?

The question gives the candidate an idea on the kind of big data he or she will be handling if selected for the hadoop developer job role. Based on the data, it gives an idea on the kind of analysis they will be required to perform on the data.

5) What is the most complex problem the company is trying to solve using Apache Hadoop?

Asking this question helps the candidate know more about the upcoming projects he or she might have to work and what are the challenges around it. Knowing this beforehand helps the interviewee prepare on his or her areas of weakness.

6) Will I get an opportunity to attend big data conferences? Or will the organization incur any costs involved in taking advanced hadoop or big data certification?

This is a very important question that you should be asking these the interviewer. This helps a candidate understand whether the prospective hiring manager is interested and supportive when it comes to professional development of the employee.

Hadoop Interview FAQ’s – Interviewer Asks an Interviewee

So, you have cleared the technical interview after preparing thoroughly with the help of the Hadoop Interview Questions shared by DeZyre. After an in-depth technical interview, the interviewer might still not be satisfied and would like to test your practical experience in navigating and analysing big data. The expectation of the interviewer is to judge whether you are really interested in the open position and ready to work with the company, regardless of the technical knowledge you have on hadoop technology.

There are quite a few on-going debates in the hadoop community, on the advantages of the various components in the hadoop ecosystem-- for example what is better MapReduce, Pig or Hive or Spark vs. Hadoop or when should a company use MapReduce over other alternative? etc. Interviewee and Interviewer should both be ready to answer such hadoop interview FAQs, as there is no right or wrong answer to these questions.The best possible way to answer these Hadoop interview FAQs is to explain why a particular interviewee favours an option. Answering these hadoop interview FAQs with practical examples as to why the candidate favours an option, demonstrates his or her understanding of the business needs and helps the interviewer judge the flexibility of the candidate to use various big data tools in the hadoop ecosystem.

Here are a few hadoop interview FAQs that are likely to be asked by the interviewer-

1) If you are an experienced hadoop professional then you are likely to be asked questions like  –

The number of nodes you have worked with in a cluster.
Which hadoop distribution have you used in your recent project.
Your experience on working with special configurations like High Availability.
The data volume you have worked with in your most recent project.
What are the various tools you used in the big data and hadoop projects you have worked on?
Your answer to these interview questions will help the interviewer understand your expertise in Hadoop based on the size of the hadoop cluster and number of nodes. Based on the highest volume of data you have handled in your previous projects, interviewer can assess your overall experience in debugging and troubleshooting issues involving huge hadoop clusters.

The number of tools you have worked with help an interviewer judge that you are aware of the overall hadoop ecosystem and not just MapReduce. To be selected, it all depends on how well you communicate the answers to all these questions.

2) What are the challenges that you faced when implementing hadoop projects?

Interviewers are interested to know more about the various issues you have encountered in the past when working with hadoop clusters and understand how you addressed them. The way you answer this question tells a lot about your expertise in troubleshooting and debugging hadoop clusters.The more issues you have encountered, the more probability there is, that you have become an expert in that area of Hadoop. Ensure that you list out all the issues that have trouble-shooted.

3) How were you involved in data modelling, data ingestion, data transformation and data aggregation?

You are likely to be involved in one or more phases when working with big data in a hadoop environment. The answer to this question helps the interviewer understand what kind of tools you are familiar with.If you answer that your focus was mainly on data ingestion then they can expect you to be well-versed with Sqoop and Flume, if you answer that you were involved in data analysis and data transformation then it gives the interviewer an impression that you have expertise in using Pig and Hive.

4) What is your favourite tool in the hadoop ecosystem?

The answer to this question will help the interviewer know more about the big data tools that you are well-versed with and are interested in working with. If you show affinity towards a particular tool then the probability that you will be deployed to work on that particular tool, is more.If you say that you have a good knowledge of all the popular big data tools like pig, hive, HBase, Sqoop, flume then it shows that you have knowledge about the hadoop ecosystem as a whole.

5) In you previous project, did you maintain the hadoop cluster in-house or used hadoop in the cloud?

Most of the organizations still do not have the budget to maintain hadoop cluster in-house and they make use of hadoop in the cloud from various vendors like Amazon, Microsoft, Google, etc. Interviewer gets to know about your familiarity with using hadoop in the cloud because if the company does not have an in-house implementation then hiring a candidate who has knowledge about using hadoop in the cloud is worth it.

BigData Interview Questins asked at Top Tech Companies

1) Write a MapReduce program to add all the elements in a file. (Hadoop Developer Interview Question asked at KPIT)

2)  What is the difference between hashset and hashmap ? (Big Data Interview Question asked at Wipro)

3) Write a Hive program to find the number of employees department wise in an organization. ( Hadoop Developer Interview Question asked at Tripod Technologies)

4) How will you read a CSV file of 10GB and store it in the database as it is in just few seconds ? (Hadoop Interview Question asked at Deutsche Bank)

5) How will a file of 100MB be stored in Hadoop ? (Hadoop Interview Question asked at Deutsche Bank)
=========================================================================================================
https://www.guru99.com/hadoop-mapreduce-interview-question.html
1) What is Hadoop Map Reduce ?

For processing large data sets in parallel across a hadoop cluster, Hadoop MapReduce framework is used.  Data analysis uses a two-step map and reduce process.

2) How Hadoop MapReduce works?

In MapReduce, during the map phase it counts the words in each document, while in the reduce phase it aggregates the data as per the document spanning the entire collection. During the map phase the input data is divided into splits for analysis by map tasks running in parallel across Hadoop framework.

3) Explain what is shuffling in MapReduce ?

The process by which the system performs the sort and transfers the map outputs to the reducer as inputs is known as the shuffle

4) Explain what is distributed Cache in MapReduce Framework ?

Distributed Cache is an important feature provided by map reduce framework. When you want to share some files across all nodes in Hadoop Cluster, DistributedCache  is used.  The files could be an executable jar files or simple properties file.

5) Explain what is NameNode in Hadoop?

NameNode in Hadoop is the node, where Hadoop stores all the file location information in HDFS (Hadoop Distributed File System).  In other words, NameNode is the centrepiece of an HDFS file system.  It keeps the record of all the files in the file system, and tracks the file data across the cluster or multiple machines

6) Explain what is JobTracker in Hadoop? What are the actions followed by Hadoop?

In Hadoop for submitting and tracking MapReduce jobs,  JobTracker is used. Job tracker run on its own JVM process

Job Tracker performs following actions in Hadoop

Client application submit jobs to the job tracker
JobTracker communicates to the Namemode to determine data location
Near the data or with available slots JobTracker locates TaskTracker nodes
On chosen TaskTracker Nodes, it submits the work
When a task fails, Job tracker notify and decides what to do then.
The TaskTracker nodes are monitored by JobTracker
7) Explain what is heartbeat in HDFS?

Heartbeat is referred to a signal used between a data node and Name node, and between task tracker and job tracker, if the Name node or job tracker does not respond to the signal, then it is considered there is some issues with data node or task tracker

8) Explain what combiners is and when you should use a combiner in a MapReduce Job?

To increase the efficiency of MapReduce Program, Combiners are used.  The amount of data can be reduced with the help of combiner’s that need to be transferred across to the reducers. If the operation performed is commutative and associative you can use your reducer code as a combiner.  The execution of combiner is not guaranteed in Hadoop

9) What happens when a datanode fails ?

When a datanode fails

Jobtracker and namenode detect the failure
On the failed node all tasks are re-scheduled
Namenode replicates the users data to another node
10) Explain what is Speculative Execution?


In Hadoop during Speculative Execution a certain number of duplicate tasks are launched.  On different slave node, multiple copies of same map or reduce task can be executed using Speculative Execution. In simple words, if a particular drive is taking long time to complete a task, Hadoop will create a duplicate task on another disk.  Disk that finish the task first are retained and disks that do not finish first are killed.

11) Explain what are the basic parameters of a Mapper?

The basic parameters of a Mapper are

LongWritable and Text
Text and IntWritable
12) Explain what is the function of MapReducer partitioner?

The function of MapReducer partitioner is to make sure that all the value of a single key goes to the same reducer, eventually which helps evenly distribution of the map output over the reducers

13) Explain what is difference between an Input Split and HDFS Block?

Logical division of data is known as Split while physical division of data is known as HDFS Block

14) Explain what happens in textinformat ?

In textinputformat, each line in the text file is a record.  Value is the content of the line while Key is the byte offset of the line. For instance, Key: longWritable, Value: text

15) Mention what are the main configuration parameters that user need to specify to run Mapreduce Job ?

The user of Mapreduce framework needs to specify

Job’s input locations in the distributed file system
Job’s output location in the distributed file system
Input format
Output format
Class containing the map function
Class containing the reduce function
JAR file containing the mapper, reducer and driver classes
16) Explain what is WebDAV in Hadoop?

To support editing and updating files WebDAV is a set of extensions to HTTP.  On most operating system WebDAV shares can be mounted as filesystems , so it is possible to access HDFS as a standard filesystem by exposing HDFS over WebDAV.

17)  Explain what is sqoop in Hadoop ?

To transfer the data between Relational database management (RDBMS) and Hadoop HDFS a tool is used known as Sqoop. Using Sqoop data can be transferred from RDMS like MySQL or Oracle into HDFS as well as exporting data from HDFS file to RDBMS

18) Explain how JobTracker schedules a task ?

The task tracker send out heartbeat messages to Jobtracker usually every few minutes to make sure that JobTracker is active and functioning.  The message also informs JobTracker about the number of available slots, so the JobTracker can stay upto date with where in the cluster work can be delegated

19) Explain what is Sequencefileinputformat?

Sequencefileinputformat is used for reading files in sequence. It is a specific compressed binary file format which is optimized for passing data between the output of one MapReduce job to the input of some other MapReduce job.


20) Explain what does the conf.setMapper Class do ?

Conf.setMapperclass  sets the mapper class and all the stuff related to map job such as reading data and generating a key-value pair out of the mapper

21) Explain what is Hadoop?

It is an open-source software framework for storing data and running applications on clusters of commodity hardware.  It provides enormous processing power and massive storage for any type of data.

22) Mention what is the difference between an RDBMS and Hadoop?

RDBMS	Hadoop
RDBMS is relational database management system	Hadoop is node based flat structure
It used for OLTP processing whereas Hadoop	It is currently used for analytical and for BIG DATA processing
In RDBMS, the database cluster uses the same data files stored in shared storage	In Hadoop, the storage data can be stored independently in each processing node.
You need to preprocess data before storing it	you don’t need to preprocess data before storing it
23) Mention Hadoop core components?

Hadoop core components include,

HDFS
MapReduce
24) What is NameNode in Hadoop?

NameNode in Hadoop is where Hadoop stores all the file location information in HDFS. It is the master node on which job tracker runs and consists of metadata.

25) Mention what are the data components used by Hadoop?

Data components used by Hadoop are

Pig
Hive
26) Mention what is the data storage component used by Hadoop?

The data storage component used by Hadoop is HBase.

27) Mention what are the most common input formats defined in Hadoop?

The most common input formats defined in Hadoop are;

TextInputFormat
KeyValueInputFormat
SequenceFileInputFormat
28) In Hadoop what is InputSplit?

It splits input files into chunks and assign each split to a mapper for processing.

29) For a Hadoop job, how will you write a custom partitioner?

You write a custom partitioner for a Hadoop job, you follow the following path


Create a new class that extends Partitioner Class
Override method getPartition
In the wrapper that runs the MapReduce
Add the custom partitioner to the job by using method set Partitioner Class or – add the custom partitioner to the job as a config file
30) For a job in Hadoop, is it possible to change the number of mappers to be created?

No, it is not possible to change the number of mappers to be created. The number of mappers is determined by the number of input splits.

31) Explain what is a sequence file in Hadoop?

To store binary key/value pairs, sequence file is used. Unlike regular compressed file, sequence file support splitting even when the data inside the file is compressed.

32) When Namenode is down what happens to job tracker?

Namenode is the single point of failure in HDFS so when Namenode is down your cluster will set off.

33) Explain how indexing in HDFS is done?

Hadoop has a unique way of indexing. Once the data is stored as per the block size, the HDFS will keep on storing the last part of the data which say where the next part of the data will be.

34) Explain is it possible to search for files using wildcards?

Yes, it is possible to search for files using wildcards.

35) List out Hadoop’s three configuration files?

The three configuration files are

core-site.xml
mapred-site.xml
hdfs-site.xml
36) Explain how can you check whether Namenode is working beside using the jps command?

Beside using the jps command, to check whether Namenode are working you can also use

/etc/init.d/hadoop-0.20-namenode status.

37) Explain what is “map” and what is "reducer" in Hadoop?

In Hadoop, a map is a phase in HDFS query solving.  A map reads data from an input location, and outputs a key value pair according to the input type.

In Hadoop, a reducer collects the output generated by the mapper, processes it, and creates a final output of its own.

38) In Hadoop, which file controls reporting in Hadoop?

In Hadoop, the hadoop-metrics.properties file controls reporting.

39) For using Hadoop list the network requirements?

For using Hadoop the list of network requirements are:

Password-less SSH connection
Secure Shell (SSH) for launching server processes
40) Mention what is rack awareness?

Rack awareness is the way in which the namenode determines on how to place blocks based on the rack definitions.

41) Explain what is a Task Tracker in Hadoop?

A Task Tracker in Hadoop is a slave node daemon in the cluster that accepts tasks from a JobTracker. It also sends out the heartbeat messages to the JobTracker, every few minutes, to confirm that the JobTracker is still alive.

42) Mention what daemons run on a master node and slave nodes?

Daemons run on Master node is "NameNode"
Daemons run on each Slave nodes are “Task Tracker” and "Data"
43) Explain how can you debug Hadoop code?

The popular methods for debugging Hadoop code are:

By using web interface provided by Hadoop framework
By using Counters
44) Explain what is storage and compute nodes?

The storage node is the machine or computer where your file system resides to store the processing data
The compute node is the computer or machine where your actual business logic will be executed.
45) Mention what is the use of Context Object?

The Context Object enables the mapper to interact with the rest of the Hadoop

system. It includes configuration data for the job, as well as interfaces which allow it to emit output.

46) Mention what is the next step after Mapper or MapTask?

The next step after Mapper or MapTask is that the output of the Mapper are sorted, and partitions will be created for the output.

47) Mention what is the number of default partitioner in Hadoop?

In Hadoop, the default partitioner is a “Hash” Partitioner.

48) Explain what is the purpose of RecordReader in Hadoop?

In Hadoop, the RecordReader loads the data from its source and converts it into (key, value) pairs suitable for reading by the Mapper.

49) Explain how is data partitioned before it is sent to the reducer if no custom partitioner is defined in Hadoop?

If no custom partitioner is defined in Hadoop, then a default partitioner computes a hash value for the key and assigns the partition based on the result.

50) Explain what happens when Hadoop spawned 50 tasks for a job and one of the task failed?

It will restart the task again on some other TaskTracker if the task fails more than the defined limit.

51) Mention what is the best way to copy files between HDFS clusters?

The best way to copy files between HDFS clusters is by using multiple nodes and the distcp command, so the workload is shared.

52) Mention what is the difference between HDFS and NAS?

HDFS data blocks are distributed across local drives of all machines in a cluster while NAS data is stored on dedicated hardware.

53) Mention how Hadoop is different from other data processing tools?

In Hadoop, you can increase or decrease the number of mappers without worrying about the volume of data to be processed.

54) Mention what job does the conf class do?

Job conf class separate different jobs running on the same cluster.  It does the job level settings such as declaring a job in a real environment.

55) Mention what is the Hadoop MapReduce APIs contract for a key and value class?

For a key and value class, there are two Hadoop MapReduce APIs contract

The value must be defining the org.apache.hadoop.io.Writable interface
The key must be defining the org.apache.hadoop.io.WritableComparable interface
56) Mention what are the three modes in which Hadoop can be run?

The three modes in which Hadoop can be run are

Pseudo distributed mode
Standalone (local) mode
Fully distributed mode
57) Mention what does the text input format do?

The text input format will create a line object that is an hexadecimal number.  The value is considered as a whole line text while the key is considered as a line object. The mapper will receive the value as ‘text’ parameter while key as ‘longwriteable’ parameter.

58) Mention how many InputSplits is made by a Hadoop Framework?

Hadoop will make 5 splits

1 split for 64K files
2 split for 65mb files
2 splits for 127mb files
59) Mention what is distributed cache in Hadoop?

Distributed cache in Hadoop is a facility provided by MapReduce framework.  At the time of execution of the job, it is used to cache file.  The Framework copies the necessary files to the slave node before the execution of any task at that node.

60) Explain how does Hadoop Classpath plays a vital role in stopping or starting in Hadoop daemons?

Classpath will consist of a list of directories containing jar files to stop or start daemons.
==============================================================================================================
https://www.edureka.co/blog/interview-questions/hadoop-interview-questions-hdfs-2/

Hadoop Interview Questions and Answers | Edureka
Here, I have provided you a list of the most frequent asked Hadoop HDFS interview questions that will help you to grab the opportunities being generated in the field of Big Data and Hadoop.


Big Data Hadoop Certification Training
 Watch The Course Preview
Hadoop HDFS Interview Questions
1. What are the core components of Hadoop?
Hadoop Core Components

Component	Description
HDFS	Hadoop Distributed file system or HDFS is a Java-based distributed file system that allows us to store Big data across multiple nodes in a Hadoop cluster.
YARN	YARN is the processing framework in Hadoop that allows multiple data processing engines to manage data stored on a single platform and provide Resource management.
2. What are the key features of HDFS?
♣Tip: You should also explain the features briefly while listing different HDFS features. 

Some of the prominent features of HDFS are as follows:

Cost effective and Scalable: HDFS, in general, is deployed on a commodity hardware. So, it is very economical in terms of the cost of ownership of the project. Also, one can scale the cluster by adding more nodes.
Variety and Volume of Data: HDFS is all about storing huge data i.e. Terabytes & Petabytes of data and different kinds of data. So, I can store any type of data into HDFS, be it structured, unstructured or semi structured.
Reliability and Fault Tolerance: HDFS divides the given data into data blocks, replicates it and stores it in a distributed fashion across the Hadoop cluster. This makes HDFS very reliable and fault tolerant. 
High Throughput: Throughput is the amount of work done in a unit time. HDFS provides high throughput access to application data.
3. Explain the HDFS Architecture and list the various HDFS daemons in HDFS cluster?
While listing various HDFS daemons, you should also talk about their roles in brief. Here is how you should answer this question:

Apache Hadoop HDFS Architecture follows a Master/Slave topology where a cluster comprises a single NameNode (Master node or daemon) and all the other nodes are DataNodes (Slave nodes or daemons).  Following daemon runs in HDFS cluster:

NameNode: It is the master daemon that maintains and manages the data block present in the DataNodes. 
DataNode: DataNodes are the slave nodes in HDFS. Unlike NameNode, DataNode is a commodity hardware, that is responsible of storing the data as blocks.
Secondary NameNode: The Secondary NameNode works concurrently with the primary NameNode as a helper daemon. It performs checkpointing. 
4. What is checkpointing in Hadoop?
Checkpointing is the process of combining the Edit Logs with the FsImage (File system Image). It is performed by the Secondary NameNode.

5. What is a NameNode in Hadoop?
The NameNode is the master node that manages all the DataNodes (slave nodes). It records the metadata information regarding all the files stored in the cluster (on the DataNodes), e.g. The location of blocks stored, the size of the files, permissions, hierarchy, etc.

6. What is a DataNode?
DataNodes are the slave nodes in HDFS. It is a commodity hardware that provides storage for the data. It serves the read and write request of the HDFS client. 

7. Is Namenode machine same as DataNode machine as in terms of hardware?
Unlike the DataNodes, a NameNode is a highly available server that manages the File System Namespace and maintains the metadata information. Therefore, NameNode requires higher RAM for storing the metadata information corresponding to the millions of HDFS files in the memory, whereas the DataNode needs to have a higher disk capacity for storing huge data sets. 

8. What is the difference between NAS (Network Attached Storage) and HDFS?
Here are the key differences between NAS and HDFS:

Network-attached storage (NAS) is a file-level computer data storage server connected to a computer network providing data access to a heterogeneous group of clients. NAS can either be a hardware or software which provides a service for storing and accessing files. Whereas Hadoop Distributed File System (HDFS) is a distributed file system to store data using commodity hardware.
In HDFS, data blocks are distributed across all the machines in a cluster. Whereas in NAS, data is stored on a dedicated hardware.
HDFS is designed to work with MapReduce paradigm, where computation is moved to the data. NAS is not suitable for MapReduce since data is stored separately from the computations.
HDFS uses commodity hardware which is cost effective, whereas a NAS is a high-end storage devices which includes high cost.
9. What is the difference between traditional RDBMS and Hadoop?
This question seems to be very easy, but in an interview these simple questions matter a lot. So, here is how you can answer the very question:

RDBMS	Hadoop
Data Types	RDBMS relies on the structured data and the schema of the data is always known.	Any kind of data can be stored into Hadoop i.e. Be it structured, unstructured or semi-structured.
Processing	RDBMS provides limited or no processing capabilities.	Hadoop allows us to process the data which is distributed across the cluster in a parallel fashion.
Schema on Read Vs. Write	RDBMS is based on ‘schema on write’ where schema validation is done before loading the data.	On the contrary, Hadoop follows the schema on read policy.
Read/Write Speed	In RDBMS, reads are fast because the schema of the data is already known.	The writes are fast in HDFS because no schema validation happens during HDFS write.
Cost	Licensed software, therefore, I have to pay for the software.	Hadoop is an open source framework. So, I don’t need to pay for the software.
Best Fit Use Case	RDBMS is used for OLTP (Online Trasanctional Processing) system.	Hadoop is used for Data discovery, data analytics or OLAP system.
10. What is throughput? How does HDFS provides good throughput?
Throughput is the amount of work done in a unit time. HDFS provides good throughput because:

The HDFS is based on Write Once and Read Many Model, it simplifies the data coherency issues as the data written once can’t be modified and therefore, provides high throughput data access.
In Hadoop, the computation part is moved towards the data which reduces the network congestion and therefore, enhances the overall system throughput.
11. What is Secondary NameNode? Is it a substitute or back up node for the NameNode?
Here, you should also mention the function of the Secondary NameNode while answering the later part of this question so as to provide clarity:

A Secondary NameNode is a helper daemon that performs checkpointing in HDFS. No, it is not a backup or a substitute node for the NameNode. It periodically, takes the edit logs (meta data file) from NameNode and merges it with the FsImage (File system Image) to produce an updated FsImage as well as to prevent the Edit Logs from becoming too large.

12. What do you mean by meta data in HDFS? List the files associated with metadata.
The metadata in HDFS represents the structure of HDFS directories and files. It also includes the various information regarding HDFS directories and files such as ownership, permissions, quotas, and replication factor.

♣Tip: While listing the files associated with metadata, give a one line definition of each metadata file.

There are two files associated with metadata present in the NameNode:

FsImage: It contains the complete state of the file system namespace since the start of the NameNode.
EditLogs: It contains all the recent modifications made to the file system with respect to the recent FsImage.
trend Trending Courses in this category
Big Data Hadoop Certification Training
5 (56450)
142k Learners Enrolled Live Class
Best Price
 18,695  21,995
Similar Courses
Apache Spark and Scala Certification TrainingPython Spark Certification Training using PySparkApache Kafka Certification Training
13. What is the problem in having lots of small files in HDFS?
As we know, the NameNode stores the metadata information regarding file system in the RAM. Therefore, the amount of memory produces a limit to the number of files in my HDFS file system. In other words, too much of files will lead to the generation of too much meta data and storing these meta data in the RAM will become a challenge. As a thumb rule, metadata for a file, block or directory takes 150 bytes.  

14. What is a heartbeat in HDFS?
Heartbeats in HDFS are the signals that are sent by DataNodes to the NameNode to indicate that it is functioning properly (alive). By default, the heartbeat interval is 3 seconds, which can be configured using dfs.heartbeat.interval in hdfs-site.xml.

15. How would you check whether your NameNode is working or not?
There are many ways to check the status of the NameNode. Most commonly, one uses the jps command to check the status of all the daemons running in the HDFS. Alternatively, one can visit the NameNode’s Web UI for the same. 

16. What is a block?
You should begin the answer with a general definition of a block. Then, you should explain in brief about the blocks present in HDFS and also mention their default size. 

Blocks are the smallest continuous location on your hard drive where data is stored. HDFS stores each file as blocks, and distribute it across the Hadoop cluster. The default size of a block in HDFS is 128 MB (Hadoop 2.x) and 64 MB (Hadoop 1.x) which is much larger as compared to the Linux system where the block size is 4KB. The reason of having this huge block size is to minimize the cost of seek and reduce the meta data information generated per block.

17. Suppose there is file of size 514 MB stored in HDFS (Hadoop 2.x) using default block size configuration and default replication factor. Then, how many blocks will be created in total and what will be the size of each block?

Default block size in Hadoop 2.x is 128 MB. So, a file of size 514 MB will be divided into 5 blocks ( 514 MB/128 MB) where the first four blocks will be of 128 MB and the last block will be of 2 MB only. Since, we are using the default replication factor i.e. 3, each block will be replicated thrice. Therefore, we will have 15 blocks in total where 12 blocks will be of size 128 MB each and 3 blocks of size 2 MB each.

18. How to copy a file into HDFS with a different block size to that of existing block size configuration?
♣Tip: You should start the answer with the command for changing the block size and then, you should explain the whole procedure with an example. This is how you should answer this question:

Yes, one can copy a file into HDFS with a different block size by using ‘-Ddfs.blocksize=block_size’ where the block_size is specified in Bytes.

Let me explain it with an example: Suppose, I want to copy a file called test.txt of size, say of 120 MB, into the HDFS and I want the block size for this file to be 32 MB (33554432 Bytes) instead of the default (128 MB). So, I would issue the following command:

hadoop fs -Ddfs.blocksize=33554432 -copyFromLocal /home/edureka/test.txt /sample_hdfs

Now, I can check the HDFS block size associated with this file by:

hadoop fs -stat %o /sample_hdfs/test.txt

Else, I can also use the NameNode web UI for seeing the HDFS directory.

♣Tip: You can go through the blog on Hadoop Shell Commands where you will find various Hadoop commands, explained with an example. 

19. Can you change the block size of HDFS files?
Yes, I can change the block size of HDFS files by changing the default size parameter present in hdfs-site.xml. But, I will have to restart the cluster for this property change to take effect.

20. What is a block scanner in HDFS?
Block scanner runs periodically on every DataNode to verify whether the data blocks stored are correct or not. The following steps will occur when a corrupted data block is detected by the block scanner:

First, the DataNode will report about the corrupted block to the NameNode.
Then, NameNode will start the process of creating a new replica using the correct replica of the corrupted block present in other DataNodes.
The corrupted data block will not be deleted until the replication count of the correct replicas matches with the replication factor (3 by default).
This whole process allows HDFS to maintain the integrity of the data when a client performs a read operation. One can check the block scanner report using the DataNode’s web interface- localhost:50075/blockScannerReport as shown below:

Block Scanner Report - Hadoop HDFS Interview Questions - Edureka

Fig. – Block Scanner Report – Hadoop HDFS Interview Question


Big Data Hadoop Certification Training
 Watch The Course Preview
21. HDFS stores data using commodity hardware which has higher chances of failures. So, How HDFS ensures the Fault Tolerance capability of the system?
♣Tip: Basically, this question is regarding replication of blocks in Hadoop and how it helps in providing fault tolerance.  

HDFS provides fault tolerance by replicating the data blocks and distributing it among different DataNodes across the cluster. By default, this replication factor is set to 3 which is configurable. So, if I store a file of 1 GB in HDFS where the replication factor is set to default i.e. 3, it will finally occupy a total space of 3 GB because of the replication. Now, even if a DataNode fails or a data block gets corrupted, I can retrieve the data from other replicas stored in different DataNodes.  

22. Replication causes data redundancy and consume a lot of space, then why is it pursued in HDFS?
Replication is pursued in HDFS to provide the fault tolerance. And, yes, it will lead to the consumption of a lot of space, but one can always add more nodes to the cluster if required. By the way, in practical clusters, it is very rare to have free space issues as the very first reason to deploy HDFS was to store huge data sets. Also, one can change the replication factor to save HDFS space or use different codec provided by Hadoop to compress the data.  

23. Can we have different replication factor of the existing files in HDFS?
♣Tip: You should always answer such type of questions by taking an example to provide clarity. 

Yes, one can have different replication factor for the files existing in HDFS. Suppose, I have a file named test.xml stored within the sample directory in my HDFS with the replication factor set to 1. Now, the command for changing the replication factor of text.xml file to 3 is:

hadoop fs -setrwp -w 3 /sample/test.xml

Finally, I can check whether the replication factor has been changed or not by using following command:

hadoop fs -ls /sample

or 

hadoop fsck /sample/test.xml -files

24. What is a rack awareness algorithm and why is it used in Hadoop?
Rack Awareness algorithm in Hadoop ensures that all the block replicas are not stored on the same rack or a single rack. Considering the replication factor is 3, the Rack Awareness Algorithm says that the first replica of a block will be stored on a local rack and the next two replicas will be stored on a different (remote) rack but, on a different DataNode within that (remote) rack. There are two reasons for using Rack Awareness:

To improve the network performance: In general, you will find greater network bandwidth between machines in the same rack than the machines residing in different rack. So, the Rack Awareness helps to reduce write traffic in between different racks and thus provides a better write performance. 
To prevent loss of data: I don’t have to worry about the data even if an entire rack fails because of the switch failure or power failure. And if one thinks about it, it will make sense, as it is said that never put all your eggs in the same basket.
25. How data or a file is written into HDFS?
The best way to answer this question is to take an example of a client and list the steps that will happen while performing the write without going into much of the details:

Suppose a client wants to write a file into HDFS. So, the following steps will be performed internally during the whole HDFS write process:

The client will divide the files into blocks and will send a write request to the NameNode.
For each block, the NameNode will provide the client a list containing the IP address of DataNodes (depending on replication factor, 3 by default) where the data block has to be copied eventually.
The client will copy the first block into the first DataNode and then the other copies of the block will be replicated by the DataNodes themselves in a sequential manner.
♣Tip: I would suggest you to go through the blog on HDFS Read/Write Architecture where the whole process of HDFS Read/Write has been explained in detail with images.

Do you prefer watching a video tutorial to understand & prepare yourself for your Hadoop interview? Here is our video on the top 50 Hadoop interview questions. 

26. Can you modify the file present in HDFS?
No, I cannot modify the files already present in HDFS, as HDFS follows Write Once Read Many model. But, I can always append data into the existing HDFS file.

27. Can multiple clients write into an HDFS file concurrently?
No, multiple clients can’t write into an HDFS file concurrently. HDFS follows single writer multiple reader model. The client which opens a file for writing is granted a lease by the NameNode. Now suppose, in the meanwhile, some other client wants to write into that very file and asks NameNode for the write permission. At first, the NameNode will check whether the lease for writing into that very particular file has been granted to someone else or not. Then, it will reject the write request of the other client if the lease has been acquired by someone else, who is currently writing into the very file.

29. Does HDFS allow a client to read a file which is already opened for writing?
Basically, the intent of asking this question is to know about the constraints associated with reading a file which is currently being written by some client. You may answer this question in following manner:

Yes, one can read the file which is already opened. But, the problem in reading a file which is currently being written lies in the consistency of the data i.e. HDFS does not provide the surety that the data which has been written into the file will be visible to a new reader before the file has been closed. For this, one can call the hflush operation explicitly which will push all the data in the buffer into the write pipeline and then the hflush operation will wait for the acknowledgements from the DataNodes. Hence, by doing this the data that has been written into the file before the hflush operation will be visible to the readers for sure.   

30. Define Data Integrity? How does HDFS ensure data integrity of data blocks stored in HDFS?
Data Integrity talks about the correctness of the data. It is very important for us to have a guarantee or assurance that the data stored in HDFS is correct. However, there is always a slight chance that the data will get corrupted during I/O operations on the disk. HDFS creates the checksum for all the data written to it and verifies the data with the checksum during read operation by default. Also, each DataNode runs a block scanner periodically, which verifies the correctness of the data blocks stored in the HDFS.

31. What do you mean by the High Availability of a NameNode? How is it achieved?
NameNode used to be single point of failure in Hadoop 1.x where the whole Hadoop cluster becomes unavailable as soon as NameNode is down. In other words, High Availability of the NameNode talks about the very necessity of a NameNode to be active for serving the requests of Hadoop clients.

To solve this Single Point of Failure problem of NameNode, HA feature was intorduced in Hadoop 2.x where we have two NameNode in our HDFS cluster in an active/passive configuration. Hence, if the active NameNode  fails, the other passive NameNode can take over the responsibility of the failed NameNode and keep the HDFS up and running.  

32. Define Hadoop Archives? What is the command for archiving a group of files in HDFS.
Hadoop Archive was introduced to cope up with the problem of increasing memory usage of the NameNode for storing the metadata information because of too many small files. Basically, it allows us to pack a number of small HDFS files into a single archive file and therefore, reducing the metadata information. The final archived file follows the .har extension and one can consider it as a layered file system on top of HDFS. 

The command for archiving a group of files:

hadoop archive –archiveName edureka_archive.har /input/location /output/location

trend Trending Courses in this category
Big Data Hadoop Certification Training
5 (56450)
142k Learners Enrolled Live Class
Best Price
 18,695  21,995
Similar Courses
Apache Spark and Scala Certification TrainingPython Spark Certification Training using PySparkApache Kafka Certification Training
33. How will you perform the inter cluster data copying work in HDFS?
One can perform the inter cluster data copy by using distributed copy command given as follows:

hadoop distcp hdfs://<source NameNode> hdfs://<target NameNode>

I hope you find this blog on Hadoop HDFS Interview questions very helpful and informative. For becoming a Hadoop expert, you need to work on various Hadoop related projects as well, apart from having sound theoretical knowledge. We have designed a curriculum which covers all the aspects of the Hadoop framework along with lots of hands on experience. You will be taught by the Industry Experts who will be sharing their precious experience of various Hadoop related projects.

Last but not the least, I would like to say that HDFS is only one of the many components present in Hadoop framework. We have created dedicated blogs for important interview questions corresponding to each Hadoop component. Therefore, I would suggest you to follow the links provided below and enjoy the readings: 

Top 50 Hadoop Interview Questions
Hadoop Cluster Interview Questions
Hadoop MapReduce Interview Questions
Pig Interview Questions
Hive Interview Questions
HBase Interview Questions
=================================================================================

https://www.upgrad.com/blog/top-15-hadoop-interview-questions-and-answers/

For that and more, let’s look at the top 15 Hadoop interview questions that can be expected in any interview you sit for.

What is Hadoop? What are the primary components Hadoop?

Hadoop is an infrastructure equipped with relevant tools and services required to process and store Big Data. To be precise, Hadoop is the ‘solution’ to all the Big Data challenges. Furthermore, the Hadoop framework also helps organizations to analyze Big Data and make better business decisions.

The primary components of Hadoop are:

HDFS
Hadoop MapReduce
Hadoop Common
YARN
PIG and HIVE – The Data Access Components.
HBase – For Data Storage
Ambari, Oozie and ZooKeeper – Data Management and Monitoring Component
Thrift and Avro – Data Serialization components
Apache Flume, Sqoop, Chukwa – The Data Integration Components
Apache Mahout and Drill – Data Intelligence Components
What are the core concepts of the Hadoop framework?

Hadoop is fundamentally based on two core concepts. They are:

HDFS: HDFS or Hadoop Distributed File System is a Java-based reliable file system used for storing vast datasets in the block format. The Master-Slave Architecture powers it.
MapReduce: MapReduce is a programming structure that helps process large datasets. This function is further broken down into two parts – while ‘map’ segregates the datasets into tuples, ‘reduce’ uses the map tuples and creates a combination of smaller chunks of tuples.
 Everything You Need to Know about Apache Storm
 

Name the most common input formats in Hadoop?

There are three common input formats in Hadoop:

Text Input Format: This is the default input format in Hadoop.
Sequence File Input Format: This input format is used for reading files in sequence.
Key Value Input Format: This one is used to read plain text files.
What is YARN?

YARN is the abbreviation of Yet Another Resource Negotiator. It is Hadoop’s data processing framework that manages data resources and creates an environment for successful processing.



What is “Rack Awareness”?

“Rack Awareness” is an algorithm that NameNode uses to determine the pattern in which the data blocks and their replicas are stored within Hadoop cluster. This is achieved with the help of rack definitions that reduce the congestion between data nodes contained in the same rack.

What are Active and Passive NameNodes?

A high-availability Hadoop system usually contains two NameNodes – Active NameNode and Passive NameNode.

The NameNode that runs the Hadoop cluster is called the Active NameNode and the standby NameNode that stores the data of the Active NameNode is the Passive NameNode.

The purpose of having two NameNodes is that if the Active NameNode crashes, the Passive NameNode can take the lead. Thus, the NameNode is always running in the cluster, and the system never fails.

 Big Data: Must Know Tools and Technologies
 

What are the different schedulers in the Hadoop framework?

There are three different schedulers in Hadoop framework:

COSHH – COSHH helps schedule decisions by reviewing the cluster and workload combined with heterogeneity.
FIFO Scheduler – FIFO lines up jobs in a queue based on their time of arrival, without using heterogeneity.
Fair Sharing – Fair Sharing creates a pool for individual users containing multiple maps and reduce slots on a resource that they can use to execute specific jobs.
What is Speculative Execution?

Often in Hadoop framework, some nodes may run slower than the rest. This tends to constrain the entire program. To overcome this, Hadoop first detects or ‘speculates’ when a task is running slower than usual, and then it launches an equivalent backup for that task. So, in the process, the master node executes both the tasks simultaneously and whichever is completed first is accepted while the other one is killed. This backup feature of Hadoop is known as Speculative Execution.

Name the main components of Apache HBase?

Apache HBase is comprised of three components:

Region Server: After a table is divided into multiple regions, clusters of these regions are forwarded to the clients via the Region Server.
HMaster: This is a tool that helps manage and coordinate the Region server.
ZooKeeper: ZooKeeper is a coordinator within the HBase distributed environment. It helps maintain a server state inside the cluster through communication in sessions.
What is “Checkpointing”? What is its benefit?

Checkpointing refers to the procedure by which a FsImage and Edit log are combined to form a new FsImage. Thus, instead of replaying the edit log, the NameNode can directly load the final in-memory state from the FsImage. The secondary NameNode is responsible for this process.

The benefit that Checkpointing offers is that it minimizes the startup time of the NameNode, thereby making the entire process more efficient.

 Big Data Applications in Pop-Culture
 

How to debug a Hadoop code?

To debug a Hadoop code, first, you need to check the list of MapReduce tasks that are presently running. Then you need to check whether or not any orphaned tasks are running simultaneously. If so, you need to find the location of Resource Manager logs by following these simple steps:

Run “ps –ef | grep –I ResourceManager” and in the displayed result, try to find if there is an error related to a specific job id.

Now, identify the worker node that was used to execute the task. Log in to the node and run “ps –ef | grep –iNodeManager.”

Finally, scrutinize the Node Manager log. Most of the errors are generated from user level logs for each map-reduce job.

What is the purpose of RecordReader in Hadoop?

Hadoop breaks data into block formats. RecordReader helps integrate these data blocks into a single readable record. For example, if the input data is split into two blocks –

Row 1 – Welcome to

Row 2 – UpGrad

RecordReader will read this as “Welcome to UpGrad.”

What are the modes in which Hadoop can run?

The modes in which Hadoop can run are:

Standalone mode – This is a default mode of Hadoop that is used for debugging purpose. It does not support HDFS.
Pseudo-distributed mode – This mode required the configuration of mapred-site.xml, core-site.xml, and hdfs-site.xml files. Both the Master and Slave Node are the same here.
Fully-distributed mode – Fully-distributed mode is Hadoop’s production stage in which data is distributed across various nodes on a Hadoop cluster. Here, the Master and the Slave Nodes are allotted separately.
Name some practical applications of Hadoop.

Here are some real-life instances where Hadoop is making a difference :

Managing street traffic
Fraud detection and prevention
Analyse customer data in real-time to improve customer service
Accessing unstructured medical data from physicians, HCPs, etc., to improve healthcare services.
What are the vital Hadoop tools that can enhance the performance of Big Data?

The Hadoop tools that boost Big Data performance significantly are Hive, HDFS, HBase, SQL, NoSQL, Oozie, Clouds, Avro, Flume, and ZooKeeper.

=========================================================================================================================================================================================

https://intellipaat.com/interview-question/hdfs-interview-questions/

Top Answers to HDFS Interview Questions

1. Compare HDFS & HBase
Criteria	HDFS	HBase
Data write process	Append method	Bulk incremental, random write
Data read process	Table scan	Table scan/random read/small range scan
Hive SQL querying	Excellent	Average
2. What is Hadoop ?

Hadoop Tutorial

3. Who is the provider of Hadoop?
Hadoop forms part of Apache project provided by Apache Software Foundation.

4. What is the use of Hadoop?
With Hadoop the user can run applications on the systems that have thousands of nodes spreading through innumerable terabytes. Rapid data processing and transfer among nodes helps uninterrupted operation even when a node fails preventing system failure.

Become Hadoop Certified in 85 hrs.
GET CERTIFIED
5. What are the operating systems on which Hadoop works?
Windows and Linux are the preferred operating system though Hadoop can work on OS x and BSD.

6. What is meant by Big Data?
Big Data refers to assortment of huge amount of data which is difficult capturing, storing, processing or reprieving. Traditional database management tools cannot handle them but Hadoop can.

7. Can you indicate Big Data examples?
Facebook alone generates more than 500 terabytes of data daily whereas many other organizations like Jet Air and Stock Exchange Market generates 1+ terabytes of data every hour. These are Big Data.

Learn more about hadoop online course.

8. What are major characteristics of Big Data?
The three characteristics of Big Data are volume, velocity, and veracity. Earlier it was assessed in megabytes and gigabytes but now the assessment is made in terabytes.

Read this blog to learn more about how to kick-start your career in Big Data and Hadoop.

9. What is the use of Big Data Analysis for an enterprise?
Analysis of Big Data identifies the problem and focus points in an enterprise. It can prevent big losses and make profits helping the entrepreneurs take informed decision.

Are you interested in learning HDFS? Well, we have the comprehensive Hadoop Analyst Training to give you a head start in your career.

Learn Hadoop from Experts! Enrol Today

10. What are the characteristics of data scientists?
Data scientists analyze data and provide solutions for business problems. They are gradually replacing business and data analysts.

Most Valuable Data Science Skills Of 2016 to learn more about must-have Data Science skills.

11. What are the basic characteristics of Hadoop?
Written in Java, Hadoop framework has the capability of solving issues involving Big Data analysis. Its programming model is based on Google MapReduce and infrastructure is based on Google’s Big Data and distributed file systems. Hadoop is scalable and more nodes can be added to it.

Get to know the history, timeline and architecture of Hadoop!

12. Which are the major players on the web that uses Hadoop?
Introduce in 2002 by Doug Cutting, Hadoop was used in Google MapReduce and HDFS project in 2004 and 2006. Yahoo and Facebook adopted it in 2008 and 2009 respectively. Major commercial enterprises using Hadoop include EMC, Hortonworks, Cloudera, MaOR, Twitter, EBay, and Amazon among others.

Want to know about the most sought-after Hadoop job roles and responsibilities?

13. How is Hadoop different from traditional RDBMS?
RDBMS can be useful for single files and short data whereas Hadoop is useful for handling Big Data in one shot.

14. What are the main components of Hadoop?
Main components of Hadoop are HDFS used to store large databases and MapReduce used to analyze them.

Learn all about Hadoop components in this Big Data Hadoop Video Tutorial.

15. What is HDFS?
HDFS is filing system use to store large data files. It handles streaming data and running clusters on the commodity hardware.

16. What are the main features of HDFS?
Great fault tolerance, high throughput, suitability for handling large data sets, and streaming access to file system data are the main features of HDFS. It can be built with commodity hardware.

Learn all about HDFS and get ahead in your career with this comprehensive Big Data Hadoop Online Training all in 1 combo course

17. Why replication is pursued in HDFS though it may cause data redundancy?
Systems with average configuration are vulnerable to crash at any time. HDFS replicates and stores data at three different locations that makes the system highly fault tolerant. If data at one location becomes corrupt and is inaccessible it can be retrieved from another location.

This insightful Cloudera article shows the steps for running HDFS on a cluster.

18. Would the calculations made on one node be replicated to others in HDFS?
No! The calculation would be made on the original node only. In case the node fails then only the master node would replicate the calculation on to a second node.

19. What is meant by streaming access?
HDFS works on the principle of “write once, read many” and the focus is on fast and accurate data retrieval. Steaming access refers to reading the complete data instead of retrieving single record from the database.

20. What is meant by ‘commodity hardware’? Can Hadoop work on them?
Average and non-expensive systems are known as commodity hardware and Hadoop can be installed on any of them. Hadoop does not require high end hardware to function.

21. Which one is the master node in HDFS? Can it be commodity?
Name node is the master node in HDFS and job tracker runs on it. The node contains metadata and works as high availability machine and single pint of failure in HDFS. It cannot be commodity as the entire HDFS works on it.

22. What is meant by Data node?
Data node is the slave deployed in each of the systems and provides the actual storage locations and serves read and writer requests for clients.

23. What is daemon?
Daemon is the process that runs in background in the UNIX environment. In Windows it is ‘services’ and in DOS it is ‘TSR’.

24. What is the function of ‘job tracker’?
Job tracker is one of the daemons that runs on name node and submits and tracks the MapReduce tasks in Hadoop. There is only one job tracker who distributes the task to various task trackers. When it goes down all running jobs comes to a halt.

25. What is the role played by task trackers?
Daemons that run on What data nodes, the task tracers take care of individual tasks on slave node as entrusted to them by job tracker.

Learn more about HDFS in this Hadoop Developer Training Course to get ahead in your career!

26. What is meant by heartbeat in HDFS?
Data nodes and task trackers send heartbeat signals to Name node and Job tracker respectively to inform that they are alive. If the signal is not received it would indicate problems with the node or task tracker.

27. Is it necessary that Name node and job tracker should be on the same host?
No! They can be on different hosts.

28. What is meant by ‘block’ in HDFS?
Block in HDFS refers to minimum quantum of data for reading or writing. Default block size is 64 MB in HDFS. If a file is 52 MB then HDFS would store it and leave 12 MB empty and ready to use.

29. Can blocks be broken down by HDFS if a machine does not have the capacity to copy as many blocks as the user wants?
Blocks in HDFS cannot be broken. Master node calculates the required space and how data would be transferred to a machine having lower space.

30. What is the process of indexing in HDFS?
Once data is stored HDFS will depend on the last part to find out where the next part of data would be stored.

31. How a data node is identified as saturated?
When a data node is full and has no space left the name node will identify it.

32. What type of data is processed by Hadoop?
Hadoop processes the digital data only.

33. How Name node determines which data node to write on?
Name node contains metadata or information in respect of all the data nodes and it will decide which data node to be used for storing data.

34. Who is the ‘user’ in HDFS?
Anyone who tries to retrieve data from database using HDFS is the user. Client is not end user but an application that uses job tracker and task tracker to retrieve data.

35. How the client communicates with Name node and Data node in HDFS?
The communication mode for clients with name node and data node in HDFS is SSH.

36. What is a rack in HDFS?
Rack is the storage location where all the data nodes are put together. Thus it is a physical collection of data nodes stored in a single location.
===============================================================================================================================================================================

https://mindmajix.com/hadoop-interview-questions

Hadoop Interview Questions & Answers
Q) MapReduce Vs Spark
Hadoop MapReduce Vs Spark
Hadoop MapReduce	Spark
Open-Source framework for application writing	Open-Source big data framework
Process Structured & unstructured data	It is for general purpose data processing
It provides batch processing only	Fits for both batch & stream processing 
It fails in real-time data processing	It can process the real-time data
It is difficult to program & require abstraction	It comes with user-friendly APIs of scala.
It reads data from disk & after sends results to HDFS	It comes with a graph comutational lib called GraphX
It receives fault tolerance via replication	It uses RDD & diff data storage models for fault tolerance
It has better security features	Its security is currently infancy.
Q1) What is big data?

Big Data is the really large amount of data that exceeds the processing capacity of conventional database systems and requires special parallel processing mechanism. The data is too big and grows rapidly. This data can be either structural or unstructured data. To retrieve meaningful information from this data, we must choose an alternative way to process it.
Characteristics of Big Data:
Data that has very large volume, comes from variety of sources and formats and flows into an organization with a great velocity is normally referred to as Big Data.



Q2) What is Hadoop?

Hadoop is a framework that allows distributed processing of large data sets across clusters of computers using simple and fault tolerant programming model. It is designed to scale up from a very few to thousands of machines, each machine provides local computation and storage. The Hadoop software library itself is designed to detect and handle failures at the application layer.
Hadoop is written in java by Apache Software Foundation.  It process data very reliably and fault-tolerant manner.
Core components of Hadoop:
HDFS (Storage) + MapReduce/YARN (Processing)



Q3) What are the sources generating big data?

Employers,Users and Machines

Employees: Historically, employees of organizations generated data.
Users: Then a shift occurred where users started generating data. For example, email, social media, photos, videos, audio and e-Commerce.
Machines: Smart phones, intelligent kitchen appliances, CCTV cameras, smart meters, global satellites, and traffic flow sensors.
Q4) Why do we need a new framework for handling big data?

Most of the traditional data was organized neatly in relational databases. Data sets now are so large and complex that they are beyond the capabilities of traditional storage and processing systems.
The following challenges demand cost-effective and innovative forms of handling big data at scale:

Lots of data
Organizations are increasingly required to store more and more data to survive in today’s highly competitive environment. The sheer volume of the data demands lower storage costs as compared to the expensive commercial relational database options.

Complex nature of data
Relational data model has great properties for structured data but many modern systems don’t fit well in row-column format. Data is now generated by diverse sources in various formats like multimedia, images, text, real-time feeds, and sensor streams. Usually for storage, the data is transformed, aggregated to fit into the structured format resulting in the loss of the original raw data.

New analysis techniques
Previously simple analysis (like average, sum) would prove to be sufficient to predict customer behavior. But now complex analysis needs to be performed to gain insightful understanding of data collected. For example, prediction models for effective micro-segmentation needs to analyse the customer’s purchase history, browsing behavior, likes and reviews on social media website to perform micro-segmentation. These advanced analytic techniques need the framework to run on.

Hadoop to rescue: Framework that provides low-cost storage and complex analytic processing capabilities

Q5) Why do we need Hadoop framework, shouldn’t DFS be able to handle large volumes of data already?

Yes, it is true that when the datasets cannot fit in a single physical machine, then Distributed File System (DFS) partitions the data, store and manages the data across different machines. But, DFS lacks the following features for which we need Hadoop framework:

Fault tolerant:
When a lot of machines are involved chances of data loss increases. So, automatic fault tolerance and failure recovery become a prime concern.

Move data to computation:
If huge amounts of data are moved from storage to the computation machines then the speed depends on network bandwidth.

Q6) What is the difference between traditional RDBMS and Hadoop?

RDBMS	Hadoop
Schema on write	Schema on read
Scale up approach	Scale out approach
Relational tables	Key-value format
Structured queries	Function programming
Online Transactions	Batch processing
Q7) What is HDFS?

Hadoop Distributed File Systems (HDFS) is one of the core components of Hadoop framework. It is a distributed file system for Hadoop. It runs on top of existing file system (ext2, ext3, etc.)

Goals: Automatic recovery from failures, Move Computation than data.

HDFS features:

Supports storage of very large datasets
Write once read many access model
Streaming data access
Replication using commodity hardware
Q8) What is difference between regular file system and HDFS?

Regular File Systems	HDFS
Small block size of data (like 512 bytes)	Large block size (orders of 64mb)
Multiple disk seeks for large files	Reads data sequentially after single seek
Q9) What HDFS is not meant for?

HDFS is not good at:

Applications that requires low latency access to data (in terms of milliseconds)
Lot of small files
Multiple writers and file modifications
Q10) What is HDFS block size and what did you chose in your project?

By default, the HDFS block size is 64MB. It can be set to higher values as 128MB or 256MB. 128MB is acceptable industry standard.

Q11) What is the default replication factor?

Default replication factor is 3

Q12) What are different hdfs dfs shell commands to perform copy operation?

$ hadoop fs -copyToLocal
$ hadoop fs -copyFromLocal
$ hadoop fs -put

Q13) What are the problems with Hadoop 1.0?

NameNode: No Horizontal Scalability and No High Availability
Job Tracker: Overburdened.
MRv1: It can only understand Map and Reduce tasks
Q14) What comes in Hadoop 2.0 and MapReduce V2 (YARN)?

NameNode: HA and Federation
JobTracker: Cluster and application resource

Q15) What different type of schedulers and type of scheduler did you use?

Capacity Scheduler
It is designed to run Hadoop applications as a shared, multi-tenant cluster while maximizing the throughput and the utilization of the cluster.

Fair Scheduler
Fair scheduling is a method of assigning resources to applications such that all apps get, on average, an equal share of resources over time.

Q16) Steps involved in decommissioning (removing) the nodes in the Hadoop cluster?

Update the network addresses in the dfs.exclude and mapred.exclude
$ hadoop dfsadmin -refreshNodes  and hadoop mradmin -refreshNodes
Check Web UI it will show “Decommissioning in Progress”
Remove the Nodes from include file and then run again the step 2 refreshNodes.
Remove the Nodes from slave file.
Q17) Steps involved in commissioning (adding) the nodes in the Hadoop cluster?

Update the network addresses in the dfs.include and mapred.include
$ hadoop dfsadmin -refreshNodes  and hadoop mradmin -refreshNodes
Update the slave file.
Start the DataNode and NodeManager on the added Node.
Q18) How to keep HDFS cluster balanced?

Balancer is a tool that tries to provide a balance to a certain threshold among data nodes by copying block data distribution across the cluster.

Q19) What is distcp?

istcp is the program comes with Hadoop for copying large amount of data to and from Hadoop file systems in parallel.
It is implemented as MapReduce job where copying is done through maps that run in parallel across the cluster.
There are no reducers.
Q20) What are the daemons of HDFS?

NameNode
DataNode
Secondary NameNode.
Q21) Command to format the NameNode?

$ hdfs namenode -format

Q22) What are the functions of NameNode?

The NameNode is mainly responsible for:

Namespace
Maintain metadata about the data

Block Management
Processes block reports and maintain location of blocks.
Supports block related operations
Manages replica placement

Q23) What is HDFS Federation?

HDFS federation allows scaling the name service horizontally; it uses multiple independent NameNodes for different namespaces.
All the NameNodes use the DataNodes as common storage for blocks.
Each DataNode registers with all the NameNodes in the cluster.
DataNodes send periodic heartbeats and block reports and handles commands from the NameNodes
Q24) What is HDFS High Availability?

In HDFS High Availability (HA) cluster; two separate machines are configured as NameNodes.
But one of the NameNodes is in an Active state; other is in a Standby state.
The Active NameNode is responsible for all client operations in the cluster, while the Standby is simply acting as a slave, maintaining enough state to provide a fast failover if necessary
They shared the same storage and all DataNodes connects to both the NameNodes.
Q25) How client application interacts with the NameNode?

Client applications interact using Hadoop HDFS API with the NameNode when it has to locate/add/copy/move/delete a file.
The NameNode responds the successful requests by returning a list of relevant DataNode servers where the data is residing.
Client can talk directly to a DataNode after the NameNode has given the location of the data
Q26) What is a DataNode?

A DataNode stores data in the Hadoop File System HDFS is a slave node.
On startup, a DataNode connects to the NameNode.
DataNode instances can talk to each other mostly during replication.
Q27) What is rack-aware replica placement policy?

Rack-awareness is used to take a node’s physical location into account while scheduling tasks and allocating storage.
Default replication factor is 3 for a data blocks on HDFS.
The first two copies are stored on DataNodes located on the same rack while the third copy is stored on a different rack.


Q28) What is the main purpose of HDFS fsck command?

fsck a utility to check health of the file system, to find missing files, over-replicated, under-replicated and corrupted blocks.

Command for finding the blocks for a file:

$ hadoop fsck -files -blocks –racks

Q29) What is the purpose of DataNode block scanner?

Block scanner runs on every DataNode, which periodically verifies all the blocks stored on the DataNode.
If bad blocks are detected it will be fixed before any client reads.
Q30) What is the purpose of dfsadmin tool?

It is used to find information about the state of HDFS
It performs administrative tasks on HDFS
Invoked by hadoop dfsadmin command as superuser
Q31) What is the command for printing the topology?

It displays a tree of racks and DataNodes attached to the tracks as viewed by the .hdfs dfsadmin -printTopology

Q32) What is RAID?

RAID is a way of combining multiple disk drives into a single entity to improve performance and/or reliability. There are a variety of different levels in RAID
For example, In RAID level 1 copy of the same data on two disks increases the read performance by reading alternately from each disk in the mirror.

Q33) Does Hadoop requires RAID?

In DataNodes storage is not using RAID as redundancy can be achieved by replication between the Nodes.
In NameNode’s disk RAID is recommended.
Q34) What are the site-specific configuration files in Hadoop?

conf/core-site.xml
conf/hdfs-site.xml
conf/yarn-site.xml
conf/mapred-site.xml.
conf/hadoop-env.sh
conf/yarn-env.sh
Q35) What is MapReduce?

MapReduce is a programming model for processing on the distributed datasets on the clusters of a computer.

MapReduce Features:

Distributed programming complexity is hidden
Built in fault-tolerance
Programming model is language independent
Parallelization and distribution are automatic
Enable data local processing
Q36) What is the fundamental idea behind YARN?

In YARN (Yet Another Resource Allocator), JobTracker responsibility is split into:

Resource management
Job scheduling/monitoring having separate daemons.
Yarn supports additional processing models and implements a more flexible execution engine.



Q37) What MapReduce framework consists of?

ResourceManager (RM)

Global resource scheduler
One master RM
NodeManager (NM)

One slave NM per cluster-node.
Container

RM creates Containers upon request by AM
Application runs in one or more containers
ApplicationMaster (AM)

One AM per application
Runs in Container
Q38) What are different daemons in YARN?

ResourceManager: Global resource manager.
NodeManager: One per data node, It manages and monitors usage of the container (resources in terms of Memory, CPU).
ApplicationMaster: One per application, Tasks are started by NodeManager
Q39) What are the two main components of ResourceManager?
Scheduler

It allocates the resources (containers) to various running applications: Container elements such as memory, CPU, disk etc.

ApplicationManager
It accepts job-submissions, negotiating for container for executing the application specific ApplicationMaster and provides the service for restarting the ApplicationMaster container on failure.

Q40) What is the function of NodeManager?

The NodeManager is the resource manager for the node (Per machine) and is responsible for containers, monitoring their resource usage (cpu, memory, disk, network) and reporting the same to the ResourceManager

Q41) What is the function of ApplicationMaster?

ApplicationMaster is per application and it has the responsibility of negotiating appropriate resource containers from the Scheduler, tracking their status and monitoring for progress.

Q42) What are the minimum configuration requirements for a MapReduce application?

The job configuration requires the

input location
output location
map() function
reduce() functions and
job parameters.
Q43) What are the steps to submit a Hadoop job?

Steps involved in Hadoop job submission:

Hadoop job client submits the job jar/executable and configuration to the ResourceManager.
ResourceManager then distributes the software/configuration to the slaves.
ResourceManager then scheduling tasks and monitoring them.
Finally, job status and diagnostic information is provided to the client.
Q44) How does MapReduce framework view its input internally?

It views the input as a set of pairs and produces a set of pairs as the output of the job.

Q45) Assuming default configurations, how is a file of the size 1 GB (uncompressed) stored in HDFS?

Default block size is 64MB. So, file of 1GB will be stored as 16 blocks. MapReduce job will create 16 input splits; each will be processed with separate map task i.e. 16 mappers.

Q46) What are Hadoop Writables?

Hadoop Writables allows Hadoop to read and write the data in a serialized form for transmission as compact binary files. This helps in straightforward random access and higher performance. Hadoop provides in built classes, which implement Writable: Text, IntWritable, LongWritable, FloatWritable, and BooleanWritable.

Q47) Why comparison of types is important for MapReduce?

A comparison is important as in the sorting phase the keys are compared with one another. For comparison, the WritableComparable interface is implemented.

Q48) What is the purpose of RawComparator interface?

RawComparator allows the implementors to compare records read from a stream without deserialization them into objects, so it will be optimized, as there is not overhead of object creation.

Q49) What is a NullWritable?

It is a special type of Writable that has zero-length serialization. In MapReduce, a key or a value can be declared as NullWritable if we don’t need that position, storing constant empty value.

Q50) What is Avro Serialization System?

Avro is a language-neutral data serialization system. It has data formats that work with different languages. Avro data is described using a language-independent schema (usually written in JSON). Avro data files support compression and are splittable.

Avro provides AvroMapper and AvroReducer to run MapReduce programs.

Q51) Explain use cases where SequenceFile class can be a good fit?

When the data is of type binary then SequenceFile will provide a persistent structure for binary key-value pairs. SequenceFiles also work well as containers for smaller files as HDFS and MapReduce are optimized for large files.

Q52) What is MapFile?

A MapFile is an indexed SequenceFile and it is used for look-ups by key.

Q53) What is the core of the job in MapReduce framework?

The core of a job:
Mapper interface: map method
Reducer interface reduce method

Q54) What are the steps involved in MapReduce framework?

Firstly, the mapper input key/value pairs maps to a set of intermediate key/value pairs.
Maps are the individual tasks that transform input records into intermediate records.
The transformed intermediate records do not need to be of the same type as the input records.
A given input pair maps to zero or many output pairs.
The Hadoop MapReduce framework creates one map task for each InputSplit generated by the InputFormat for the job.
It then calls map(WritableComparable, Writable, Context) for each key/value pair in the InputSplit for that task.
All intermediate values associated with a given output key are grouped passed to the Reducers.
Q55) Where is the Mapper Output stored?

The mapper output is stored on the Local file system of each individual mapper nodes. The intermediate data is cleaned up after the Hadoop Job completes.

Q56) What is a partitioner and how the user can control which key will go to which reducer?

Partitioner controls the partitioning of the keys of the intermediate map-outputs by the default. The key to decide the partition uses hash function. Default partitioner is HashPartitioner.
A custom partitioner is implemented to control, which keys go to which Reducer.

public class SamplePartitioner extends Partitioner {

@Override

public int getPartition(Text key, Text value, int numReduceTasks) {

}

}

Q57) What are combiners and its purpose?

Combiners are used to increase the efficiency of a MapReduce program. It can be used to aggregate intermediate map output locally on individual mapper outputs.
Combiners can help reduce the amount of data that needs to be transferred across to the reducers.
Reducer code as a combiner if the operation performed is commutative and associative.
Hadoop may or may not execute a combiner.
Q58) How a number of partitioners and reducers are related?

The total numbers of partitions are the same as the number of reduce tasks for the job.

Q59) What is IdentityMapper?

IdentityMapper implements the mapping inputs directly to output. IdentityMapper.class is used as a default value when JobConf.setMapperClass is not set.

Q60) What is IdentityReducer?

In IdentityReducer no reduction is performed, writing all input values directly to the output. IdentityReducer.class is used as a default value when JobConf.setReducerClass is not set

Q61) What is the reducer and its phases?

Reducer reduces a set of intermediate values, which has same key to a smaller set of values. The framework then calls reduce().
Syntax:
reduce(WritableComparable, Iterable, Context) method for each pair in the grouped inputs.
Reducer has three primary phases:

Shuffle
Sort
Reduce
Q62) How to set the number of reducers?

The number of reduces for the user sets the job:

Job.setNumReduceTasks(int)
-D mapreduce.job.reduces
Q63) Detail description of the Reducer phases?

Shuffle:
Sorted output (Mapper) à Input (Reducer). Framework then fetches the relevant partition of the output of all the mappers.

Sort:
The framework groups Reducer inputs by keys. The shuffle and sort phases occur simultaneously; while map-outputs are being fetched they are merged.

Secondary Sort:
Grouping the intermediate keys are required to be different from those for grouping keys before reduction, then Job.setSortComparatorClass(Class).

Reduce:
reduce(WritableComparable, Iterable, Context) method is called for each pair in the grouped inputs.
The output of the reduce task is typically written using Context.write(WritableComparable, Writable).

Q64) Can there be no Reducer?

Yes, the number of reducer can be zero if no reduction of values is required.

Q65) What can be optimum value for Reducer?

Value of Reducers can be: 0.95

1.75 multiplied by ( * < number of maximum containers per node>)
Increasing number of reducers

Increases the framework overhead
Increases load balancing
Lowers the cost of failures
Q66) What are a Counter and its purpose?

The counter is a facility for MapReduce applications to report its statistics. They can be used to track job progress in a very easy and flexible manner. It is defined by MapReduce framework or by applications. Each Counter can be of any Enum type. Applications can define counters of type Enum and update them via counters.incrCounter in the map and/or reduce methods.

Q67) Define different types of Counters?

Built in Counters:

Map Reduce Task Counters
Job Counters
Custom Java Counters:

MapReduce allows users to specify their own counters (using Java enums) for performing their own counting operation.
Q68) Why Counter values are shared by all map and reduce tasks across the MapReduce framework?

Counters are global so shared across the MapReduce framework and aggregated at the end of the job across all the tasks.

Q69) Explain speculative execution.

Speculative execution is a way of dealing with individual machine’s performance. As there are lots of machines in the cluster, some machines can have low performance, which affects the performance of the whole job.
Speculative execution in Hadoop can run multiple copies of the same map or reduce task on different task tracker nodes and the results from first node to finish are used.
Q70) What is DistributedCache and its purpose?

DistributedCache is a facility provided by the MapReduce framework to cache files (text, archives, jars etc.) needed by applications. It distributes application-specific, large, read-only files efficiently. The user needs to use DistributedCache to distribute and symlink the script file.

Q71) What is the Job interface in MapReduce framework?

Job is the primary interface for a user to describe a MapReduce job to the Hadoop framework for execution. Some basic parameters are configured for example:

Job.setNumReduceTasks(int)
Configuration.set(JobContext.NUM_MAPS, int)
Mapper
Combiner (if any)
Partitioner
Reducer
InputFormat
OutputFormat implementations
setMapSpeculativeExecution(boolean))/ setReduceSpeculativeExecution(boolean))
Maximum number of attempts per task (setMaxMapAttempts(int)/ setMaxReduceAttempts(int)) etc.
DistributedCache for large amounts of (read-only) data.
Q72) What is the default value of map and reduce max attempts?

The framework will try to execute a map task or reduce task by default 4 times before giving up on it.

Q73) Explain InputFormat?

InputFormat describes the input-specification for a MapReduce job. The MapReduce framework depends on the InputFormat of the job to:

Checks the input-specification of the job.
It then splits the input file(s) into logical InputSplit instances, each of which is then assigned to an individual Mapper.

To extract input records from the logical InputSplit for processing by the Mapper it provides the RecordReader implementation.
Default: TextInputFormat

Q74) What is InputSplit and RecordReader?

InputSplit specifies the data to be processed by an individual Mapper.
In general, InputSplit presents a byte-oriented view of the input.

Default: FileSplit
RecordReader reads pairs from an InputSplit, then processes them and presents record-oriented view

Q75) Explain the Job OutputFormat?

OutputFormat describes details of the output for a MapReduce job.
The MapReduce framework depends on the OutputFormat of the job to:
It checks the job output-specification

To write the output files of the job in the pairs, it provides the RecordWriter implementation.
Default: TextOutputFormat

Q76) How is the option in Hadoop to skip the bad records?

Hadoop provides an option where a certain set of bad input records can be skipped when processing map inputs. This feature can be controlled by the SkipBadRecords class.

Check Out Hadoop Tutorials

Q77) Different ways of debugging a job in MapReduce?

Add debug statement to log to standard error along with the message to update the task’s status message. Web UI makes it easier to view.
Create a custom counter, it gives valuable information to deal with the problem dataset
Task page and task detailed page
Hadoop Logs
MRUnit testing
PROGRAM 1: Counting the number of words in an input file

Introduction
This section describes how to get the word count of a sample input file.

Software Versions
The software versions used are:
VirtualBox: 4.3.20
CDH 5.3: Default MapReduce Version
hadoop-core-2.5.0
hadoop-yarn-common-2.5.0

Steps
1. Create the input file
Create the input.txt file with sample text.
$ vi input.txt
Thanks Lord Krishna for helping us write this book
Hare Krishna Hare Krishna Krishna Krishna Hare Hare
Hare Rama Hare Rama Rama Rama Hare Hare

2. Move the input file into HDFS
Use the –put or –copyFromLocal command to move the file into HDFS
$ hadoop fs -put input.txt

3. Code for the MapReduce program
Java files:
WordCountProgram.java  // Driver Program
WordMapper.java         // Mapper Program
WordReducer.java        // Reducer Program
————————————————–
WordCountProgram.java File: Driver Program
————————————————–
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
public class WordCountProgram extends Configured implements Tool{
@Override
public int run(String[] args) throws Exception {
Configuration conf = new Configuration();
Job job = new Job(conf, “wordcountprogram”);
job.setJarByClass(getClass());
// Configure output and input source
TextInputFormat.addInputPath(job, new Path(args[0]));
job.setInputFormatClass(TextInputFormat.class);
job.setMapperClass(WordMapper.class);
job.setReducerClass(WordReducer.class);
// Configure output
TextOutputFormat.setOutputPath(job, new Path(args[1]));
job.setOutputFormatClass(TextOutputFormat.class);
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(IntWritable.class);
return job.waitForCompletion(true) ? 0 : 1;
}
public static void main(String[] args) throws Exception {
int exitCode = ToolRunner.run(new WordCountProgram(), args);
System.exit(exitCode);
}
}
————————————————–
WordMapper.java File: Mapper Program
————————————————–
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
public class WordMapper extends Mapper {
private final static IntWritable count = new IntWritable(1);
private final Text nameText = new Text();
@Override
protected void map(LongWritable key, Text value, Context context) throws IOException,
InterruptedException {
StringTokenizer tokenizer = new StringTokenizer(value.toString(),” “);
while (tokenizer.hasMoreTokens()) {
nameText.set(tokenizer.nextToken());
context.write(nameText, count);
}
}
}
———————————————–
WordReducer.java file: Reducer Program
————————————————–
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
public class WordReducer extends Reducer {
@Override
protected void reduce(Text t, Iterable counts, Context context)
throws IOException, InterruptedException {
int sum = 0;
for (IntWritable count : counts) {
sum += count.get();
}
context.write(t, new IntWritable(sum));
}
}
4. Run the MapReduce program
Create the jar of the Code in Step 3 and use the following command to run the MapReduce program
$ hadoop jar WordCount.jar WordCountProgram input.txt output1
Here,
WordCount.jar: Name of jar exported having the all the methods.
WordCountProgram: Driver Program having the entire configuration
input.txt: Input file
output1: Output folder where the output file will be stored
5. View the Output
View the output in the output1 folder
$ hadoop fs -cat /user/cloudera/output1/part-r-00000
Hare  8
Krishna     5
Lord  1
Rama  4
Thanks      1
book  1
for   1
helping     1
this  1
us    1
write 1

Q78) What problem does Apache Flume solve?

Scenario:

There are several services producing a huge number of logs that run in different servers. These logs need to be accumulated, stored and analyzed together.
Hadoop has emerged as a cost effective and scalable framework for storage and analysis for big data.
Problem:

How can these logs be collected, aggregated and stored to a place where Hadoop can process them?
Now there is a requirement for a reliable, scalable, extensible and manageable solution.
Q79) What is Apache Flume?

Apache Flume is a distributed data collection service that gets flows of data (like logs) from the systems that generate them and aggregates them to a centralized data store where they can be processed together.

Goals: reliability, recoverability, and scalability

Flume features:

Ensures guaranteed data delivery
Gather high volume data streams in real time
Streaming data is coming from multiple sources into Hadoop for analysis
Scales horizontally
Q80) How is Flume-NG different from Flume 0.9?

Flume 0.9:

Centralized configuration of the agents handled by Zookeeper.
Input data and writing data are handled by same thread.
Flume 1.X (Flume-NG):
No centralized configuration. Instead a simple on-disk configuration file is used.
Different threads called runners handle input data and writing data.

Q90) What is the problem with HDFS and streaming data (like logs)?

In a regular filesystem when you open a file and write data, it exists on disk even before it is closed.
Whereas in HDFS, the file exists only as a directory entry of zero length till it is closed. This implies that if data is written to a file for an extended period without closing it, you may be left with an empty file if there is a network disconnect with the client.
It is not a good approach to close the files frequently and create smaller files as this leads to poor efficiency in HDFS.
Q91) What are core components of Flume?

Flume architecture:



Flume Agent:

An agent is a daemon (physical Java virtual machine) running Flume.
It receives and stores the data until it is written to a next destination.
Flume source, channel and sink run in an agent.
Source:

A source receives data from some application that is producing data.
A source writes events to one or more channels.
Sources either poll for data or wait for data to be delivered to them.
For Example: log4j, Avro, syslog, etc.
Sink:

A sink removes the events from the agent and delivering it to the destination.
The destination could be different agent or HDFS, HBase, Solr etc.
For Example: Console, HDFS, HBase, etc.
Channel:

A channel holds events passing from a source to a sink.
A source ingests events into the channel while sink removes them.
A sink gets events from one channel only.
For Example: Memory, File, JDBC etc.
Q92) Explain a common use case for Flume?

Common Use case: Receiving web logs from several sources into HDFS.
Web server logs → Apache Flume → HDFS (Storage) → Pig/Hive (ETL) → HBase (Database) → Reporting (BI Tools)

Logs are generated by several log servers and saved in local hard disks, which need to be pushed into HDFS using Flume framework.
Flume agents, which are running on, log servers collect the logs, which are pushed into HDFS.
Data analytics tools like Pig or Hive then process this data.
The analysed data is stored in structured format in HBase or other database.
Business intelligence tools will then generate reports on this data.
Q93) What are Flume events?

Flume events:

Basic payload of data transported by Flume (typically a single log entry)
It has zero or more headers and a body


Event Headers are key-value pairs that are used to make routing decisions or carry other structured information like:

Timestamp of the event
Hostname of the server where event has originated
Event Body

Event Body is an array of bytes that contains the actual payload.

Q94) Can we change the body of the flume event?

Yes, editing Flume Event using interceptors can change its body.

Q95) What are interceptors?



Interceptor

An interceptor is a point in your data flow where you can inspect and alter flume events. After the source creates an event, there can be zero or more interceptors tied together before it is delivered to sink.

Q96) What are channel selectors?

Channel selectors:

Channel selectors are responsible for how an event moves from a source to one or more channels.
Types of channel selectors are:

Replicating Channel Selector: This is the default channel selector that puts a copy of event into each channel
Multiplexing Channel Selector: Routes data into different channel depending on header information and/or interceptor logic
Q97) What are sink processors?

Sink processor:

Sink processor is a mechanism for failover and load balancing events across multiple sinks from a channel

Q98) How to Configure an Agent?

An agent is configured using a simple Java property file of key/value pairs
This configuration file is passed as an argument to the agent upon startup.
You can configure multiple agents in a single configuration file. It is required to pass an agent identifier (called a name).
Each agent is configured starting with:
agent.sources=
agent.channels=
agent.sinks=
Each source, channel and sink also has a distinct name within the context of that agent.
Q99) Explain the “Hello world” example in flume.

In the following example, the source listens on a socket for network clients to connect and sends event data. Those events were written to an in-memory channel and then fed to a log4j sink to become output.
Configuration file for one agent (called a1) that has a source named s1, a channel named c1 and a sink named k1.
# netcatAgent.conf: Logs the netcat events to console
# Name of the components on this agent
a1.sources=s1
a1.channels=c1
a1.sinks=k1
# Configure the source
a1.sources.s1.type=Netcat
 
Q100) What is Hive?

Hive is a Hadoop based system for querying and analyzing large volumes of Structured data which is stored on HDFS or in other words Hive is an query engine built to work on top of Hadoop that can compile queries into Map Reduce jobs and run them on the cluster.

Q101) In which scenario Hive is good fit?

Data warehousing applications where more static data is analyzed.
Fast response time is not the criteria.
Data is not changing rapidly.
An abstract to underlying MapReduce programs
Like SQL
Q102) What are the limitations of Hive?

Hive does not provide:

Record-level operations like INSERT, DELETE or UPDATE.
Cannot be used for low latency jobs.
Transaction.
Q103) What are the differences between Hive and RDBMS?

HIVE:

Schema on Read
Batch processing jobs
Data stored on HDFS
Processed using MapReduce
RDBMS:

Schema on write
Real time jobs
Data stored on internal structure
Processed using database
Q104) What are the components of Hive architecture?

Hive Driver
Metastore
Hive CLI/HUE/HWI
Q105) What is the purpose of Hive Driver?

Hive Driver is responsible for compiling, optimizing and then executing the HiveQL.

Q106) What is a Metastore and what it stores?

It is a database by default Derby SQL server
Holds metadata about table definition, column, and data types partitioning information,
It can be stored in MySQL, derby, oracle etc.
Q107) What is the purpose of storing the metadata?

People want to read the dataset with a particular schema in mind.
For e.g.: BA and CFO of a company look at the data with a particular schema.
BA may be interested in say IP addresses and timings of the clicks in a weblog while the CFO may be interested in say the clicks that were direct clicks on the website or from paid Google adds.
Underneath it’s the same dataset that is accessed. This schema is used again and again. So it makes sense to store this schema in a RDBMS.

Q108) List the various options available with the Hive command.

Syntax:

$ ./hive –service serviceName
where
serviceName options are:
cli
help
hiveserver
hwi
jar
lineage
metastore
rcfile

Q109) Explain the different services that can be invoked using the Hive command.

cli

default service
used to define tables, run queries, etc.
hiveserver

aemon that listens for Thrift connections from other processes
hwi

Simple web interface for running queries
jar

Extension of the hadoop jar command
metastore

External Hive metastore service to support multiple clients
rcfile

Tool for printing the contents of an RFFile
Q110) Can you execute Hadoop dfs Commands from Hive CLI? How?

Hadoop dfs commands can be run from within the hive CLI by dropping the hadoop work from the command and adding a semicolon in the end.

For Example:

Hadoop dfs command:
hadoop dfs -ls /
From within hive
hive > dfs -ls / ;

Q111) How to give multiline comments in Hive Scripts?

Hive does not support multiline comments. All lines of comments should start with the string —
For e.g.
— This is first line of comment
— This is second line of comment !!

Q112) What is the reason for creating a new metastore_db whenever Hive query is run from a different directory?

Embedded mode:
Whenever Hive runs in embedded mode, it checks whether the metastore exists. If the metastore does not exist then it creates the local metastore.

Property: Default value

javax.jdo.option.ConnectionURL = “jdbc:derby:;databaseName=metastore_db;create=true”

Q113) When Hive is run in embedded mode, how to share the metastore within multiple users?

No.
For sharing use the standalone database (like MySQL, PostGresQL) for metastore

Q114) How can an application connect to Hive run as a server?

Thrift Client: Hive commands can be called hive command from programming languages like Java, PHP, Python, Ruby, C++
JDBC Driver: Type 4 (pure Java) JDBC Driver
ODBC driver:  ODBC protocol

Q115) List the Primitive Data Types?

DataTypes:
TINYINT

Q116) What problem does Apache Pig solve?

Scenario
1. MapReduce paradigm presented by Hadoop is low level and rigid so developing can be challenging.
2. Jobs are (mainly) in Java where developer needs to think in terms of map and reduce

Problem
1. Many common operations like filters, projections, joins requires a custom code
2. Not everyone is a Java expert!!!
3. MapReduce has a long development cycle

Q117) What is Apache Pig?

Apache Pig is a platform for analyzing large data sets that consists high-level language for expressing data analysis programs, with infrastructure for evaluating these programs.
Goals: Ease of programming, Improved Code readability, Flexible, Extensible
Pig Features:
Ease of programming:

Generates MapReduce programs automatically
Fewer lines of code
Flexible:

Metadata is optional
Extensible:

Easy extensible by UDFs
Resides on the client machine



Q118) In which scenario MapReduce is a better fit than Pig?

Some problems are harder to express in Pig. For example:

Complex grouping or joins
Combining lot of datasets
Replicated join
Complex cross products
In such cases, Pig’s MAPREDUCE relational operator can be used which allows plugging in Java MapReduce job.

Q119) In which scenario Pig is better fit than MapReduce?

Pig provides common data operations (joins, filters, group by, order by, union) and nested data types (tuple, bag and maps), which are missing from MapReduce.

Q120) Where not to use Pig?

Completely unstructured data. For example: images, audio, video
When more power to optimize the code is required
Retrieving a single record in a very large dataset
Q121) What can be feed to Pig?

We can input structured, semi-structured or unstructured data to Pig.
For example, CSV’s, TSV’s, Delimited Data, Logs

Q122) What are the components of Apache Pig platform?

Pig Engine
Parser, Optimizer and produces sequences of MapReduce programs

Grunt
Pig’s interactive shell
It allows users to enter Pig Latin interactively and interact with HDFS

Pig Latin
High level and easy to understand dataflow language
Provides ease of programming, extensibility and optimization.

Q123) What are the execution modes in Pig?

Pig has two execution modes:

Local mode
No Hadoop / HDFS installation is required
All processing takes place in only one local JVM
Used only for quick prototyping and debugging Pig Latin script
pig -x local

MapReduce mode (Default)
Parses, checks and optimizes locally

Plans execution as one MapReduce job
Submits job to Hadoop
Monitors job progress
pig or pig -x mapreduce

Q124) Different running modes for running Pig?

Pig has two running modes:

Interactive mode
Pig commands runs one at a time in the grunt shell

Batch mode
Commands are in pig script file.

Q125) What are the different ways to develop PigLatin scripts?

Plugins are available which features such as syntax/error highlighting, auto completion etc.
Eclipse plugins

PigEditor
PigPen
Pig-Eclipse
Vim, Emacs, TextMate plugins also available

Q126) What are the Data types in Pig?

Scalar Types
Int, long, float, double, chararray, bytearray, boolean (since Release 0.10.0)

Complex Types
Map, Tuple, Bag

Q127) Which type in Pig is not required to fit in Memory?

Bag is the type not required to fit in memory, as it can be quite large.
It can store bags to disk when necessary.
Q128) What is a Map in Pig?

Map is a chararray to data element mapping, where data element be of any Pig data type.
It can also be called as a set of key-value pairs where
     Keys → chararray and Values → any pig data type
For example [‘student’#’Mahi’, ’Rank’#1]

Q129) What is a Tuple in Pig? (~ RDBMS row in a table)

A tuple is an ordered set of fields; fields can be of any data type.
It can also be called as a sequence of fields of any type.

Explore Hadoop Sample Resumes! Download & Edit, Get Noticed by Top Employers!Download Now!
 

Q130) HDFS and Mapreduce Features in Hadoop Versions:

Feature	1.x	0.22	2.x
Secure Authentication	Yes	No	Yes
Old Configuration Names	Yes	Deprecate	Deprecate
New Configuration Names	No	Yes	Yes
Old Mapreduce API	Yes	Yes	Yes
New Mapreduce API	Yes	Yes	Yes
Mapreduce 1 Runtime	No	No	Yes
Mapreduce 2 Runtime	No 	No	Yes
HDFS Federation	No	No	Yes
HDFS High Availability	No	No	Yes
Q131) What is Cluster Rebalancing?

The architecture of HDFS is in flow with data rebalancing schemes.
A scheme automatically move data from one data node into another data node.
If in case, there is a sudden rise in particular file, a scheme dynamically creates additional replies and rebalance the data within cluster. This type of data rebalancing schemes are yet to be positioned.

Q132) What is Data Integrity?

Data Integrity is a method where a block of data is fetched from a datanode, but it comes corrupted. This corruption is due to faults in storage device, network and buggy software.
HDFS software employs checksum checking on the HDFS file contents.
When a client handles file contents, it verifies that data retrieved matches the checksum of the relevant checksum file.
If not, then the client may opt to retrieve the block from variant datanode that replicates the other block.
Q133) What is Hadoop File System?

Hadoop File System indicating the compiler to interact with Linux local environment to HDFS environment.
Hadoop File System is not support the -vi command. Because HDFS is write once.
Hadoop File System is support for -touchz command.
Hadoop File System looks the only HDFS directory but not local directory.
We can not create a file on top of HDFS.
We can not create the file on local.
We can not update a file on top of HDFS. We can updations in local, after that file is put into the HDFS.
Hadoop file system does not support hard links (or) soft links.
Hadoop File System does not implement user quotas.
Error implementation is sent to stderr & output is sent to stdout.
Display detailed help for a command:
Hadoop fs - help

Q134) User Command Archive?

Hadoop stores the small files inefficiently such as each file get stored in a  block & namenode has to keep the metadata information in memory so with this reason most of the namenode memory will get eat up this small files only which results in a wastage of memory.
To avoid the same problem we use hadoop archives (or) har files (.har a the extension for all the archive files).
When creating archive directory the input is converted to mapreduce jobs, so we can call hadoop archives as a input for our mapreduce programming.
1. Hadoop archives are special format archives.
2. Hadoop archive maps to a file system directory.
3. Hadoop archive always has a .har extension
4. Hadoop archive directory contains metadata.

Q135) What is Serialization?

Serialization transforms objects into bytes
Hadoop utilizes PR6 for transmitting across the network.
Hadoop employs a very own serialization format which is writable
Comparison types are crucial 
Hadoop enables a Raw comparator, that abolishes deserialization
External frameworks are enables via : enter Avro
Q136) Datanode block scanner?

All the datanodes runs the block scanner, which periodically verifies all the blocks stored on the datanode. This allows bad blocks to be detected and fixed before they are read by clients.
It maintains

A list of blocks to verify
It scans them one by one for checksum errors.
Block scanner report can be verify by visiting http://datanode:50075/blockScannerReport.
Q137) What is HBASE Data Storage?

HBASE is column oriented data storage

Column-Oriented:

The reason to store values on a per column basis instead is based on the assumption 
That for specific queries, not all of the values are needed
Reduced I/O
The data of column-oriented databases is saved in the way grouped by columns and the following column values are stored on the contiguous disk locations. This is quite different from the conventional approach followed by the traditional databases which stores all the rows contiguously. 
Storefile: Store File for each state for each region for the table.
Block: Blocks within a store file within a store for each region for the table
Hlog used for recovering
Send heartbeat(loadinfo) to master
Write requests handle
Read request handle
Flush
Compaction
Region Splits(Manage)
Q138) What is Hadoop Streaming?

A utility to enable Mapreduce code in any language: C, Perl, Python, C++, Bash etc. The examples include a python mapper and an AWK reducer.

Q139) What is the difference between Base & NOSQL?

Favours consistencies are availability (but availability is good in practice)
Great hadoop integration (very efficient bulk loads, Mapreduce Analysis)
Ordered range partitions(not hash)
Automatically shards/scales (just run on more servers)
Sparse column stronge(not key value)

Q140) What is HBASE Client?

The HBase client finds the HRegion servers that serve the specific row range of interest. The HBase client, on instantiation, exchanges information with the HBase Master to locate the ROOT region. The client communicates with the region server of interest once the ROOT region is located and scans it to locate the META region that contains the user region’s location which consists of the desired row range.

Q141) Why HBASE?

We can infrastructure, no usage limits
Data Model
Semistructured data in Hbase
Time series ordered
Scaling is built-in (Just add more servers)
But extra indexing is DIY
Very active developer community
Established, mature project (in relative terms)
Matches our own toolset (Java/Linux based)

Q142) What is ZOOKEEPER?

Master election and server availability
cluster management: Assignment transaction state management
Client contacts zookeeper to bootstrap connection to the HBase cluster.
Region key ranges, region server address
Guarantees consistency of data across clients.
==============================================================================================================
https://www.tutorialspoint.com/hadoop/hadoop_interview_questions.htm

What does ‘jps’ command do?
It gives the status of the deamons which run Hadoop cluster. It gives the output mentioning the status of namenode, datanode , secondary namenode, Jobtracker and Task tracker.

How to restart Namenode?
Step-1. Click on stop-all.sh and then click on start-all.sh OR

Step-2. Write sudo hdfs (press enter), su-hdfs (press enter), /etc/init.d/ha (press enter) and then /etc/init.d/hadoop-0.20-namenode start (press enter).

Which are the three modes in which Hadoop can be run?
The three modes in which Hadoop can be run are −

standalone (local) mode
Pseudo-distributed mode
Fully distributed mode
What does /etc /init.d do?
/etc /init.d specifies where daemons (services) are placed or to see the status of these daemons. It is very LINUX specific, and nothing to do with Hadoop.

What if a Namenode has no data?
It cannot be part of the Hadoop cluster.

What happens to job tracker when Namenode is down?
When Namenode is down, your cluster is OFF, this is because Namenode is the single point of failure in HDFS.

What is Big Data?
Big Data is nothing but an assortment of such a huge and complex data that it becomes very tedious to capture, store, process, retrieve and analyze it with the help of on-hand database management tools or traditional data processing techniques.

What are the four characteristics of Big Data?
the three characteristics of Big Data are −

 
Volume − Facebook generating 500+ terabytes of data per day.

Velocity − Analyzing 2 million records each day to identify the reason for losses.

Variety − images, audio, video, sensor data, log files, etc.  Veracity: biases, noise and abnormality in data

How is analysis of Big Data useful for organizations?
Effective analysis of Big Data provides a lot of business advantage as organizations will learn which areas to focus on and which areas are less important. Big data analysis provides some early key indicators that can prevent the company from a huge loss or help in grasping a great opportunity with open hands! A precise analysis of Big Data helps in decision making! For instance, nowadays people rely so much on Facebook and Twitter before buying any product or service. All thanks to the Big Data explosion.

Why do we need Hadoop?
Everyday a large amount of unstructured data is getting dumped into our machines. The major challenge is not to store large data sets in our systems but to retrieve and analyze the big data in the organizations, that too data present in different machines at different locations. In this situation a necessity for Hadoop arises. Hadoop has the ability to analyze the data present in different machines at different locations very quickly and in a very cost effective way. It uses the concept of MapReduce which enables it to divide the query into small parts and process them in parallel. This is also known as parallel computing. The following link Why Hadoop gives a detailed explanation about why Hadoop is gaining so much popularity!

What is the basic difference between traditional RDBMS and Hadoop?
Traditional RDBMS is used for transactional systems to report and archive the data, whereas Hadoop is an approach to store huge amount of data in the distributed file system and process it. RDBMS will be useful when you want to seek one record from Big data, whereas, Hadoop will be useful when you want Big data in one shot and perform analysis on that later

What is Fault Tolerance?
Suppose you have a file stored in a system, and due to some technical problem that file gets destroyed. Then there is no chance of getting the data back present in that file. To avoid such situations, Hadoop has introduced the feature of fault tolerance in HDFS. In Hadoop, when we store a file, it automatically gets replicated at two other locations also. So even if one or two of the systems collapse, the file is still available on the third system.

Replication causes data redundancy, then why is it pursued in HDFS?
HDFS works with commodity hardware (systems with average configurations) that has high chances of getting crashed any time. Thus, to make the entire system highly fault-tolerant, HDFS replicates and stores data in different places. Any data on HDFS gets stored at least 3 different locations. So, even if one of them is corrupted and the other is unavailable for some time for any reason, then data can be accessed from the third one. Hence, there is no chance of losing the data. This replication factor helps us to attain the feature of Hadoop called Fault Tolerant.

Since the data is replicated thrice in HDFS, does it mean that any calculation done on one node will also be replicated on the other two?
No, calculations will be done only on the original data. The master node will know which node exactly has that particular data. In case, if one of the nodes is not responding, it is assumed to be failed. Only then, the required calculation will be done on the second replica.

What is a Namenode?
Namenode is the master node on which job tracker runs and consists of the metadata. It maintains and manages the blocks which are present on the datanodes. It is a high-availability machine and single point of failure in HDFS.

Is Namenode also a commodity hardware?
No. Namenode can never be commodity hardware because the entire HDFS rely on it. It is the single point of failure in HDFS. Namenode has to be a high-availability machine.

What is a Datanode?
Datanodes are the slaves which are deployed on each machine and provide the actual storage. These are responsible for serving read and write requests for the clients.

Why do we use HDFS for applications having large data sets and not when there are lot of small files?
HDFS is more suitable for large amount of data sets in a single file as compared to small amount of data spread across multiple files. This is because Namenode is a very expensive high performance system, so it is not prudent to occupy the space in the Namenode by unnecessary amount of metadata that is generated for multiple small files. So, when there is a large amount of data in a single file, name node will occupy less space. Hence for getting optimized performance, HDFS supports large data sets instead of multiple small files.

What is a job tracker?
Job tracker is a daemon that runs on a namenode for submitting and tracking MapReduce jobs in Hadoop. It assigns the tasks to the different task tracker. In a Hadoop cluster, there will be only one job tracker but many task trackers. It is the single point of failure for Hadoop and MapReduce Service. If the job tracker goes down all the running jobs are halted. It receives heartbeat from task tracker based on which Job tracker decides whether the assigned task is completed or not.

What is a task tracker?
Task tracker is also a daemon that runs on datanodes. Task Trackers manage the execution of individual tasks on slave node. When a client submits a job, the job tracker will initialize the job and divide the work and assign them to different task trackers to perform MapReduce tasks. While performing this action, the task tracker will be simultaneously communicating with job tracker by sending heartbeat. If the job tracker does not receive heartbeat from task tracker within specified time, then it will assume that task tracker has crashed and assign that task to another task tracker in the cluster.

What is a heartbeat in HDFS?
A heartbeat is a signal indicating that it is alive. A datanode sends heartbeat to Namenode and task tracker will send its heart beat to job tracker. If the Namenode or job tracker does not receive heart beat then they will decide that there is some problem in datanode or task tracker is unable to perform the assigned task.

What is a ‘block’ in HDFS?
A ‘block’ is the minimum amount of data that can be read or written. In HDFS, the default block size is 64 MB as contrast to the block size of 8192 bytes in Unix/Linux. Files in HDFS are broken down into block-sized chunks, which are stored as independent units. HDFS blocks are large as compared to disk blocks, particularly to minimize the cost of seeks. If a particular file is 50 mb, will the HDFS block still consume 64 mb as the default size? No, not at all! 64 mb is just a unit where the data will be stored. In this particular situation, only 50 mb will be consumed by an HDFS block and 14 mb will be free to store something else. It is the MasterNode that does data allocation in an efficient manner.

What are the benefits of block transfer?
A file can be larger than any single disk in the network. There’s nothing that requires the blocks from a file to be stored on the same disk, so they can take advantage of any of the disks in the cluster. Making the unit of abstraction a block rather than a file simplifies the storage subsystem. Blocks provide fault tolerance and availability. To insure against corrupted blocks and disk and machine failure, each block is replicated to a small number of physically separate machines (typically three). If a block becomes unavailable, a copy can be read from another location in a way that is transparent to the client?

How indexing is done in HDFS?
Hadoop has its own way of indexing. Depending upon the block size, once the data is stored, HDFS will keep on storing the last part of the data which will say where the next part of the data will be.

Are job tracker and task trackers present in separate machines?
Yes, job tracker and task tracker are present in different machines. The reason is job tracker is a single point of failure for the Hadoop MapReduce service. If it goes down, all running jobs are halted.

What is the communication channel between client and namenode/datanode?
The mode of communication is SSH.

What is a rack?
Rack is a storage area with all the datanodes put together. These datanodes can be physically located at different places. Rack is a physical collection of datanodes which are stored at a single location. There can be multiple racks in a single location.

What is a Secondary Namenode? Is it a substitute to the Namenode?
The secondary Namenode constantly reads the data from the RAM of the Namenode and writes it into the hard disk or the file system. It is not a substitute to the Namenode, so if the Namenode fails, the entire Hadoop system goes down.

Explain how do ‘map’ and ‘reduce’ works.
Namenode takes the input and divide it into parts and assign them to data nodes. These datanodes process the tasks assigned to them and make a key-value pair and returns the intermediate output to the Reducer. The reducer collects this key value pairs of all the datanodes and combines them and generates the final output.

Why ‘Reading‘ is done in parallel and ‘Writing‘ is not in HDFS?
Through mapreduce program the file can be read by splitting its blocks when reading. But while writing as the incoming values are not yet known to the system mapreduce cannot be applied and no parallel writing is possible.

Copy a directory from one node in the cluster to another
Use ‘-distcp’ command to copy,

Default replication factor to a file is 3.
Use ‘-setrep’ command to change replication factor of a file to 2.

hadoop fs -setrep -w 2 apache_hadoop/sample.txt

What is rack awareness?
Rack awareness is the way in which the namenode decides how to place blocks based on the rack definitions Hadoop will try to minimize the network traffic between datanodes within the same rack and will only contact remote racks if it has to. The namenode is able to control this due to rack awareness.

Which file does the Hadoop-core configuration?
core-default.xml

Is there a hdfs command to see available free space in hdfs
hadoop dfsadmin -report

The requirement is to add a new data node to a running Hadoop cluster; how do I start services on just one data node?
You do not need to shutdown and/or restart the entire cluster in this case.

First, add the new node's DNS name to the conf/slaves file on the master node.

Then log in to the new slave node and execute −

$ cd path/to/hadoop

$ bin/hadoop-daemon.sh start datanode

$ bin/hadoop-daemon.sh start tasktracker

then issuehadoop dfsadmin -refreshNodes and hadoop mradmin -refreshNodes so that the NameNode and JobTracker know of the additional node that has been added.

How do you gracefully stop a running job?
Hadoop job –kill jobid

Does the name-node stay in safe mode till all under-replicated files are fully replicated?
No. During safe mode replication of blocks is prohibited. The name-node awaits when all or majority of data-nodes report their blocks.

What happens if one Hadoop client renames a file or a directory containing this file while another client is still writing into it?
A file will appear in the name space as soon as it is created. If a writer is writing to a file and another client renames either the file itself or any of its path components, then the original writer will get an IOException either when it finishes writing to the current block or when it closes the file.

How to make a large cluster smaller by taking out some of the nodes?
Hadoop offers the decommission feature to retire a set of existing data-nodes. The nodes to be retired should be included into the exclude file, and the exclude file name should be specified as a configuration parameter dfs.hosts.exclude.

The decommission process can be terminated at any time by editing the configuration or the exclude files and repeating the -refreshNodes command

Can we search for files using wildcards?
Yes. For example, to list all the files which begin with the letter a, you could use the ls command with the * wildcard &minu;

hdfs dfs –ls a*

What happens when two clients try to write into the same HDFS file?
HDFS supports exclusive writes only.

When the first client contacts the name-node to open the file for writing, the name-node grants a lease to the client to create this file. When the second client tries to open the same file for writing, the name-node will see that the lease for the file is already granted to another client, and will reject the open request for the second client

What does "file could only be replicated to 0 nodes, instead of 1" mean?
The namenode does not have any available DataNodes.

What is a Combiner?
The Combiner is a ‘mini-reduce’ process which operates only on data generated by a mapper. The Combiner will receive as input all data emitted by the Mapper instances on a given node. The output from the Combiner is then sent to the Reducers, instead of the output from the Mappers

Consider case scenario: In M/R system, - HDFS block size is 64 MB
- Input format is FileInputFormat

– We have 3 files of size 64K, 65Mb and 127Mb

How many input splits will be made by Hadoop framework?

Hadoop will make 5 splits as follows −

- 1 split for 64K files
- 2 splits for 65MB files
- 2 splits for 127MB files
Suppose Hadoop spawned 100 tasks for a job and one of the task failed. What will Hadoop do?
It will restart the task again on some other TaskTracker and only if the task fails more than four ( the default setting and can be changed) times will it kill the job.

What are Problems with small files and HDFS?
HDFS is not good at handling large number of small files. Because every file, directory and block in HDFS is represented as an object in the namenode’s memory, each of which occupies approx 150 bytes So 10 million files, each using a block, would use about 3 gigabytes of memory. when we go for a billion files the memory requirement in namenode cannot be met.

What is speculative execution in Hadoop?
If a node appears to be running slow, the master node can redundantly execute another instance of the same task and first output will be taken .this process is called as Speculative execution.

Can Hadoop handle streaming data?
Yes, through Technologies like Apache Kafka, Apache Flume, and Apache Spark it is possible to do large-scale streaming.

Why is Checkpointing Important in Hadoop?
As more and more files are added the namenode creates large edit logs. Which can substantially delay NameNode startup as the NameNode reapplies all the edits. Checkpointing is a process that takes an fsimage and edit log and compacts them into a new fsimage. This way, instead of replaying a potentially unbounded edit log, the NameNode can load the final in-memory state directly from the fsimage. This is a far more efficient operation and reduces NameNode startup time.

What is Twitter Bootstrap?
Bootstrap is a sleek, intuitive, and powerful mobile first front-end framework for faster and easier web development. It uses HTML, CSS and Javascript.

Why use Bootstrap?
Bootstrap can be used as −

Mobile first approach − Since Bootstrap 3, the framework consists of Mobile first styles throughout the entire library instead of in separate files.

Browser Support − It is supported by all popular browsers.


Easy to get started − With just the knowledge of HTML and CSS anyone can get started with Bootstrap. Also the Bootstrap official site has a good documentation.

Responsive design − Bootstrap's responsive CSS adjusts to Desktops,Tablets and Mobiles.

Provides a clean and uniform solution for building an interface for developers.

It contains beautiful and functional built-in components which are easy to customize.

It also provides web based customization.

And best of all it is an open source.

What does Bootstrap package includes?
Bootstrap package includes −

Scaffolding − Bootstrap provides a basic structure with Grid System, link styles, background. This is is covered in detail in the section Bootstrap Basic Structure

CSS − Bootstrap comes with feature of global CSS settings, fundamental HTML elements styled and enhanced with extensible classes, and an advanced grid system. This is covered in detail in the section Bootstrap with CSS.

Components − Bootstrap contains over a dozen reusable components built to provide iconography, dropdowns, navigation, alerts, popovers, and much more. This is covered in detail in the section Layout Components.

JavaScript Plugins − Bootstrap contains over a dozen custom jQuery plugins. You can easily include them all, or one by one. This is covered in details in the section Bootstrap Plugins.

Customize − You can customize Bootstrap's components, LESS variables, and jQuery plugins to get your very own version.

What is Contextual classes of table in Bootstrap?
The Contextual classes allow you to change the background color of your table rows or individual cells.

Class	Description
.active	Applies the hover color to a particular row or cell
.success	Indicates a successful or positive action
.warning	Indicates a warning that might need attention
.danger	Indicates a dangerous or potentially negative action
What is Bootstrap Grid System?
Bootstrap includes a responsive, mobile first fluid grid system that appropriately scales up to 12 columns as the device or viewport size increases. It includes predefined classes for easy layout options, as well as powerful mixins for generating more semantic layouts.

What are Bootstrap media queries?
Media Queries in Bootstrap allow you to move, show and hide content based on viewport size.

Show a basic grid structure in Bootstrap.
Following is basic structure of Bootstrap grid −

<div class="container">
   <div class="row">
      <div class="col-*-*"></div>
      <div class="col-*-*"></div>      
   </div>
   <div class="row">...</div>
</div>
<div class="container">....
What are Offset columns?
Offsets are a useful feature for more specialized layouts. They can be used to push columns over for more spacing, for example. The .col-xs=* classes don't support offsets, but they are easily replicated by using an empty cell.

How can you order columns in Bootstrap?
You can easily change the order of built-in grid columns with .col-md-push-* and .col-md-pull-* modifier classes where * range from 1 to 11.

How do you make images responsive?
Bootstrap 3 allows to make the images responsive by adding a class .img-responsive to the <img> tag. This class applies max-width: 100%; and height: auto; to the image so that it scales nicely to the parent element.

Explain the typography and links in Bootstrap.
Bootstrap sets a basic global display (background), typography, and link styles −

Basic Global display − Sets background-color: #fff; on the <body> element.

Typography − Uses the @font-family-base, @font-size-base, and @line-height-base attributes as the typographic base

Link styles − Sets the global link color via attribute @link-color and apply link underlines only on :hover.

What is Normalize in Bootstrap?
Bootstrap uses Normalize to establish cross browser consistency.

Normalize.css is a modern, HTML5-ready alternative to CSS resets. It is a small CSS file that provides better cross-browser consistency in the default styling of HTML elements.

What is Lead Body Copy
To add some emphasis to a paragraph, add class="lead". This will give you larger font size, lighter weight, and a taller line height

Explain types of lists supported by Bootstrap.
Bootstrap supports ordered lists, unordered lists, and definition lists.

Ordered lists − An ordered list is a list that falls in some sort of sequential order and is prefaced by numbers.

Unordered lists − An unordered list is a list that doesn't have any particular order and is traditionally styled with bullets. If you do not want the bullets to appear then you can remove the styling by using the class .list-unstyled. You can also place all list items on a single line using the class .list-inline.

Definition lists − In this type of list, each list item can consist of both the <dt> and the <dd> elements. <dt> stands for definition term, and like a dictionary, this is the term (or phrase) that is being defined. Subsequently, the <dd> is the definition of the <dt>.

You can make terms and descriptions in <dl> line up side-by-side using class dl-horizontal.
What are glyphicons?
Glyphicons are icon fonts which you can use in your web projects. Glyphicons Halflings are not free and require licensing, however their creator has made them available for Bootstrap projects free of cost.

How do you use Glyphicons?
To use the icons, simply use the following code just about anywhere in your code. Leave a space between the icon and text for proper padding.

<span class="glyphicon glyphicon-search"></span>
What is a transition plugin?
The transition plugin provides simple transition effects such as Sliding or fading in modals.

What is a Modal Plugin?
A modal is a child window that is layered over its parent window. Typically, the purpose is to display content from a separate source that can have some interaction without leaving the parent window. Child windows can provide information, interaction, or more.

How do you use the Dropdown plugin?
You can toggle the dropdown plugin's hidden content:

Via data attributes: Add data-toggle="dropdown" to a link or button to toggle a dropdown as shown below −

<div class="dropdown">
  <a data-toggle="dropdown" href="#">Dropdown trigger</a>
  <ul class="dropdown-menu" role="menu" aria-labelledby="dLabel">
    ...
  </ul>
</div>
If you need to keep links intact (which is useful if the browser is not enabling JavaScript), use the data-target attribute instead of href="#" −

<div class="dropdown">
  <a id="dLabel" role="button" data-toggle="dropdown" data-target="#" href="/page.html">
    Dropdown <span class="caret"></span>
  </a>


  <ul class="dropdown-menu" role="menu" aria-labelledby="dLabel">
    ...
  </ul>
</div>
Via JavaScript − To call the dropdown toggle via JavaScript, use the following method:

$('.dropdown-toggle').dropdown()
What is Bootstrap caraousel?
The Bootstrap carousel is a flexible, responsive way to add a slider to your site. In addition to being responsive, the content is flexible enough to allow images, iframes, videos, or just about any type of content that you might want.

What is button group
Button groups allow multiple buttons to be stacked together on a single line. This is useful when you want to place items like alignment buttons together.

Which class is used for basic button group
.btn-group class is used for a basic button group. Wrap a series of buttons with class .btn in .btn-group.

Which class is used to draw a toolbar of buttons
.btn-toolbar helps to combine sets of <div class="btn-group"> into a <div class="btn-toolbar"> for more complex components.

Which classses can be applied to button group instead of resizing each button
.btn-group-lg, .btn-group-sm, .btn-group-xs classses can be applied to button group instead of resizing each button.

Which class make a set of buttons appear vertically stacked rather than horizontally
.btn-group-vertical class make a set of buttons appear vertically stacked rather than horizontally.

What are input groups
Input groups are extended Form Controls. Using input groups you can easily prepend and append text or buttons to the text-based inputs.

By adding prepended and appended content to an input field, you can add common elements to the user's input. For example, you can add the dollar symbol, the @ for a Twitter username, or anything else that might be common for your application interface.

To prepend or append elements to a .form-control −

Wrap it in a <div> with class .input-group

As a next step, within that same <div> , place your extra content inside a <span> with class .input-group-addon.

Now place this <span> either before or after the <input> element.

How will you create a tabbed navigation menu
To create a tabbed navigation menu −

Start with a basic unordered list with the base class of .nav.

Add class .nav-tabs.

How will you create a pills navigation menu
To create a pills navigation menu −

Start with a basic unordered list with the base class of .nav.

Add class .nav-pills.

How will you create a vertical pills navigation menu
You can stack the pills vertically using the class .nav-stacked along with the classes: .nav, .nav-pills.

What is bootstrap navbar
The navbar is one of the prominent features of Bootstrap sites. Navbars are responsive 'meta' components that serve as navigation headers for your application or site. Navbars collapse in mobile views and become horizontal as the available viewport width increases. At its core, the navbar includes styling for site names and basic navigation.

How to create a navbar in bootstrap
To create a default navbar −

Add the classes .navbar, .navbar-default to the <nav> tag.

Add role="navigation" to the above element, to help with accessibility.

Add a header class .navbar-header to the <div> element. Include an <a> element with class navbar-brand. This will give the text a slightly larger size.

To add links to the navbar, simply add an unordered list with the classes of .nav, .navbar-nav.

What is bootstrap breadcrumb
Breadcrumbs are a great way to show hierarchy-based information for a site. In the case of blogs, breadcrumbs can show the dates of publishing, categories, or tags. They indicate the current page's location within a navigational hierarchy.

A breadcrumb in Bootstrap is simply an unordered list with a class of .breadcrumb. The separator is automatically added by CSS (bootstrap.min.css).

Which class is used for basic pagination
.pagination class is uesed to add the pagination on a page.

How will you customize links of pagination
You can customize links by using .disabled for unclickable links and .active to indicate the current page.

What are bootstrap labels
Bootstrap labels are great for offering counts, tips, or other markup for pages. Use class .label to display labels.

What are bootstrap badges
Badges are similar to labels; the primary difference is that the corners are more rounded. Badges are mainly used to highlight new or unread items. To use badges just add <span class="badge"> to links, Bootstrap navs, and more.

What is Bootstrap Jumbotron
As the name suggest this component can optionally increase the size of headings and add a lot of margin for landing page content. To use the Jumbotron −

Create a container <div> with the class of .jumbotron.

In addition to a larger <h1>, the font-weight is reduced to 200px.

What is Bootstrap page header
The page header is a nice little feature to add appropriate spacing around the headings on a page. This is particularly helpful on a web page where you may have several post titles and need a way to add distinction to each of them. To use a page header, wrap your heading in a <div> with a class of .page-header.

How to create thumbnails using Bootstrap
To create thumbnails using Bootstrap −

Add an <a> tag with the class of .thumbnail around an image.

This adds four pixels of padding and a gray border.

On hover, an animated glow outlines the image.

How to customize thumbnails using Bootstrap
it's possible to add any kind of HTML content like headings, paragraphs, or buttons into thumbnails. Follow the steps below −

Change the <a> tag that has a class of .thumbnail to a <div>.

Inside of that <div>, you can add anything you need. As this is a <div>, we can use the default span-based naming convention for sizing.

If you want to group multiple images, place them in an unordered list, and each list item will be floated to the left.

What are bootstrap alerts?
Bootstrap Alerts provide a way to style messages to the user. They provide contextual feedback messages for typical user actions.

You can add an optional close icon to alert.

How will you create a bootstrap alert?
You can add a basic alert by creating a wrapper <div> and adding a class of .alert and one of the four contextual classes (e.g., .alert-success, .alert-info, .alert-warning, .alert-danger).

How will you create a Bootstrap Dismissal Alert?
To build a dismissal alert −

Add a basic alert by creating a wrapper <div> and adding a class of .alert and one of the four contextual classes (e.g., .alert-success, .alert-info, .alert-warning, .alert-danger).

Also add optional .alert-dismissable to the above <div> class.

Add a close button.

Use the <button> element with the data-dismiss="alert" data attribute.

How will you create a progress bar using bootstrap?
To create a basic progress bar −

Add a <div> with a class of .progress.

Next, inside the above <div>, add an empty <div> with a class of .progress-bar.

Add a style attribute with the width expressed as a percentage. Say for example, style="60%"; indicates that the progress bar was at 60%.

How will you create a alternate progress bar using bootstrap?
To create a progress bar with different styles −

Add a <div> with a class of .progress.

Next, inside the above <div>, add an empty <div> with a class of .progress-bar and class progress-bar-* where * could be success, info, warning, danger.

Add a style attribute with the width expressed as a percentage. Say for example, style="60%"; indicates that the progress bar was at 60%.

How will you create a striped progress bar using bootstrap
To create a striped progress bar −

Add a <div> with a class of .progress and .progress-striped.

Next, inside the above <div>, add an empty <div> with a class of .progress-bar and class progress-bar-* where * could be success, info, warning, danger.

Add a style attribute with the width expressed as a percentage. Say for example, style="60%"; indicates that the progress bar was at 60%.

How will you create a animated progress bar using bootstrap?
To create an animated progress bar −

Add a <div> with a class of .progress and .progress-striped. Also add class .active to .progress-striped.

Next, inside the above <div>, add an empty <div> with a class of .progress-bar.

Add a style attribute with the width expressed as a percentage. Say for example, style="60%"; indicates that the progress bar was at 60%.

How will you create a stacked progress bar using bootstrap
You can even stack multiple progress bars. Place the multiple progress bars into the same .progress to stack them.

What are bootstrap media objects
These are abstract object styles for building various types of components (like blog comments, Tweets, etc.) that feature a left-aligned or right-aligned image alongside the textual content. The goal of the media object is to make the code for developing these blocks of information drastically shorter.

The goal of media objects (light markup, easy extendability) is achieved by applying classes to some of the simple markup.

What is the purpose of .mecia class in bootstrap?
This class allows to float a media object (images, video, and audio) to the left or right of a content block.

What is the purpose of .media-list class in bootstrap
If you are preparing a list where the items will be part of an unordered list, use a class. useful for comment threads or articles lists.

What are bootstrap panels
Panel components are used when you want to put your DOM component in a box. To get a basic panel, just add class .panel to the <div> element. Also add class .panel-default to this element.

How will you create a bootstrap panel with heading
here are two ways to add panel heading −

Use .panel-heading class to easily add a heading container to your panel.

Use any <h1>-<h6> with a .panel-title class to add a pre-styled heading.

How will you create a bootstrap panel with footer
You can add footers to panels, by wrapping buttons or secondary text in a <div> containing class .panel-footer.

What contextual classes are available to style the panels
Use contextual state classes such as, panel-primary, panel-success, panel-info, panel-warning, panel-danger, to make a panel more meaningful to a particular context.

Can you put a table within bootstrap panel
Yes! To get a non-bordered table within a panel, use the class .table within the panel. Suppose there is a <div> containing .panel-body, we add an extra border to the top of the table for separation. If there is no <div> containing .panel-body, then the component moves from panel header to table without interruption.

Can you put a listgroup within bootstrap panel
Yes! You can include list groups within any panel. Create a panel by adding class .panel to the <div> element. Also add class .panel-default to this element. Now within this panel include your list groups.

What is bootstrap well
A well is a container in <div> that causes the content to appear sunken or an inset effect on the page. To create a well, simply wrap the content that you would like to appear in the well with a <div> containing the class of .well.

What is Scrollspy plugin
The Scrollspy (auto updating nav) plugin allows you to target sections of the page based on the scroll position. In its basic implementation, as you scroll, you can add .active classes to the navbar based on the scroll position.

What is affix plugin
The affix plugin allows a <div> to become affixed to a location on the page. You can also toggle it's pinning on and off using this plugin. A common example of this are social icons. They will start in a location, but as the page hits a certain mark, the <div> will be locked in place and will stop scrolling with the rest of the page.

======================================================================================================================

Alo read https://data-flair.training/blogs/top-100-hadoop-interview-questions-and-answers/



